{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader(\"pdfs/2410.09836v2.pdf\")\n",
    "pages=loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdfs/2410.09836v2.pdf', 'page': 0}, page_content='Learning Pattern-Specific Experts for Time Series\\nForecasting Under Patch-level Distribution Shift\\nYanru Sun, Zongxia Xie∗, Emadeldeen Eldele‡†, Dongyue Chen, Qinghua Hu, Min Wu‡\\nCollege of Intelligence and Computing, Tianjin University, China\\n‡I2R, Agency for Science, Technology and Research, Singapore.\\n†Department of Computer Science, Khalifa University, UAE.\\n{yanrusun, dyuechen, huqinghua}@tju.edu.cn,caddiexie@hotmail.com,\\nemad0002@ntu.edu.sg,wumin@i2r.a-star.edu.sg\\nAbstract\\nTime series forecasting, which aims to predict future values based on historical\\ndata, has garnered significant attention due to its broad range of applications.\\nHowever, real-world time series often exhibit heterogeneous pattern evolution\\nacross segments, such as seasonal variations, regime changes, or contextual shifts,\\nmaking accurate forecasting challenging. Existing approaches, which typically\\ntrain a single model to capture all these diverse patterns, often struggle with the\\npattern drifts between patches and may lead to poor generalization. To address these\\nchallenges, we proposeTFPS, a novel architecture that leverages pattern-specific\\nexperts for more accurate and adaptable time series forecasting. TFPS employs a\\ndual-domain encoder to capture both time-domain and frequency-domain features,\\nenabling a more comprehensive understanding of temporal dynamics. It then\\nperforms subspace clustering to dynamically identify distinct patterns across data\\nsegments. Finally, these patterns are modeled by specialized experts, allowing the\\nmodel to learn multiple predictive functions. Extensive experiments on real-world\\ndatasets demonstrate that TFPS outperforms state-of-the-art methods, particularly\\non datasets exhibiting significant distribution shifts. The data and code are available:\\nhttps://github.com/syrGitHub/TFPS.\\n1 Introduction\\nTime series forecasting plays a critical role in various domains, such as finance [ 18], weather\\n[3, 63, 24], traffic [35, 22], and others [59, 33, 68], by modeling the relationship between historical\\ndata and future outcomes. However, the inherent complexity of time series data, including temporal\\ndependencies and non-stationarity, poses significant challenges in achieving reliable forecasts.\\nRecent Transformer-based models have shown great promise in time series forecasting due to their\\nability to model long-range dependencies [29, 53]. In particular, models like PatchTST [ 44] split\\ncontinuous time series into discrete patches and process them with Transformer blocks. While\\nthese models are effective, a closer examination reveals that patches often exhibit distribution shifts,\\nwhich are frequently associated with concept drift [36]. For example, patches from different regimes,\\nseasons, or operating modes may not only differ in statistical properties [27], but also in the functional\\nrelationships between historical and future values [60, 56]. However, this variability contradicts the\\nassumptions of most existing models [44, 69, 9], which adopt the Uniform Distribution Modeling\\n(UDM) strategy by treating all patches as samples from a single underlying distribution. This\\noversimplified view ignores structural heterogeneity and temporal variation across segments, thereby\\nlimiting the model’s ability to generalize and degrading its forecasting performance [43, 25].\\n∗Corresponding author\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2410.09836v2  [cs.LG]  1 Oct 2025')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI,OpenAI\n",
    "\n",
    "model=ChatOpenAI(\n",
    "  model=\"qwen2.5-coder-1.5b-instruct\",\n",
    "  openai_api_key=\"EMPTY\",\n",
    "  base_url=\"http://127.0.0.1:1234/v1\",\n",
    "  temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Pattern-Specific Experts for Time Series\\nForecasting Under Patch-level Distribution Shift\\nYanru Sun, Zongxia Xie∗, Emadeldeen Eldele‡†, Dongyue Chen, Qinghua Hu, Min Wu‡\\nCollege of Intelligence and Computing, Tianjin University, China\\n‡I2R, Agency for Science, Technology and Research, Singapore.\\n†Department of Computer Science, Khalifa University, UAE.\\n{yanrusun, dyuechen, huqinghua}@tju.edu.cn,caddiexie@hotmail.com,\\nemad0002@ntu.edu.sg,wumin@i2r.a-star.edu.sg\\nAbstract\\nTime series forecasting, which aims to predict future values based on historical\\ndata, has garnered significant attention due to its broad range of applications.\\nHowever, real-world time series often exhibit heterogeneous pattern evolution\\nacross segments, such as seasonal variations, regime changes, or contextual shifts,\\nmaking accurate forecasting challenging. Existing approaches, which typically\\ntrain a single model to capture all these diverse patterns, often struggle with the\\npattern drifts between patches and may lead to poor generalization. To address these\\nchallenges, we proposeTFPS, a novel architecture that leverages pattern-specific\\nexperts for more accurate and adaptable time series forecasting. TFPS employs a\\ndual-domain encoder to capture both time-domain and frequency-domain features,\\nenabling a more comprehensive understanding of temporal dynamics. It then\\nperforms subspace clustering to dynamically identify distinct patterns across data\\nsegments. Finally, these patterns are modeled by specialized experts, allowing the\\nmodel to learn multiple predictive functions. Extensive experiments on real-world\\ndatasets demonstrate that TFPS outperforms state-of-the-art methods, particularly\\non datasets exhibiting significant distribution shifts. The data and code are available:\\nhttps://github.com/syrGitHub/TFPS.\\n1 Introduction\\nTime series forecasting plays a critical role in various domains, such as finance [ 18], weather\\n[3, 63, 24], traffic [35, 22], and others [59, 33, 68], by modeling the relationship between historical\\ndata and future outcomes. However, the inherent complexity of time series data, including temporal\\ndependencies and non-stationarity, poses significant challenges in achieving reliable forecasts.\\nRecent Transformer-based models have shown great promise in time series forecasting due to their\\nability to model long-range dependencies [29, 53]. In particular, models like PatchTST [ 44] split\\ncontinuous time series into discrete patches and process them with Transformer blocks. While\\nthese models are effective, a closer examination reveals that patches often exhibit distribution shifts,\\nwhich are frequently associated with concept drift [36]. For example, patches from different regimes,\\nseasons, or operating modes may not only differ in statistical properties [27], but also in the functional\\nrelationships between historical and future values [60, 56]. However, this variability contradicts the\\nassumptions of most existing models [44, 69, 9], which adopt the Uniform Distribution Modeling\\n(UDM) strategy by treating all patches as samples from a single underlying distribution. This\\noversimplified view ignores structural heterogeneity and temporal variation across segments, thereby\\nlimiting the model’s ability to generalize and degrading its forecasting performance [43, 25].\\n∗Corresponding author\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2410.09836v2  [cs.LG]  1 Oct 2025(a) Sudden drift\\n (b) Gradual drift\\nFigure 1: Illustration of distribution shifts between time series patches on the ETTh1 dataset,\\nquantified by Wasserstein distance. The combined time- and frequency-domain views reveal richer\\nand more complementary shift patterns arising from temporal non-stationarity.\\nTo quantify these distributional shifts, we split the ETTh1 dataset into patches and analyze two\\nrepresentative cases: sudden drift and gradual drift, in both time and frequency domains. Specifically,\\nwe compute the Wasserstein distance between patches and visualize the results as heatmaps in Figure 1,\\nwhich clearly illustrate the discrepancies across segments. Notably, sudden drift (Figure 1 (a)) leads\\nto a sharp discrepancy between patches 9 and 10 and the remaining segments, while gradual drift\\n(Figure 1 (b)) reveals that patches 0 to 5 differ from patches 6 to 11, exhibiting a progressive shift that\\nmakes forecasting more challenging. Furthermore, the frequency domain offers a complementary\\nperspective on the shifts [42]. These observations highlight the complex and evolving nature of time\\nseries data, where different segments may follow distinct distributions and exhibit heterogeneous\\ntemporal patterns [55, 27, 17, 56, 52].\\nTo address the challenges posed by distribution shifts in time series data, we propose a novel\\nTime-FrequencyPattern-Specific (TFPS) architecture to effectively model the complex temporal\\npatterns. In particular, TFPS consists of the following three key components. The first is a Dual-\\nDomain Encoder (DDE), which extracts features from both time and frequency domains to provide a\\ncomprehensive representation of the time series data, enabling the model to capture both short-term\\nand long-term dependencies. Second, TFPS addresses the issue of concept drift by incorporating a\\nPattern Identifier (PI), that utilizes a subspace clustering approach to dynamically identify the distinct\\npatterns across patches. This enables the model to effectively handle nonlinear cluster boundaries\\nand accurately assign patches to their corresponding clusters. Finally, TFPS constructs a Mixture\\nof Pattern Experts (MoPE)—a set of specialized expert models, each tailored to a specific pattern\\nidentified by the PI. By dynamically assigning patches to the appropriate experts, TFPS learns\\npattern-specific predictive functions that effectively capture heterogeneous temporal dynamics and\\ndistributional variations. This specialized modeling strategy enhances the model’s adaptability and\\nyields significant forecasting improvements, particularly on datasets with severe distributional drift.\\nIn summary, the key contributions of this work are:\\n• We introduce a novel pattern-specific forecasting paradigm that enables segment-wise expert\\nmodeling based on latent pattern structure, overcoming the limitations of uniform modeling\\nunder distribution shift.\\n• We propose TFPS, a dual-domain framework that integrates time- and frequency-domain\\nrepresentations with subspace clustering and dynamic expert routing, enabling the model to\\nexplicitly adapt to concept drift and capture evolving patterns in non-stationary time series.\\n• We evaluate our approach on nine real-world multivariate time series datasets, demonstrating\\nits effectiveness. Our model achieves top-1 performance in 57 out of 72 settings, showcasing\\nits competitive edge in improving forecasting accuracy.\\n22 Related Work\\nTime Series Forecasting Models.In recent years, deep models with elaborately designed archi-\\ntectures have achieved great progress in time series forecasting [ 51, 67, 28, 45]. Approaches like\\nTimesNet [61] and ModernTCN [41] utilize convolutional neural networks with time-series-specific\\nmodifications, making them better suited for forecasting tasks. Additionally, simpler architectures\\nsuch as Multi-Layer Perceptron (MLP)-based models [69, 8] have demonstrated competitive perfor-\\nmance. However, Transformer-based models have gained particular prominence due to their ability\\nto model long-term dependencies in time series [70, 62, 71, 29]. Notably, PatchTST [44] has become\\na widely adopted Transformer variant, introducing a channel-independent patching mechanism to\\nenhance temporal representations. This approach has been further extended by subsequent models\\n[29, 9].\\nWhile previous work has primarily focused on capturing nonlinear dependencies in time series\\nthrough enhanced model structures, our approach addresses the distribution shifts caused by evolving\\npatterns within the data, which is a key limitation of existing methods.\\nNon-stationary Time Series Forecasting.Non-stationarity in time series data complicate predictive\\nmodeling, necessitating effective solutions to handle shifting distributions [ 36, 11]. To address\\nvarying distributions, normalization techniques have emerged as a focal point in recent research,\\naiming to mitigate non-stationary elements and align data with a consistent distribution.\\nFor instance, adaptive norm [46] applies z-score normalization using global statistics and DAIN [47]\\nintroduces a neural layer for adaptively normalizing each input instance. Reversible instance normal-\\nization (RevIN) [20] is proposed to alleviate series shift. Furthermore, Non-stationary transformer\\n[31] points that directly stationarizing time series will damage the model’s capability to capture spe-\\ncific temporal dependencies and introduces an innovative de-stationary attention mechanism within\\nself-attention frameworks. Recent advancement include Dish-TS [10], which identifies both intra-\\nand inter-space distribution shifts in time series data, and SAN [34], which applies normalization at\\nthe slice level, thus opening new avenues for handling non-stationary time series data. Lastly, SIN\\n[17] introduces a novel method to selecting the statistics and learning normalization transformations\\nto capture local invariance in time series data.\\nHowever, normalization methods can only address changes in statistical properties, and over-reliance\\non them may lead to over-stationarization, where meaningful temporal variations are inadvertently\\nsmoothed out [34]. In contrast, our approach preserves the intrinsic non-stationarity of the original\\nseries in the latent representation space, enabling the model to better adapt to evolving regimes by\\ntailoring experts to diverse temporal patterns and distributional structures.\\n3 Method\\n3.1 Preliminaries\\nTime series forecasting aims to uncover relationships between historical time series data and future\\ndata. Let X denote the time series, and xt represent the value at timestep t. Given the historical\\ntime series data X= [x t−L+1,· · ·, x t]∈R L×C, where L is the length of the look-back window\\nand C >1 is the number of features in each timestep, the objective is to predict the future series\\nY= [x t+1,· · ·, x t+H]∈R H×C , whereHis the forecast horizon.\\n3.2 Overall Architecture\\nOur model introduces three novel components: the Dual-Domain Encoder (DDE), the Pattern\\nIdentifier (PI), and the Mixture of Pattern Experts (MoPE), as illustrated in Figure 2. The DDE goes\\nbeyond traditional time-domain encoding by incorporating a frequency encoder that applies Fourier\\nanalysis, transforming time series data into the frequency domain. This enables the model to capture\\nperiodic patterns and frequency-specific features, providing a more comprehensive understandinganalysis, transforming time series data into the frequency domain. This enables the model to capture\\nperiodic patterns and frequency-specific features, providing a more comprehensive understanding\\nof the data. The PI is a clustering-based module that distinguishes patches with distinct patterns,\\neffectively addressing the variability in the data. MoPE then utilizes multiple MLP-based experts,\\neach dedicated to modeling a specific pattern, thereby enhancing the model’s ability to adapt to the\\ntemporal dynamics of time series. Collectively, these components form a cohesive framework that\\neffectively handles concept drift between patches, leading to more accurate time series forecasting.\\n3Figure 2: The structure of our proposed TFPS. The input time series is divided into patches, and\\npositional embeddings are added. These embeddings are processed through two branches: time-\\ndomain branch and frequency-domain branch. Each branch consists of three key components: (1) an\\nencoder to capture patch-wise features, (2) a clustering mechanism to identify patches with similar\\npatterns, and (3) a mixture of pattern experts block to model the patterns of each cluster. Finally, the\\noutputs from both branches are combined for the final prediction.\\n3.3 Embedding Layer\\nFirstly, the input sequence X∈R L×C is divided into patches of length P , resulting in N=\\n⌊ (L−P)\\nS + 2⌋ tokens, where S denotes the stride, defining the non-overlapping region between\\nconsecutive patches. Each patch is denoted as Pi ∈R C×P . These patches are then projected into a\\nnew dimensionD, via a linear transformation, such that,P i → P ′\\ni ∈R C×D.\\nNext, positional embeddings are added to each patch to preserve the temporal ordering disrupted dur-\\ning the segmentation process. The position embedding for the i-th patch, denoted as Ei, is a vector of\\nthe same dimension as the projected patch. The enhanced patch is computed by summing the original\\npatch and its positional embedding: XP Ei =P ′\\ni +E i, and XP E ={X P E1 , XP E2 ,· · ·, X P EN }.\\nNotably, the positional embeddings are learnable parameters, which enables the model to capture\\nthe temporal dependencies in the time series more effectively. As a result, the final enriched patch\\nrepresentations areX P E ∈R C×N×D .\\n3.4 Dual-Domain Encoder\\nAs shown in Figure 1, both time and frequency domains reveal distinct concept drifts that can\\nsignificantly affect the performance of forecasting models. To effectively address these drifts, we\\npropose a Dual-Domain Encoder (DDE) architecture that captures both temporal and frequency\\ndependencies inherent in time series data.\\nWe utilize the patch-based Transformer [44] as an encoder to extract embeddings for each patch,\\ncapturing the global trend feature. The multi-head attention is employed to obtain the attention output\\nOt ∈R N×D :\\nOt =Attention(Q, K, V) =Softmax\\n\\x12 QK T\\n√dk\\n\\x13\\nV,\\nQ=X P EWQ, K=X P EWK, V=X P EWV .\\n(1)\\nThe encoder block also incorporates BatchNorm layers and a feed-forward network with residual\\nconnections, as shown in Figure 2 (b). This process generates the temporal featuresz t ∈R C×N×D .\\nIn parallel with the time encoder, we incorporate a Frequency Encoder by replacing the self-attention\\nsublayer of the Transformer with a Fourier sublayer [26]. This sublayer applies a 2D Fast Fourier\\nTransform (the number of patches, hidden dimension) to the patch representation, expressed as:\\nOf =F patch(Fh(XP E)).(2)\\n4Figure 3: Illustration of the proposed Pattern Identifier and Mixture of Pattern Experts. The embedded\\nrepresentation z from DDE combines with subspace D to construct the subspace affinity vector,\\nwhich yields the normalized subspace affinity S. Subsequently, the refined subspace affinity ˆS\\nis computed from S to provide self-supervised information. Then, we assign the corresponding\\npatch-wise experts to the embedded representationzaccording toSfor modeling.\\nWe only keep the real part of the result, and hence, we do not modify the feed-forward layers in the\\nTransformer. The structure of the Frequency Encoder is depicted in Figure 2 (c), yielding frequency\\nfeaturesz f ∈R C×N×D .\\nBy modeling data in both the time and frequency domains, the DDE provides a more comprehensive\\nunderstanding of temporal patterns, enabling the model to effectively handle complexities such as\\nconcept drift and evolving dynamics. This dual-domain perspective enhances the model’s robustness\\nand predictive accuracy, offering a versatile foundation for real-world time series forecasting.\\n3.5 Pattern Identifier\\nTo address the complex and evolving patterns in time series data, we introduce a novel Pattern\\nIdentifier (PI) module, an essential innovation within our framework. Unlike traditional approaches\\nthat treat the entire time series uniformly, our PI module dynamically classifies patches based on their\\ndistributional characteristics, enabling a more precise and adaptive modeling strategy.\\nThe core of our approach lies in leveraging subspace clustering to detect concept shifts across multiple\\nsubspaces, as illustrated in Figure 3. The PI module plays a central role by directly analyzing the\\nintrinsic properties of each patch and clustering them into distinct groups based on their latent patterns.\\nIn the time domain, PI enables TFPS to identify shifts in temporal characteristics such as seasonality\\nand trends. In the frequency domain, it captures shifts associated with frequency-specific structures,\\nlike periodic behaviors and spectral changes, providing a comprehensive perspective on evolving\\npatterns throughout the series.\\nTo provide clarity, Figure 3 showcases an application of the PI module exclusively within the\\ntime domain. However, the insights and methodology seamlessly extend to the frequency domain,\\npresenting a unified solution to the challenge of concept shifts.\\nThe PI module iteratively refines subspace bases, which in turn improve representation learning and\\nenable more accurate modeling of evolving patterns. It operates through the following three steps.\\nConstruction of Subspace Bases.We define a new variable D= [D (1),D (2),· · ·,D (K)] to\\nrepresent the bases of K subspaces, where D consists of K blocks, each D(j) ∈R q×d,\\n\\r\\r\\rD(j)\\nu\\n\\r\\r\\r =\\n1, u= 1,· · ·d, j= 1,· · ·, K . To control the column sizes of D, we impose the following constraint:\\nR1 = 1\\n2\\n\\r\\rDT D⊙I−I\\n\\r\\r2\\nF ,(3)\\nwhere⊙denotes the Hadamard product, andIis an identity matrix of sizeKd×Kd.\\n5Subspaces Differentiation.To ensure the dissimilarity between different subspaces, we introduce\\nthe second constraint:\\nR2 = 1\\n2\\n\\r\\r\\rD(j)T D(l)\\n\\r\\r\\r\\n2\\nF\\n, j̸=l,\\n= 1\\n2\\n\\r\\rDT D⊙O\\n\\r\\r2\\nF ,\\n(4)\\nwhere O is a matrix with all off-diagonal d-size blocks set to 1 and diagonal blocks set to 0.\\nCombiningR 1 andR 2 yields the regularization term forR:\\nR=α(R 1 +R 2),(5)\\nwhereαis a tuning parameters, fixed at10 −3 in this work.\\nSubspace Affinity and Refinement.We propose a novel subspace affinity measure S to assess\\nthe relationship between the embedded representation z from DDE and the subspace bases D. The\\naffinity sij, representing the probability that the embedded zi belongs to the j-th subspace, is defined\\nas:\\nsij =\\n\\r\\rzT\\ni D(j)\\r\\r2\\nF +ηd\\nP\\nj(\\n\\r\\rzT\\ni D(j)\\n\\r\\r2\\nF +ηd)\\n,(6)\\nwhere η is a parameter controlling the smoothness, fixed to the same value as d. To emphasize more\\nconfident assignments, we introduce a refined subspace affinityˆsij:\\nˆsij = s2\\nij/P\\ni sij\\nP\\nj(s2\\nij/P\\ni sij) .(7)\\nThis refinement sharpens the clustering by weighting high-confidence assignments more. The\\nsubspace clustering objective based on the Kullback-Leibler divergence is:\\nLsub =KL( ˆS∥S) =\\nX\\ni\\nX\\nj\\nˆsijlog ˆsij\\nsij\\n.(8)\\nThe clustering loss is defined as:\\nLP I =R+βL sub,(9)\\nwhere β is a hyperparameter balancing the regularization and subspace clustering terms. A detailed\\nsensitivity analysis ofαandβis presented in Appendix J.\\n3.6 Mixture of Pattern Experts\\nTraditional time series forecasting methods often rely on a uniform distribution modeling (UDM)\\napproach, which struggles to adapt to the complexities of diverse and evolving patterns in real-world\\ndata. To address this limitation, we introduce the Mixture of Pattern Experts module (MoPE), which\\nassigns specialized experts to patches based on their unique underlying patterns, enabling more\\nprecise and adaptive forecasting.\\nGiven the cluster assignments s obtained from the PI module, we apply the Patch-wise MoPE to the\\nfeature tensorz∈R C×N×D . The MoPE module consists of the following key components:\\nGating Network.The gating network G calculates the gating weights for each expert based on the\\ncluster assignmentsand selects the topkexperts. The gating weights are computed as:\\nG(s) =Softmax(TopK(s)).(10)\\nHere, the top k logits are selected and normalized using the Softmax function to produce the gating\\nweights.\\nExpert Networks.The MoPE contains K expert networks, denoted as E1, . . . , EK. Each expert\\nnetwork is modeled as an MLP consisting of two linear layers and a ReLU activation. Given a\\npatch-wise featurez, each expert networkE k processes the input to generate its respective output.\\n6Output Aggregation.The final output h of the MoPE module is a weighted sum of the outputs from\\nall the selected experts, with the weights provided by the gating network:\\nh=\\nKX\\nk=1\\nG(s)Ek(z).(11)\\nAfter the frequency branch is processed by the inverse Fast Fourier transform, the time-frequency\\noutputsh t andh f , are concatenated to formh=concat(h t, hf)∈R C×N×2D .\\nFinally, a linear transformation is applied to the disentangled and pattern-specific representations h to\\ngenerate the prediction: ˆY=Linear(h)∈R H×C .\\nThis approach ensures that the MoPE dynamically assigns and aggregates contributions from various\\nexperts based on evolving patterns, improving the model’s adaptability and accuracy.\\n3.7 Loss Function\\nFollowing [44], we use the Mean Squared Error (MSE) loss to quantify the discrepancy between\\npredicted values ˆY and ground truth values Y : LM SE = ( ˆY−Y) 2. In addition to the MSE loss, we\\nincorporate the clustering regularization loss from the PI module, yielding the final loss function:\\nL=L M SE +L P It +L P If .(12)\\nThis combined loss ensures that the model not only minimizes forecasting errors but also accurately\\nidentifies and maintains the integrity of pattern clusters across time. The algorithm is provided in the\\nAppendix L.\\n4 Experiments\\n4.1 Experimental Setup\\nDatasets and Baselines.We conducted our experiments on nine publicly available real-world\\nmultivariate time series datasets, i.e., ETT (ETTh1, ETTh2, ETTm1, ETTm2), Exchange, Weather,\\nElectricity, Traffic, and ILI. These datasets are provided in [62] for time series forecasting. More\\ndetails about these datasets are included in Appendix A.\\nWe employed a diverse set of state-of-the-art forecasting models as baselines, categorized based on\\nthe type of information they utilize as follows.(1) Time-domain methods:PatchTST [ 44], DLinear\\n[69], TimesNet [61] and iTransformer [29];(2) Frequency-domain methods:FEDformer [ 71] and\\nFITS [64];(3) Time-frequency methods:TFDNet-IK [ 42] and TSLANet [ 9]. We rerun all the\\nexperiments with codes provided by their official implementation.\\nIn addition, we compare TFPS with recent foundation models, including AutoTimes [30], Moment\\n[16], and Timer [32]. We rerun all experiments for a fair comparison: AutoTimes is reproduced using\\nits official implementation, while Moment and Timer are evaluated based on the OpenLTM [32].\\nWe further include comparisons with normalization techniques, MoE-based architectures, and meth-\\nods designed to address distribution shifts. Comprehensive results are presented in Appendix G.\\nExperiments Details.Following previous works [ 44], we used ADAM [ 21] as the default opti-\\nmizer across all the experiments. We employed the MSE and mean absolute error (MAE) as the\\nevaluation metrics, where lower values indicate better performance. A detailed explanation is pro-\\nvided in Appendix E. TFPS was implemented by PyTorch [ 48] and trained on a single NVIDIA\\nRTX 3090 24GB GPU. We conducted grid search to optimize the following three parameters, i.e.,\\nlearning rate={0.0001,0.0005,0.001,0.005,0.01,0.05} , the number of experts in the time domain\\nKt ={1,2,4,8}, and the number of experts in the frequency domainK f ={1,2,4,8}.\\n4.2 Overall Performance Comparison\\nTable 1 highlights the consistent superiority of TFPS across multiple datasets and prediction horizons,\\nsecuring the top performance in 57 out of 72 experimental configurations. In particular, TFPS\\ndemonstrates significant improvements over time-domain methods, with an overall improvement of\\n7Table 1: Multivariate long-term forecasting results with prediction lengthsH∈ {24,36,48,60}for\\nILI and H∈ {96,192,336,720} for others. The input lengths are L= 104 for ILI and L= 96 for\\nothers. The best results are highlighted inboldand the second best are underlined.\\nModelIMP. TFPS TSLANetFITS iTransformerTFDNet-IKPatchTSTTimesNetDLinearFEDformer(Our) (2024) (2024) (2024) (2023) (2023) (2023) (2023) (2022)MetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -1.1%0.398 0.4130.387 0.4050.3950.4030.3870.4050.396 0.4090.413 0.4190.389 0.4120.398 0.4100.3850.4251924.8%0.423 0.4230.448 0.4360.445 0.4320.441 0.4360.451 0.4410.460 0.4450.441 0.4420.4340.4270.441 0.4613361.8%0.484 0.4610.491 0.4870.489 0.4630.491 0.4630.4950.4620.497 0.4630.491 0.4670.499 0.4770.491 0.4737203.0%0.488 0.4760.505 0.4860.496 0.4850.509 0.4940.4920.4820.501 0.4860.512 0.4910.508 0.5030.501 0.499\\nETTh2\\n96 -2.0%0.313 0.3550.290 0.3450.2950.3440.301 0.3500.289 0.3370.299 0.3480.324 0.3680.315 0.3740.342 0.383192-2.9%0.405 0.4100.362 0.3910.382 0.3960.380 0.3990.3790.3950.383 0.3980.393 0.4100.432 0.4470.434 0.44033610.5%0.392 0.4150.4010.4190.416 0.4250.424 0.4320.416 0.4220.424 0.4310.429 0.4370.486 0.4810.512 0.49772012.6%0.410 0.4330.419 0.4390.4180.4370.430 0.4470.424 0.4410.429 0.4450.433 0.4480.732 0.6140.467 0.476\\nETTm1\\n96 4.1%0.327 0.3670.3290.3680.354 0.3750.342 0.3770.331 0.3690.331 0.3700.337 0.3770.346 0.3740.360 0.4061922.6%0.3740.3950.3760.3830.392 0.3930.383 0.3960.3760.3810.374 0.3950.395 0.4060.382 0.3920.395 0.4273364.2%0.401 0.4080.403 0.4140.425 0.4150.418 0.4180.4050.4100.402 0.4120.433 0.4320.414 0.4140.448 0.458720-0.7%0.479 0.4560.4450.4380.486 0.4490.487 0.4570.4710.4370.466 0.4460.484 0.4580.478 0.4550.491 0.479\\nETTm2\\n96 6.9%0.170 0.2550.179 0.2610.183 0.2660.186 0.2720.176 0.2670.1770.2600.182 0.2620.184 0.2760.193 0.2851927.1%0.235 0.2960.243 0.3030.247 0.3050.254 0.3140.2450.3020.248 0.3060.252 0.3070.282 0.3570.256 0.3243364.6%0.297 0.3350.308 0.3450.307 0.3420.316 0.3510.3030.3400.303 0.3410.312 0.3460.324 0.3640.321 0.3647203.6%0.401 0.3970.403 0.4000.407 0.4010.414 0.4070.4050.3990.405 0.4030.417 0.4040.441 0.4540.434 0.426\\nExchange\\n96 12.7%0.083 0.2050.085 0.2060.088 0.2100.086 0.2060.0840.2050.089 0.2060.105 0.2330.089 0.2190.136 0.26519211.2%0.174 0.2970.178 0.3000.181 0.3040.181 0.3040.1760.2990.178 0.3020.219 0.3420.180 0.3190.279 0.38433610.4%0.310 0.3980.329 0.4150.324 0.4130.338 0.4220.3210.4090.326 0.4110.353 0.4330.313 0.4230.465 0.504720-13.3%1.011 0.7560.850 0.6930.846 0.6960.853 0.6960.835 0.6890.840 0.6900.912 0.7240.8370.6901.169 0.826\\nWeather\\n96 15.6%0.154 0.2020.176 0.2160.167 0.2140.176 0.2160.1650.2090.177 0.2190.168 0.2180.197 0.2570.236 0.32519210.6%0.205 0.2490.226 0.2580.215 0.2570.225 0.2570.2140.2520.225 0.2590.226 0.2670.237 0.2940.268 0.3373369.1%0.262 0.2890.279 0.2990.270 0.2990.281 0.2990.2670.2980.278 0.2980.283 0.3050.283 0.3320.366 0.4027204.1%0.344 0.3420.355 0.3550.3470.3450.358 0.3500.347 0.3460.351 0.3460.355 0.3530.347 0.3820.407 0.422\\nElectricity\\n96 14.6%0.149 0.2360.155 0.2490.200 0.2780.1510.2410.171 0.2540.166 0.2520.168 0.2720.195 0.2770.189 0.30419212.0%0.162 0.2530.170 0.2640.200 0.2810.1670.2580.189 0.2690.174 0.2610.186 0.2890.194 0.2810.198 0.3123360.2%0.200 0.3100.197 0.2820.214 0.2950.179 0.2710.205 0.2840.1900.2770.197 0.2980.207 0.2960.212 0.3267207.2%0.2200.3200.2240.3180.256 0.3280.229 0.3190.247 0.3180.2300.3120.225 0.3220.243 0.3300.242 0.351\\nTraffic\\n96 21.1%0.4270.2960.475 0.3070.651 0.3880.4280.2950.519 0.3140.446 0.2840.586 0.3160.650 0.3970.575 0.35719217.7%0.445 0.2980.478 0.3060.603 0.3640.4480.3020.513 0.3140.453 0.2850.618 0.3230.600 0.3720.613 0.38133617.0%0.459 0.3070.494 0.3120.610 0.3660.4650.3110.525 0.3190.467 0.2910.634 0.3370.606 0.3740.622 0.38072015.1%0.496 0.3130.528 0.3310.648 0.3870.5010.3330.561 0.3360.501 0.4920.659 0.3490.646 0.3960.630 0.383\\nILIILI\\n24 40.9%1.349 0.7601.749 0.8983.489 1.3732.443 1.0781.8240.8241.614 0.8351.699 0.8712.239 1.0413.217 1.24636 43.6%1.239 0.7521.754 0.9123.530 1.3702.455 1.0861.6990.8131.475 0.8591.733 0.9132.238 1.0492.688 1.07448 40.4%1.461 0.8012.050 0.9843.671 1.3913.437 1.3311.7620.8311.642 0.8802.272 0.9992.252 1.0642.540 1.05760 39.8%1.458 0.8362.240 1.0394.030 1.4622.734 1.1551.7580.8631.608 0.8851.998 0.9742.236 1.0572.782 1.1361stCount 57 3 1 3 6 1 0 0 1\\nTable 2: Ablation study of TFPS components. The model variants in our ablation study include the\\nfollowing configurations across both time and frequency branches: (a) inclusion of the encoder, PI\\nand MoPE; (b) PI replaced with Linear; (c) only the encoder. The best results are inbold.\\nTime Branch Frequency Branch ETTh1 ETTh2\\nEncoder PI MoPE Encoder PI MoPE 96 192 336 720 96 192 336 720\\n✓ ✓ ✓ ✓ ✓ ✓ 0.398 0.423 0.484 0.488 0.313 0.405 0.392 0.410\\n✓ ✓ ✓ 0.401 0.459 0.486 0.492 0.318 0.409 0.400 0.428\\n✓Linear✓ 0.401 0.451 0.494 0.509 0.325 0.411 0.400 0.434\\n✓ 0.414 0.460 0.501 0.500 0.339 0.411 0.426 0.431\\n✓ ✓ ✓ 0.455 0.507 0.539 0.576 0.324 0.407 0.417 0.436\\n✓Linear✓ 0.503 0.535 0.558 0.583 0.398 0.446 0.457 0.444\\n✓ 0.552 0.583 0.591 0.594 0.371 0.426 0.418 0.463\\n9.5% in MSE and 6.4% in MAE. Compared to frequency-domain methods, TFPS shows even more\\npronounced enhancements, with MSE improved by 16.9% and MAE by 12.4%.\\nWhile the time-frequency methods like TSLANet and TFDNet perform competitively on several\\ndatasets, TFPS still outperforms them, showing improvement of 5.2% in MSE and 2.2% in MAE.\\nThese substantial improvements can be attributed to the integration of both time- and frequency-\\ndomain information, combined with our innovative approach to modeling distinct patterns with\\nspecialized experts. By addressing the underlying concept shifts and capturing complex, evolving\\npatterns in time series data, TFPS achieves more accurate predictions than other baselines.\\n8Table 3: Compared with foundation models.\\nModelIMP. TFPS AutoTimesMoment Timer(Our) (2024) (2024) (2024)\\nMetricMSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh10.2%0.4010.4120.396 0.4280.415 0.4390.3940.417ETTh210.9%0.335 0.3860.3630.4060.381 0.4120.382 0.418ETTm12.4%0.343 0.3740.364 0.3890.348 0.3860.3440.378ETTm27.3%0.248 0.3080.273 0.3270.265 0.3250.2640.321Traffic-3.6%0.398 0.2680.3790.2650.395 0.2730.3790.255Electricity3.7%0.159 0.2490.168 0.2610.163 0.2630.1650.258\\nTable 4: Compared with normalization methods.\\nModel IMP. TFPS DLinear\\nSIN SAN Dish-TS RevIN\\nETTh11.5% 0.448 0.454 0.456 0.461 0.451ETTh22.4% 0.380 0.386 0.388 0.392 0.390ETTm12.3% 0.395 0.4050.399 0.406 0.409ETTm23.3% 0.276 0.2830.280 0.293 0.284Weather5.3% 0.241 0.2530.249 0.263 0.254\\n4.3 Ablation Study\\nTable 2 presents the MSE results of TFPS and its variants with different combinations of encoders,\\nPI, and MoPE.1) Best Result.The full TFPS model, i.e., both the time and frequency branches,\\nalong with their respective encoders, PI, and MoPE are included, performs the best across all the\\nforecast horizons for both datasets.2) Linear vs. PI.We replace PI with a linear layer and find that it\\ngenerally results in higher MSE in most cases, indicating that accurately capturing specific patterns\\nis crucial.3) Impact of Pattern-aware Modeling.Additionally, when comparing the results with\\nthe encoder-only configuration, two variants with MoPE in each branch achieved improved MSE,\\nfurther supporting the necessity of patter-aware modeling.4) Importance of DDE.Furthermore, we\\nfind that both the time encoder and frequency encoder alone yield worse performance, with the time\\nencoder playing a more significant role. In summary, incorporating both branches with PI and MoPE\\nprovides the best performance, while simpler configurations result in higher MSE. See Appendix K\\nfor an in-depth analysis of each component’s contribution.\\n4.4 Comparsion with Foundation Models\\nTo ensure a fair comparison with foundation models, we searched input lengths among 96, 192, 336,\\nand 512. The average results across all forecasting lengths are included in Table 3, with detailed\\nresults provided in Appendix F.2. As shown in Table 3, TFPS consistently outperforms recent\\nfoundation models. Notably, on challenging datasets such as ETTh2 and ETTm2, TFPS achieves\\nsubstantial improvements in MSE by 10.9% and 7.3%, respectively. Although TFPS performs slightly\\nworse on the Traffic dataset, we attribute this to the relatively mild distribution shift observed in\\nTraffic (see Table 5), which may reduce the benefit of our pattern-specific modeling. These results\\nsuggest that TFPS not only matches but often surpasses large-scale foundation models in forecasting\\naccuracy, benefiting from its expert-based design that explicitly captures distributional heterogeneity.\\n4.5 Comparsion with Normalization Methods\\nNormalization methods can reduce fluctuations to enhance performance and are widely used for\\nnon-stationary time series forecasting [17, 34, 10, 31, 20]. We compare our TFPS with these state-of-\\nthe-art normalization methods and Table 4 presents the average MSE across all forecasting lengths for\\neach dataset. While normalization improves stability by enforcing distributional consistency, TFPS\\nretains the intrinsic non-stationarity and models diverse patterns through distribution-specific experts,\\nachieving better adaptability and forecasting accuracy. Detailed results are provided in Appendix G.1.\\n4.6 Visualization\\nWe visualize the prediction curves for ETTh1 withH= 192 . Given that DLinear exhibits competitive\\nperformance in Table 1, we compare its results with those of TFPS in Figure 4 under two scenarios:\\n(a) sudden drift caused by external factors or random events, and (b) gradual drift where the trend is\\ndominant. It is evident that DLinear struggles to achieve accurate predictions in both scenarios. In\\ncontrast, our TFPS consistently produces accurate forecasts despite these challenges, demonstratingcontrast, our TFPS consistently produces accurate forecasts despite these challenges, demonstrating\\nits robustness in dealing with various concept dynamics.\\n4.7 Analysis of Experts\\nQualitative Visualizations of Pattern Identifier.Through training, pattern experts in MoPE\\nspontaneously specialize, and we present two examples in Figure 5. We visualize the expert with the\\nhighest score as the routed expert for each instance pair. In the provided examples, we observe that\\n9(a) Sudden Drift\\n (b) Gradual Drift\\nFigure 4: Visualizations of DLinear and TFPS on the ETTh1 dataset whenH= 192.\\n(a) Expert 0\\n (b) Expert 4\\nFigure 5: Interpretable patterns via PI. Expert-0\\nspecializes in downward trends, while Expert-4\\nfocuses on parabolic trends.\\n(a) ETTh1\\n (b) ETTh2\\nFigure 6: Experiments on the number of experts\\nwhen H= 96 . Further analysis of expert be-\\nhavior is provided in Appendix H.\\nexpert-0 specialize in downward-related concepts, while expert-4 focuses on parabolic trend. These\\nexamples also demonstrate the interpretability of MoPE.\\nNumber of Experts.In Figure 6, we set the learning rate to 0.0001 and conducted four sets of\\nexperiments on the ETTh1 and Weather datasets, Kt = 1, Kf ={1,2,4,8} , to explore the effect of\\nthe number of frequency experts on the results. For example, Kt1Kf 4 means that the TFPS contains\\n1 time experts and 4 frequency experts. We observed that Kt1Kf 2 outperformed Kt1Kf 4 in both\\ncases, suggesting that increasing the number of experts does not always lead to better performance.\\nIn addition, we conducted three experiments based on the optimal number of frequency experts to\\nverify the impact of varying the number of time experts on the results. As shown in Figure 6, the\\nbest results for ETTh1 were obtained with Kt4Kf 2, while for Weather, the optimal results were\\nachieved with Kt4Kf 8. Combined with the average Wasserstein distance in Table 5, we attribute\\nthis to the fact that, in cases where concept drift is more severe, such as Weather, more experts are\\nneeded, whereas fewer experts are sufficient when the drift is less severe.\\n5 Conclusion\\nIn this paper, we propose a novel pattern-aware time series forecasting framework, TFPS, which\\nincorporates a dual-domain mixture of pattern experts approach. Our TFPS framework aims to\\naddress the distribution shift across time series patches and effectively assigns pattern-specific experts\\nto model them. Experimental results across eight diverse datasets demonstrate that TFPS surpasses\\nstate-of-the-art methods in both quantitative metrics and visualizations. Future work will focus on\\ninvestigating evolving distribution shifts, particularly those introduced by the emergence of new\\npatterns, such as unforeseen epidemics or outbreaks.\\n6 Acknowledgments and Disclosure of Funding\\nThis work was supported in part by the National Natural Science Foundation of China under Grants\\nU23B2049, 62376194, 61925602, 62406219, and 62436001; in part by the China Postdoctoral\\nScience Foundation - Tianjin Joint Support Program under Grant 2023T014TJ; and in part by the\\nChina Scholarship Council under Grant 202406250137.\\n10References\\n[1] Khaled Alkilane, Yihang He, and Der-Horng Lee. Mixmamba: Time series modeling with\\nadaptive expertise.Information Fusion, 112:102589, 2024.\\n[2] Quentin Gregory Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba:\\nMixture of experts for state-space models. InICLR 2024 Workshop on Mathematical and\\nEmpirical Understanding of Foundation Models, 2024.\\n[3] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate\\nmedium-range global weather forecasting with 3d neural networks.Nature, 619(7970):533–538,\\n2023.\\n[4] Yushu Chen, Shengzhuo Liu, Jinzhe Yang, Hao Jing, Wenlai Zhao, and Guangwen Yang. A joint\\ntime-frequency domain transformer for multivariate time series forecasting.Neural Networks,\\n176:106334, 2024.\\n[5] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li,\\nWangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization\\nin mixture-of-experts language models.arXiv preprint arXiv:2401.06066, 2024.\\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. InProceedings of the 2019 conference of\\nthe North American chapter of the association for computational linguistics: human language\\ntechnologies, volume 1 (long and short papers), pages 4171–4186, 2019.\\n[7] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\\nlanguage models with mixture-of-experts. InInternational Conference on Machine Learning,\\npages 5547–5569. PMLR, 2022.\\n[8] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.\\nTsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. InProceedings\\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages\\n459–469, 2023.\\n[9] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. TSLANet:\\nRethinking transformers for time series representation learning. InForty-first International\\nConference on Machine Learning, 2024.\\n[10] Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, and Yanjie Fu.\\nDish-ts: a general paradigm for alleviating distribution shift in time series forecasting. In\\nProceedings of the AAAI conference on artificial intelligence, volume 37, pages 7522–7529,\\n2023.\\n[11] Wei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Jiang Bian, and Yanjie Fu. Addressing\\ndistribution shift in time series forecasting with instance normalization flows.arXiv preprint\\narXiv:2401.16777, 2024.\\n[12] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efficient sparsity.Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\n[13] Yunpeng Gong, Liqing Huang, and Lifei Chen. Eliminate deviation with deviation for data\\naugmentation and a general multi-modal data learning method.arXiv preprint arXiv:2101.08533,\\n2021.\\n[14] Yunpeng Gong, Liqing Huang, and Lifei Chen. Person re-identification method based on color\\nattack and joint defence. InProceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 4313–4322, 2022.\\n[15] Yunpeng Gong, Zhun Zhong, Yansong Qu, Zhiming Luo, Rongrong Ji, and Min Jiang. Cross-\\nmodality perturbation synergy attack for person re-identification.Advances in Neural Informa-\\ntion Processing Systems, 37:23352–23377, 2024.\\n11[16] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.\\nMoment: A family of open time-series foundation models. InInternational Conference on\\nMachine Learning, pages 16115–16152. PMLR, 2024.\\n[17] Lu Han, Han-Jia Ye, and De-Chuan Zhan. SIN: Selective and interpretable normalization for\\nlong-term time series forecasting. InForty-first International Conference on Machine Learning,\\n2024.\\n[18] Hongbin Huang, Minghua Chen, and Xiao Qiao. Generative learning for financial time series\\nwith irregular and scale-invariant patterns. InThe Twelfth International Conference on Learning\\nRepresentations, 2024.\\n[19] Yushan Jiang, Kanghui Ning, Zijie Pan, Xuyang Shen, Jingchao Ni, Wenchao Yu, Anderson\\nSchneider, Haifeng Chen, Yuriy Nevmyvaka, and Dongjin Song. Multi-modal time series\\nanalysis: A tutorial and survey. InProceedings of the 31st ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining V . 2, pages 6043–6053, 2025.\\n[20] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.\\nReversible instance normalization for accurate time-series forecasting against distribution shift.\\nInInternational Conference on Learning Representations, 2021.\\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Weiyang Kong, Ziyu Guo, and Yubao Liu. Spatio-temporal pivotal graph neural networks\\nfor traffic flow forecasting. InProceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 38, pages 8627–8635, 2024.\\n[23] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term\\ntemporal patterns with deep neural networks. InThe 41st international ACM SIGIR conference\\non research & development in information retrieval, pages 95–104, 2018.\\n[24] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,\\nFerran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning\\nskillful medium-range global weather forecasting.Science, 382(6677):1416–1421, 2023.\\n[25] Sanghyun Lee and Chanyoung Park. Continual traffic forecasting via mixture of experts.arXiv\\npreprint arXiv:2406.03140, 2024.\\n[26] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens\\nwith fourier transforms. InProceedings of the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, pages\\n4296–4313, 2022.\\n[27] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An\\ninvestigation on linear mapping.arXiv preprint arXiv:2305.10721, 2023.\\n[28] Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, and Bin Yang.\\nRethinking irregular time series forecasting: A simple yet effective baseline.arXiv preprint\\narXiv:2505.11250, 2025.\\n[29] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng\\nLong. itransformer: Inverted transformers are effective for time series forecasting. InThe\\nTwelfth International Conference on Learning Representations, 2024.\\n[30] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Autotimes: Au-\\ntoregressive time series forecasters via large language models.Advances in Neural Information\\nProcessing Systems, 37:122154–122184, 2024.\\n[31] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:\\nExploring the stationarity in time series forecasting.Advances in Neural Information Processing\\nSystems, 35:9881–9893, 2022.\\n12[32] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long.\\nTimer: generative pre-trained transformers are large time series models. InProceedings of the\\n41st International Conference on Machine Learning, pages 32369–32399, 2024.\\n[33] Zhanyu Liu, Guanjie Zheng, and Yanwei Yu. Cross-city few-shot traffic forecasting via traffic\\npattern bank. InProceedings of the 32nd ACM International Conference on Information and\\nKnowledge Management, pages 1451–1460, 2023.\\n[34] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and Enhong Chen.\\nAdaptive normalization for non-stationary time series forecasting: A temporal slice perspective.\\nAdvances in Neural Information Processing Systems, 36, 2023.\\n[35] Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, and Yuanchun Zhou. Un-\\nveiling delay effects in traffic forecasting: A perspective from spatial-temporal delay differential\\nequations. InProceedings of the ACM on Web Conference 2024, pages 1035–1044, 2024.\\n[36] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under\\nconcept drift: A review.IEEE transactions on knowledge and data engineering, 31(12):2346–\\n2363, 2018.\\n[37] Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, and Adams Wai-Kin\\nKong. Does flux already know how to perform physically plausible image composition?arXiv\\npreprint arXiv:2509.21278, 2025.\\n[38] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free\\ncross-domain image composition. InProceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 2294–2305, 2023.\\n[39] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept\\nerasure in diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 6430–6440, 2024.\\n[40] Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust water-\\nmarking using generative priors against image editing: From benchmarking to advances.arXiv\\npreprint arXiv:2410.18775, 2024.\\n[41] Donghao Luo and Xue Wang. Moderntcn: A modern pure convolution structure for general\\ntime series analysis. InThe Twelfth International Conference on Learning Representations,\\n2024.\\n[42] Yuxiao Luo, Ziyu Lyu, and Xingyu Huang. Tfdnet: Time-frequency enhanced decomposed\\nnetwork for long-term time series forecasting.arXiv preprint arXiv:2308.13386, 2023.\\n[43] Ronghao Ni, Zinan Lin, Shuaiqi Wang, and Giulia Fanti. Mixture-of-linear-experts for long-\\nterm time series forecasting. InInternational Conference on Artificial Intelligence and Statistics,\\npages 4672–4680. PMLR, 2024.\\n[44] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth\\n64 words: Long-term forecasting with transformers. InThe Eleventh International Conference\\non Learning Representations, 2023.\\n[45] Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y Zhang, Kashif Rasul, Anderson\\nSchneider, Lintao Ma, Yuriy Nevmyvaka, and Dongjin Song. Ts-rag: Retrieval-augmented\\ngeneration based time series foundation models are stronger zero-shot forecaster.arXiv preprint\\narXiv:2503.07649, 2025.\\n[46] Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbrão, Gisele L\\nPappa, and Marta Mattoso. Adaptive normalization: A novel data normalization approach for\\nnon-stationary time series. InThe 2010 International Joint Conference on Neural Networks\\n(IJCNN), pages 1–8. IEEE, 2010.\\n[47] Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros\\nIosifidis. Deep adaptive input normalization for time series forecasting.IEEE transactions on\\nneural networks and learning systems, 31(9):3760–3765, 2019.\\n13[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\\nstyle, high-performance deep learning library.Advances in neural information processing\\nsystems, 32, 2019.\\n[49] Maciej Pióro, Kamil Ciebiera, Krystian Król, Jan Ludziejewski, Michał Krutul, Jakub Krajewski,\\nSzymon Antoniak, Piotr Miło´s, Marek Cygan, and Sebastian Jaszczur. Moe-mamba: Efficient\\nselective state space models with mixture of experts. InICLR 2024 Workshop on Mathematical\\nand Empirical Understanding of Foundation Models, 2024.\\n[50] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft\\nmixtures of experts.arXiv preprint arXiv:2308.00951, 2023.\\n[51] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo,\\nAoying Zhou, Christian S. Jensen, Zhenli Sheng, and Bin Yang. Tfb: Towards comprehensive\\nand fair benchmarking of time series forecasting methods.Proc. VLDB Endow., 17(9):2363–\\n2377, 2024.\\n[52] Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, and Bin Yang. Duet: Dual\\nclustering enhanced multivariate time series forecasting. InProceedings of the 31st ACM\\nSIGKDD Conference on Knowledge Discovery and Data Mining, pages 1185–1196, 2025.\\n[53] Xiangfei Qiu, Yuhan Zhu, Zhengyu Li, Hanyin Cheng, Xingjian Wu, Chenjuan Guo, Bin Yang,\\nand Jilin Hu. Dag: A dual causal network for time series forecasting with exogenous variables.\\narXiv preprint arXiv:2509.14933, 2025.\\n[54] Aaditya Ramdas, Nicolás García Trillos, and Marco Cuturi. On wasserstein two-sample testing\\nand related families of nonparametric tests.Entropy, 19(2):47, 2017.\\n[55] Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer\\nthe embedding space for metric learning. InProceedings of the ieee/cvf conference on computer\\nvision and pattern recognition, pages 471–480, 2019.\\n[56] Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao, Tao\\nSun, Guangyin Jin, Xin Cao, et al. Exploring progress in multivariate time series forecasting:\\nComprehensive benchmarking and heterogeneity analysis.IEEE Transactions on Knowledge\\nand Data Engineering, 2024.\\n[57] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\\nexperts layer. InInternational Conference on Learning Representations, 2017.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information\\nprocessing systems, 30, 2017.\\n[59] Hao Wang, Zhiyu Wang, Yunlong Niu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, and\\nXinggao Liu. An accurate and interpretable framework for trustworthy process monitoring.\\nIEEE Transactions on Artificial Intelligence, 2023.\\n[60] Qingsong Wen, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan, et al.\\nOnenet: Enhancing time series forecasting models under concept drift by online ensembling.\\nAdvances in Neural Information Processing Systems, 36:69949–69980, 2023.\\n[61] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:\\nTemporal 2d-variation modeling for general time series analysis. InThe Eleventh International\\nConference on Learning Representations, 2023.\\n[62] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-\\nformers with auto-correlation for long-term series forecasting.Advances in neural information\\nprocessing systems, 34:22419–22430, 2021.\\n14[63] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting\\nfor worldwide stations with a unified deep model.Nature Machine Intelligence, 5(6):602–611,\\n2023.\\n[64] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: Modeling time series with $10k$ parameters. In\\nThe Twelfth International Conference on Learning Representations, 2024.\\n[65] Kaiwen Yan, Chen Long, Huisi Wu, and Zhenkun Wen. Multi-resolution expansion of analysis\\nin time-frequency domain for time series forecasting.IEEE Transactions on Knowledge and\\nData Engineering, 2024.\\n[66] Zhangjing Yang, Weiwu Yan, Xiaolin Huang, and Lin Mei. Adaptive temporal-frequency\\nnetwork for time-series forecasting.IEEE Transactions on Knowledge and Data Engineering,\\n34(4):1576–1587, 2020.\\n[67] Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Zhulin An,\\nQi Wang, and Yongjun Xu. Ginar+: A robust end-to-end framework for multivariate time series\\nforecasting with missing values.IEEE Transactions on Knowledge and Data Engineering,\\n2025.\\n[68] Chengqing Yu, Fei Wang, Yilun Wang, Zezhi Shao, Tao Sun, Di Yao, and Yongjun Xu. Mgsf-\\nformer: A multi-granularity spatiotemporal fusion transformer for air quality prediction.Infor-\\nmation Fusion, 113:102607, 2025.\\n[69] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\nforecasting? InProceedings of the AAAI conference on artificial intelligence, volume 37, pages\\n11121–11128, 2023.\\n[70] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\\nProceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106–11115,\\n2021.\\n[71] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\\nFrequency enhanced decomposed transformer for long-term series forecasting. InInternational\\nconference on machine learning, pages 27268–27286. PMLR, 2022.\\n15NeurIPS Paper Checklist\\n1.Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\\npaper’s contributions and scope?\\nAnswer: [Yes]\\nJustification: The main claims in the abstract and introduction accurately reflect the contribu-\\ntions and scope of the paper. They are supported by the proposed methodology, theoretical\\nmotivation, and extensive empirical results across diverse datasets.\\nGuidelines:\\n• The answer NA means that the abstract and introduction do not include the claims\\nmade in the paper.\\n• The abstract and/or introduction should clearly state the claims made, including the\\ncontributions made in the paper and important assumptions and limitations. A No or\\nNA answer to this question will not be perceived well by the reviewers.\\n• The claims made should match theoretical and experimental results, and reflect how\\nmuch the results can be expected to generalize to other settings.\\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\\nare not attained by the paper.\\n2.Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes]\\nJustification: The paper discusses the limitations of the proposed method in Section N,\\nincluding its assumptions about fixed patch length and potential challenges under evolving\\ndistributions.\\nGuidelines:\\n• The answer NA means that the paper has no limitation while the answer No means that\\nthe paper has limitations, but those are not discussed in the paper.\\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\\n• The paper should point out any strong assumptions and how robust the results are to\\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\\nmodel well-specification, asymptotic approximations only holding locally). The authors\\nshould reflect on how these assumptions might be violated in practice and what the\\nimplications would be.\\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\\nonly tested on a few datasets or with a few runs. In general, empirical results often\\ndepend on implicit assumptions, which should be articulated.\\n• The authors should reflect on the factors that influence the performance of the approach.\\nFor example, a facial recognition algorithm may perform poorly when image resolution\\nis low or images are taken in low lighting. Or a speech-to-text system might not be\\nused reliably to provide closed captions for online lectures because it fails to handle\\ntechnical jargon.\\n• The authors should discuss the computational efficiency of the proposed algorithms\\nand how they scale with dataset size.\\n• If applicable, the authors should discuss possible limitations of their approach to\\naddress problems of privacy and fairness.\\n• While the authors might fear that complete honesty about limitations might be used by\\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\\nlimitations that aren’t acknowledged in the paper. The authors should use their best\\njudgment and recognize that individual actions in favor of transparency play an impor-\\ntant role in developing norms that preserve the integrity of the community. Reviewers\\nwill be specifically instructed to not penalize honesty concerning limitations.\\n3.Theory assumptions and proofs\\n16Question: For each theoretical result, does the paper provide the full set of assumptions and\\na complete (and correct) proof?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not include theoretical results.\\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\\nreferenced.\\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\\n• The proofs can either appear in the main paper or the supplemental material, but if\\nthey appear in the supplemental material, the authors are encouraged to provide a short\\nproof sketch to provide intuition.\\n• Inversely, any informal proof provided in the core of the paper should be complemented\\nby formal proofs provided in appendix or supplemental material.\\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4.Experimental result reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: The paper provides sufficient details regarding the model architecture, training\\nsettings, data preprocessing, and evaluation metrics, enabling reproduction of the main\\nexperimental results and verification of the core claims.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• If the paper includes experiments, a No answer to this question will not be perceived\\nwell by the reviewers: Making the paper reproducible is important, regardless of\\nwhether the code and data are provided or not.\\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\\nto make their results reproducible or verifiable.\\n• Depending on the contribution, reproducibility can be accomplished in various ways.\\nFor example, if the contribution is a novel architecture, describing the architecture fully\\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\\nbe necessary to either make it possible for others to replicate the model with the same\\ndataset, or provide access to the model. In general. releasing code and data is often\\none good way to accomplish this, but reproducibility can also be provided via detailed\\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\\nof a large language model), releasing of a model checkpoint, or other means that are\\nappropriate to the research performed.\\n• While NeurIPS does not require releasing code, the conference does require all submis-\\nsions to provide some reasonable avenue for reproducibility, which may depend on the\\nnature of the contribution. For example\\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\\nto reproduce that algorithm.\\n(b) If the contribution is primarily a new model architecture, the paper should describe\\nthe architecture clearly and fully.\\n(c) If the contribution is a new model (e.g., a large language model), then there should\\neither be a way to access this model for reproducing the results or a way to reproduce\\nthe model (e.g., with an open-source dataset or instructions for how to construct\\nthe dataset).\\n(d) We recognize that reproducibility may be tricky in some cases, in which case\\nauthors are welcome to describe the particular way they provide for reproducibility.\\nIn the case of closed-source models, it may be that access to the model is limited in\\nsome way (e.g., to registered users), but it should be possible for other researchers\\nto have some path to reproducing or verifying the results.\\n175.Open access to data and code\\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\\ntions to faithfully reproduce the main experimental results, as described in supplemental\\nmaterial?\\nAnswer: [Yes]\\nJustification: The source code is included in the supplementary materials to facilitate\\nreproducibility and further research.\\nGuidelines:\\n• The answer NA means that paper does not include experiments requiring code.\\n• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\\npublic/guides/CodeSubmissionPolicy) for more details.\\n• While we encourage the release of code and data, we understand that this might not be\\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\\nincluding code, unless this is central to the contribution (e.g., for a new open-source\\nbenchmark).\\n• The instructions should contain the exact command and environment needed to run to\\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n• The authors should provide instructions on data access and preparation, including how\\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n• The authors should provide scripts to reproduce all experimental results for the new\\nproposed method and baselines. If only a subset of experiments are reproducible, they\\nshould state which ones are omitted from the script and why.\\n• At submission time, to preserve anonymity, the authors should release anonymized\\nversions (if applicable).\\n• Providing as much information as possible in supplemental material (appended to the\\npaper) is recommended, but including URLs to data and code is permitted.\\n6.Experimental setting/details\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\\nresults?\\nAnswer: [Yes]\\nJustification: Training and testing details are described in Appendix A. The hyperparameter\\nsearch ranges and sensitivity analysis are provided in Appendix J.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The experimental setting should be presented in the core of the paper to a level of detail\\nthat is necessary to appreciate the results and make sense of them.\\n• The full details can be provided either with the code, in appendix, or as supplemental\\nmaterial.\\n7.Experiment statistical significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiments?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\\ndence intervals, or statistical significance tests, at least for the experiments that support\\nthe main claims of the paper.\\n• The factors of variability that the error bars are capturing should be clearly stated (for\\nexample, train/test split, initialization, random drawing of some parameter, or overall\\nrun with given experimental conditions).\\n18• The method for calculating the error bars should be explained (closed form formula,\\ncall to a library function, bootstrap, etc.)\\n• The assumptions made should be given (e.g., Normally distributed errors).\\n• It should be clear whether the error bar is the standard deviation or the standard error\\nof the mean.\\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\\nof Normality of errors is not verified.\\n• For asymmetric distributions, the authors should be careful not to show in tables or\\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\\nerror rates).\\n• If error bars are reported in tables or plots, The authors should explain in the text how\\nthey were calculated and reference the corresponding figures or tables in the text.\\n8.Experiments compute resources\\nQuestion: For each experiment, does the paper provide sufficient information on the com-\\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\\nthe experiments?\\nAnswer: [Yes]\\nJustification: The paper provides sufficient details on the computational resources used,\\nincluding the type of GPU, memory, and training time, in Section 4.1. This information\\nallows readers to estimate the resources required for reproduction.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\\nor cloud provider, including relevant memory and storage.\\n• The paper should provide the amount of compute required for each of the individual\\nexperimental runs as well as estimate the total compute.\\n• The paper should disclose whether the full research project required more compute\\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\\ndidn’t make it into the paper).\\n9.Code of ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the\\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: We have read the NeurIPS Code of Ethics, and we are certain that the paper\\nconform to it.\\nGuidelines:\\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n• If the authors answer No, they should explain the special circumstances that require a\\ndeviation from the Code of Ethics.\\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\\neration due to laws or regulations in their jurisdiction).\\n10.Broader impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative\\nsocietal impacts of the work performed?\\nAnswer: [Yes]\\nJustification: The paper discusses both potential positive and negative societal impacts of\\nthe proposed method in Appendix M.\\nGuidelines:\\n• The answer NA means that there is no societal impact of the work performed.\\n19• If the authors answer NA or No, they should explain why their work has no societal\\nimpact or why the paper does not address societal impact.\\n• Examples of negative societal impacts include potential malicious or unintended uses\\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\\ngroups), privacy considerations, and security considerations.\\n• The conference expects that many papers will be foundational research and not tied\\nto particular applications, let alone deployments. However, if there is a direct path to\\nany negative applications, the authors should point it out. For example, it is legitimate\\nto point out that an improvement in the quality of generative models could be used to\\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\\nthat a generic algorithm for optimizing neural networks could enable people to train\\nmodels that generate Deepfakes faster.\\n• The authors should consider possible harms that could arise when the technology is\\nbeing used as intended and functioning correctly, harms that could arise when the\\ntechnology is being used as intended but gives incorrect results, and harms following\\nfrom (intentional or unintentional) misuse of the technology.\\n• If there are negative societal impacts, the authors could also discuss possible mitigation\\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\\nfeedback over time, improving the efficiency and accessibility of ML).\\n11.Safeguards\\nQuestion: Does the paper describe safeguards that have been put in place for responsible\\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\\nimage generators, or scraped datasets)?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper poses no such risks.\\n• Released models that have a high risk for misuse or dual-use should be released with\\nnecessary safeguards to allow for controlled use of the model, for example by requiring\\nthat users adhere to usage guidelines or restrictions to access the model or implementing\\nsafety filters.\\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\\nshould describe how they avoided releasing unsafe images.\\n• We recognize that providing effective safeguards is challenging, and many papers do\\nnot require this, but we encourage authors to take this into account and make a best\\nfaith effort.\\n12.Licenses for existing assets\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected?\\nAnswer: [Yes]\\nJustification: All external assets used in the paper, including datasets and code, are properly\\ncredited. Their licenses and terms of use are explicitly acknowledged and respected.\\nGuidelines:\\n• The answer NA means that the paper does not use existing assets.\\n• The authors should cite the original paper that produced the code package or dataset.\\n• The authors should state which version of the asset is used and, if possible, include a\\nURL.\\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n• For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n20• If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com/datasets\\nhas curated licenses for some datasets. Their licensing guide can help determine the\\nlicense of a dataset.\\n• For existing datasets that are re-packaged, both the original license and the license of\\nthe derived asset (if it has changed) should be provided.\\n• If this information is not available online, the authors are encouraged to reach out to\\nthe asset’s creators.\\n13.New assets\\nQuestion: Are new assets introduced in the paper well documented and is the documentation\\nprovided alongside the assets?\\nAnswer: [Yes]\\nJustification: The new assets introduced in this paper, including the implementation code\\nand model components, are well documented. Documentation is provided alongside the\\ncode in the supplementary materials.\\nGuidelines:\\n• The answer NA means that the paper does not release new assets.\\n• Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n• The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n• At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14.Crowdsourcing and research with human subjects\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\\ninclude the full text of instructions given to participants and screenshots, if applicable, as\\nwell as details about compensation (if any)?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n• Including this information in the supplemental material is fine, but if the main contribu-\\ntion of the paper involves human subjects, then as much detail as possible should be\\nincluded in the main paper.\\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\\nor other labor should be paid at least the minimum wage in the country of the data\\ncollector.\\n15. Institutional review board (IRB) approvals or equivalent for research with human\\nsubjects\\nQuestion: Does the paper describe potential risks incurred by study participants, whether\\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\\napprovals (or an equivalent approval/review based on the requirements of your country or\\ninstitution) were obtained?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\\nmay be required for any human subjects research. If you obtained IRB approval, you\\nshould clearly state this in the paper.\\n21• We recognize that the procedures for this may vary significantly between institutions\\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\\nguidelines for their institution.\\n• For initial submissions, do not include any information that would break anonymity (if\\napplicable), such as the institution conducting the review.\\n16.Declaration of LLM usage\\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\\nnon-standard component of the core methods in this research? Note that if the LLM is used\\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\\nscientific rigorousness, or originality of the research, declaration is not required.\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the core method development in this research does not\\ninvolve LLMs as any important, original, or non-standard components.\\n• Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM)\\nfor what should or should not be described.\\n22A Dataset\\nWe evaluate the performance of TFPS on eight widely used datasets, including four ETT datasets\\n(ETTh1, ETTh2, ETTm1 and ETTm2), Exchange, Weather, Electricity, and ILI. This subsection\\nprovides a summary of the datasets:\\n• ETT 2 [70] (Electricity Transformer Temperature) dataset contains two electric transformers,\\nETT1 and ETT2, collected from two separate counties. Each of them has two versions of\\nsampling resolutions (15min & 1h). Thus, there are four ETT datasets:ETTm1,ETTm2,\\nETTh1, andETTh2.\\n• Exchange-Rate 3 [23] the exchange-rate dataset contains the daily exchange rates of eight\\nforeign countries including Australia, British, Canada, Switzerland, China, Japan, New\\nZealand, and Singapore ranging from 1990 to 2016.\\n• Weather 4 [62] dataset contains 21 meteorological indicators in Germany, such as humidity\\nand air temperature.\\n• Electricity 5 [62] is a dataset that describes 321 customers’ hourly electricity consumption.\\n• Traffic 6 [62] is a dataset featuring hourly road occupancy rates from 862 sensors along the\\nfreeways in the San Francisco Bay area.\\n• ILI 7 [62] dataset collects the number of patients and influenza-like illness ratio in a weekly\\nfrequency.\\nFor the data split, we follow [69] and split the data into training, validation, and testing by a ratio of\\n6:2:2 for the ETT datasets and 7:1:2 for the others. Details are shown in Table 5. The best parameters\\nare selected based on the lowest validation loss and then applied to the test set for performance\\nevaluation. The data and codes are available:https://github.com/syrGitHub/TFPS.\\nTable 5: The statistics of the datasets.\\nDatasets Variates Prediction Length Timesteps GranularityAverage Wasserstein*\\n(Time Domain)Average Wasserstein*\\n(Frequency Domain)\\nETTh1 7 {96, 192, 336, 720} 17,420 1 hour 9.268 11.561ETTh2 7 {96, 192, 336, 720} 17,420 1 hour 13.221 18.970ETTm1 7 {96, 192, 336, 720} 69,680 15 min 9.336 10.660ETTm2 7 {96, 192, 336, 720} 69,680 15 min 13.606 16.574Exchange-Rate8 {96, 192, 336, 720} 7,588 1 day 0.132 0.144Weather 21 {96, 192, 336, 720} 52,696 10 min 39.742 77.422Electricity 321 {96, 192, 336, 720} 26,304 1 hour 520.162 1018.311Traffic 862 {96, 192, 336, 720} 17,451 1 hour 0.011 0.028ILI 7 {24, 36, 48, 60} 966 1 week 258881.714 381377.494\\n*A large Wasserstein indicates a more severe drift.\\nB Related Work\\nDeep learning has achieved remarkable success across diverse domains such as computer vision\\n[14, 38, 39, 40, 37], natural language processing [58, 6], and multi-modality [13, 15], and has also\\nadvanced the state of the art in time series modeling [56, 19].\\nThe Combination of Time and Frequency Domains.Time-domain models excel at capturing\\nsequential trends, while frequency-domain models are essential for identifying periodic and oscillatory\\npatterns. Recent research has increasingly emphasized integrating information from both domains to\\nbetter interpret underlying patterns. For instance, ATFN [66] demonstrates the advantage of frequency\\ndomain methods for forecasting strongly periodic time series through a time–frequency adaptive\\n2https://github.com/zhouhaoyi/ETDataset\\n3https://github.com/laiguokun/multivariate-time-series-data\\n4https://www.bgc-jena.mpg.de/wetter/\\n5https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\n6http://pems.dot.ca.gov\\n7https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\\n23network. TFDNet [42] adopts a branching structure to capture long-term latent patterns and temporal\\nperiodicity from both domains. Similarly, JTFT [4] utilizes the frequency domain representation to\\nextract multi-scale dependencies while enhancing local relationships modeling through time domain\\nrepresentation. TFMRN [65] expands data in both domains to capture finer details that may not be\\nevident in the original data. Recently, TSLANet [9] leverages Fourier analysis to enhance feature\\nrepresentation and capture both long-term and short-term interactions.\\nBuilding on these approaches, our proposed method, TFPS, introduces a novel Dual-Domain Encoder\\nthat effectively combines time and frequency domain information to capture both trend and periodic\\npatterns. By integrating time-frequency features, TFPS significantly advances the field in addressing\\nthe complexities inherent in time series forecasting.\\nMixture-of-Experts.Mixture-of-Experts (MoE) models have gained attention for their ability to\\nscale efficiently by activating only a subset of experts for each input, as first introduced by [ 57].\\nDespite their success, challenges such as training instability, expert redundancy, and limited expert\\nspecialization have been identified [50, 5]. These issues hinder the full potential of MoE models in\\nreal-world tasks.\\nRecent advances have integrated MoE with Transformers to improve scalability and efficiency. For\\nexample, GLaM [7] and Switch Transformer [12] interleave MoE layers with Transformer blocks,\\nreducing computational costs. Other models like state space models (SSMs) [49, 2], [1] combines\\nMoE with alternative architectures for enhanced scalability and inference speed.\\nIn contrast, our approach introduces MoE into time series forecasting by assigning experts to specific\\ntime-frequency patterns, enabling more effective, patch-level adaptation. This approach represents a\\nsignificant innovation in time series forecasting, offering a more targeted and effective way to handle\\nvarying patterns across both time and frequency domains.\\nC Wasserstein Distance\\nThe Wasserstein distance, also called the Earth mover’s distance or the optimal transport distance, is a\\nsimilarity metric between two probability distributions. In the discrete case, the Wasserstein distance\\ncan be understood as the cost of an optimal transport plan to convert one distribution into the other.\\nThe cost is calculated as the product of the amount of probability mass being moved and the distance\\nit is being moved.\\nGiven two one-dimensional probability mass functions,u and v, the first Wasserstein distance between\\nthem is defined as:\\nl1(u, v) = inf\\nπ∈Γ(u,v)\\nZ\\nR×R\\n|x−y|dπ(x, y),(13)\\nwhere Γ(u, v) is the set of (probability) distributions on R×R whose marginals are u and v on the\\nfirst and second factors respectively. Here, u(x) represents the probability of u at position x, with the\\nsame interpretation forv(x).\\nIn the special case of one-dimensional distributions, the Wasserstein distance can be equivalently\\nexpressed using their cumulative distribution functions (CDFs),UandV, as:\\nl1(u, v) =\\nZ +∞\\n−∞\\n|U−V|.(14)\\nThis equivalence is rigorously proved in [54].\\nThe input distributions can be empirical, therefore coming from samples whose values are effectively\\ninputs of the function, or they can be seen as generalized functions, in which case they are weighted\\nsums of Dirac delta functions located at the specified values.\\nD Distribution Shifts in both Time and Frequency Domains\\nThe time seriesX is segmented intoN patches, where each patchPn ={x n1, xn2, . . . , xnP } consists\\nof P consecutive timesteps for n= 1,2,· · ·, N . For the frequency domain, we apply a Fourier\\ntransformFto each patchP n, obtaining its frequency-domain representation as ˆPn =F(P n).\\n24Each patch’s probability distribution in the time domain is denoted as pt(Pn), representing the\\nstatistical properties of Pn, while its frequency domain distribution, denoted as pf( ˆPn), captures its\\nspectral characteristics.\\nThe distribution shifts between two patches Pi and Pj are characterized by the comparing their\\nprobability distributions in both time and frequency domains. These shifts are defined as:\\nDt(Pi,P j) =|d(p t(Pi), pt(Pj))|> θ,(15)\\nDf( ˆPi, ˆPj) =|d(p f( ˆPi), pf( ˆPj))|> θ,(16)\\nwhere d is a distance metric, such as Wasserstein distance or Kullback-Leibler divergence, andθ is\\na threshold indicating a significant distribution shift. If Dt(Pi,P j) or Df( ˆPi, ˆPj) exceeds θ, this\\nimplies a significant distribution shift between the two patches in either domain. It is important to\\nnote that θ serves only as a conceptual threshold for defining distribution shifts in the analysis and\\ndoes not participate in the modeling or training process of TFPS.\\nE Metric Illustration\\nWe use mean square error (MSE) and mean absolute error (MAE) as our metrics for evaluation of all\\nforecasting models. Then calculation of MSE and MAE can be described as:\\nMSE= 1\\nH\\nL+HX\\ni=L+1\\n( ˆYi −Y i)2,(17)\\nMAE= 1\\nH\\nL+HX\\ni=L+1\\n\\x0c\\x0c\\x0c ˆYi −Y i\\n\\x0c\\x0c\\x0c ,(18)\\nwhere ˆYis predicted vector withHfuture values, whileYis the ground truth.\\nIn addition, we report the IMP (Improvement) metric, which is defined as:\\nIMP= Avg MSE of baselines−MSE of TFPS\\nAvg MSE of baselines ×100%.(19)\\nThis metric quantifies the relative percentage improvement of TFPS over the average MSE of all\\nbaseline methods. A higher IMP value indicates better overall performance of TFPS compared to the\\nbaselines.\\nF Hyperparameter-search Results\\nF.1 Comparsion with Specific Models\\nTo ensure a fair comparison between models, we conducted experiments using unified parameters\\nand reported results in the main text.\\nIn addition, considering that the reported results in different papers are mostly obtained through\\nhyperparameter search, we provide the experiment results with the full version of the parameter\\nsearch. We searched for input length among 96, 192, 336, and 512. The results are included in\\nTable 6. All baselines are reproduced by their official code.\\nWe can find that the relative promotion of TFPS over TFDNet is smaller under comprehensive\\nhyperparameter search than the unified hyperparameter setting. It is worth noticing that TFPS runs\\nmuch faster than TFDNet according to the efficiency comparison in Table 12. Therefore, considering\\nperformance, hyperparameter-search cost and efficiency, we believe TFPS is a practical model in\\nreal-world applications and is valuable to deep time series forecasting community.\\nF.2 Comparsion with Foundation Models\\nTable 7 presents the detailed results of TFPS compared to recent foundation models (AutoTimes\\n[30], Moment [ 16], and Timer [ 32]) across six datasets and four forecasting lengths. Consistent\\n25Table 6: Experiment results under hyperparameter searching for the long-term forecasting task. The\\nbest results are highlighted inboldand the second best are underlined.\\nModelIMP. TFPS TSLANetFITS iTransformerTFDNet-IKPatchTSTTimesNetDlinearFEDformer(Our) (2024) (2024) (2024) (2023) (2023) (2023) (2023) (2022)MetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 1.5%0.372 0.4040.3680.3940.374 0.3950.387 0.4050.360 0.3870.375 0.4000.389 0.4120.384 0.4050.385 0.4251925.7%0.401 0.4100.413 0.4180.407 0.4140.441 0.4360.4030.4120.414 0.4210.441 0.4420.443 0.4500.441 0.4613369.8%0.409 0.4020.4120.4160.429 0.4280.491 0.4630.434 0.4290.432 0.4360.491 0.4670.447 0.4480.491 0.47372011.2%0.423 0.4330.473 0.4770.4250.4460.509 0.4940.437 0.4520.450 0.4660.512 0.4910.504 0.5150.501 0.499\\nETTh2\\n96 9.3%0.268 0.3250.283 0.3440.274 0.3370.301 0.3500.2710.3290.278 0.3360.324 0.3680.290 0.3530.342 0.38319210.4%0.3290.3760.3310.3780.337 0.3770.380 0.3990.3330.3720.339 0.3800.393 0.4100.388 0.4220.434 0.44033617.7%0.329 0.4010.319 0.3770.360 0.3980.424 0.4320.361 0.3960.3360.3800.429 0.4370.463 0.4730.512 0.4977209.0%0.412 0.4410.407 0.4490.386 0.4230.430 0.4470.3820.4180.3820.4210.433 0.4480.733 0.6060.467 0.476\\nETTm1\\n9610.2%0.281 0.3290.291 0.3530.303 0.3450.342 0.3770.2830.3300.288 0.3420.337 0.3770.301 0.3450.360 0.4061928.5%0.324 0.3540.329 0.3720.337 0.3650.383 0.3960.3270.3560.334 0.3720.395 0.4060.336 0.3660.395 0.4273368.2%0.359 0.4040.3570.3920.3720.3850.418 0.4180.3610.3750.367 0.3930.433 0.4320.372 0.3890.448 0.4587208.2%0.409 0.4080.423 0.4250.428 0.4160.487 0.4570.4110.4090.417 0.4220.484 0.4580.427 0.4230.491 0.479\\nETTm2\\n96 8.9%0.158 0.2430.167 0.2560.165 0.2550.186 0.2720.1580.2440.164 0.2530.182 0.2620.172 0.2670.193 0.2851925.7%0.222 0.3020.221 0.2940.2200.2910.254 0.3140.219 0.2820.221 0.2920.252 0.3070.237 0.3140.256 0.3243368.5%0.268 0.3160.277 0.3290.274 0.3260.316 0.3510.2730.3170.277 0.3290.312 0.3460.295 0.3590.321 0.36472012.0%0.344 0.3730.356 0.3820.367 0.3830.414 0.4070.3460.3740.365 0.3840.417 0.4040.427 0.4390.434 0.426\\nTraffic\\n9617.8%0.370 0.2500.375 0.2600.398 0.2850.428 0.2950.3770.2530.373 0.2590.586 0.3160.413 0.2870.575 0.35719217.0%0.390 0.2580.395 0.2720.408 0.2880.448 0.3020.3910.2600.395 0.2730.618 0.3230.424 0.2900.613 0.38133617.2%0.4010.2710.402 0.2720.420 0.2920.465 0.3110.4080.2660.402 0.2740.634 0.3370.438 0.2990.622 0.38072015.7%0.432 0.2940.431 0.2880.448 0.3100.501 0.3330.4510.2910.435 0.2930.659 0.3490.466 0.3160.630 0.383\\nElectricity\\n9610.3%0.128 0.2200.137 0.2290.135 0.2310.148 0.2390.1300.2220.130 0.2230.168 0.2720.140 0.2370.188 0.30319211.9%0.145 0.2350.153 0.2420.149 0.2440.167 0.2580.1460.2370.149 0.2400.186 0.2890.154 0.2500.197 0.3113366.8%0.1660.2580.165 0.2630.165 0.2600.178 0.2710.162 0.2540.168 0.2620.196 0.2970.169 0.2680.212 0.3277206.9%0.198 0.2830.206 0.2940.204 0.2930.211 0.3000.2010.2870.204 0.2890.235 0.3290.204 0.3000.243 0.3521stCount 32 5 0 0 10 1 0 0 0\\nTable 7: Detailed results of the comparison between TFPS and foundation models. The best results\\nare highlighted inboldand the second best are underlined.\\nModel IMP. TFPS AutoTimesMoment Timer(Our) (2024) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -2.2%0.3720.4040.365 0.4050.369 0.4060.359 0.392192-1.3%0.4010.4100.392 0.4230.405 0.4310.3910.413336 0.5%0.4090.4020.4060.4330.420 0.4410.4070.424720 3.1%0.4230.4330.423 0.4500.466 0.4790.4210.441\\nETTh2\\n96 6.3%0.268 0.3250.286 0.3490.287 0.3470.2850.344192 9.2%0.329 0.3760.3510.3930.371 0.4010.365 0.40033617.2%0.329 0.4010.3770.4170.404 0.4250.412 0.440720 9.8%0.412 0.4410.4390.4640.463 0.4760.467 0.487\\nETTm1\\n96 1.3%0.2810.3290.297 0.3500.281 0.3430.2760.335192 1.2%0.3240.3540.344 0.3770.3180.3680.3230.365336 1.6%0.359 0.4040.380 0.3980.3560.3910.3580.388720 4.8%0.409 0.4080.433 0.4310.438 0.4410.4190.423\\nETTm2ETTm2\\n96 8.6%0.158 0.2430.181 0.2660.170 0.2580.1670.254192 5.5%0.2220.3020.241 0.3060.2330.3010.2290.297336 6.9%0.268 0.3160.295 0.3410.287 0.3400.2820.335720 8.1%0.344 0.3730.3760.3930.371 0.3990.376 0.398\\nTraffic\\n96 -5.5%0.370 0.2500.3470.2490.360 0.2540.345 0.237192-5.3%0.390 0.2580.3660.2580.381 0.2650.365 0.247336-3.1%0.401 0.2710.3830.2670.404 0.2770.381 0.256720-1.1%0.432 0.2940.4200.2860.438 0.2970.4240.280\\nElectricity\\n96 3.4%0.128 0.2200.135 0.2300.133 0.2360.1300.224192 4.3%0.145 0.2350.153 0.2470.152 0.2540.1490.243336 1.5%0.166 0.2580.172 0.2660.166 0.2650.1680.263720 5.2%0.198 0.2830.212 0.3000.2020.2980.213 0.303\\n1stCount 30 2 2 14\\nwith Table 6, for TFPS, we searched input lengths among 96, 192, 336, and 512, and report the\\nbest-performing configuration for each forecasting length.\\nTFPS achieves the best performance in30 out of 48settings, significantly outperforming AutoTimes\\nand Moment, which only achieve 2 best scores each. Although Timer demonstrates competitive\\n26Table 8: Detailed results of the comparison between TFPS and normalization-based methods using\\nFEDformer. The best results are highlighted inboldand the second best are underlined.\\nFEDformerTFPS + SIN + SAN + Dish-TS+ RevINModelIMP. (Our) (2024) (2023) (2023) (2021)\\nMetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\n96 -0.9%0.398 0.4130.4130.3720.3830.4090.390 0.4240.392 0.413192 3.7%0.4230.4230.4430.4170.431 0.4380.441 0.4580.443 0.444336-0.5%0.484 0.4610.465 0.4480.4710.4560.495 0.4860.495 0.467\\nETTh1720 4.8%0.488 0.4760.509 0.4900.5040.4880.519 0.5090.520 0.498\\n96 34.0%0.3130.3550.412 0.3570.3000.3550.806 0.5890.380 0.40219228.3%0.4050.4100.472 0.4530.3920.4130.936 0.6590.457 0.44333638.2%0.392 0.4150.527 0.5270.4590.4621.039 0.7020.515 0.479\\nETTh272041.4%0.410 0.4330.593 0.6390.4620.4721.237 0.7590.507 0.487\\n96 4.5%0.327 0.3670.3730.3200.3110.3550.348 0.3970.340 0.385192 2.9%0.374 0.3950.3940.3660.3510.3830.406 0.4280.390 0.411336 4.4%0.401 0.4080.4180.4050.3900.4070.438 0.4500.432 0.436\\nETTm1720-0.8%0.4790.4560.4510.4750.4560.4440.497 0.4810.497 0.466\\n96 37.5%0.1700.2550.3260.2110.175 0.2660.394 0.3950.192 0.27219235.9%0.235 0.2960.402 0.3160.2460.3150.552 0.4720.270 0.32033638.6%0.297 0.3350.465 0.3990.3150.3620.808 0.6010.348 0.367\\nETTm272040.2%0.401 0.3970.555 0.5470.412 0.4221.282 0.7710.4300.415\\n96 30.7%0.154 0.2020.2800.2150.179 0.2390.244 0.3170.187 0.23419225.6%0.205 0.2490.3140.2640.234 0.2960.320 0.3800.235 0.27233622.0%0.262 0.2890.3290.2930.304 0.3480.424 0.4520.287 0.307\\nWeather72021.3%0.344 0.3420.3820.3700.400 0.4040.604 0.5530.361 0.353\\n1stCount 24 9 7 0 0\\nTable 9: Detailed results of the comparison between TFPS and normalization-based methods using\\nDLinear. The best results are highlighted inboldand the second best are underlined.\\nDLinearTFPS + SIN + SAN + Dish-TS+ RevINModel IMP. (Our) (2024) (2023) (2023) (2021)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -1.6%0.398 0.4130.401 0.4150.385 0.3950.3890.3990.393 0.4161922.9%0.423 0.4230.438 0.4560.4320.4230.443 0.4330.431 0.428336-0.4%0.484 0.4610.462 0.4460.490 0.4630.4870.4560.488 0.4837204.6%0.488 0.4760.515 0.5000.516 0.5040.523 0.5080.4930.482\\nETTh2\\n96 4.9%0.313 0.3550.3590.3590.319 0.3640.317 0.3650.322 0.3611921.1%0.405 0.4100.409 0.4240.407 0.4390.4080.4200.412 0.4243363.6%0.392 0.4150.398 0.4290.4110.4250.416 0.4260.403 0.4277202.7%0.410 0.4330.419 0.4420.417 0.4410.4280.4390.422 0.446\\nETTm1\\n96 4.9%0.327 0.3670.350 0.3830.333 0.3740.343 0.3750.3520.3691921.9%0.3740.3950.383 0.3960.374 0.3960.3810.3910.388 0.3963363.0%0.401 0.4080.413 0.4160.406 0.4180.416 0.4170.4190.4147200.0%0.479 0.4560.4730.4520.4830.4510.482 0.4580.478 0.463\\nETTm2\\n96 5.5%0.170 0.2550.172 0.2830.179 0.2720.1890.2640.179 0.2691924.4%0.235 0.2960.2490.3010.239 0.3160.249 0.3020.248 0.3023361.2%0.297 0.3350.2990.3390.301 0.3530.305 0.3490.299 0.3457203.2%0.401 0.3970.412 0.4210.404 0.4080.4290.4020.411 0.402\\nWeather\\n96 4.5%0.154 0.2020.162 0.2230.1570.2150.173 0.2410.154 0.2431927.6%0.205 0.2490.216 0.2590.2140.2580.225 0.2630.233 0.2653366.8%0.262 0.2890.2790.2910.275 0.2920.289 0.3050.282 0.2937203.0%0.344 0.3420.3550.3410.349 0.3510.366 0.3690.348 0.362\\n1stCount 33 3 3 1 0\\nperformance (with 14 best scores), TFPS consistently achieves lower error in datasets with higher\\ndistribution shifts, such as ETTh2 and ETTm2, indicating its advantage in handling pattern hetero-\\ngeneity.\\nThese results reinforce the effectiveness of TFPS’s pattern-specific modeling strategy, especially in\\nscenarios where traditional large-scale foundation models struggle to generalize.\\n27Table 10: Comparison between TFPS and MoE-based methods. The best results are highlighted in\\nboldand the second best are underlined.\\nModel IMP. TFPS MoLE MoU KAN4TSF(Our) (2024) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -4.3%0.398 0.4130.3830.3920.3810.4030.3820.400192 1.7%0.423 0.4230.4340.4260.429 0.4300.430 0.426336 1.6%0.484 0.4610.489 0.4780.4880.4630.498 0.467720 8.2%0.488 0.4760.602 0.5450.499 0.4840.4940.479\\nETTh2\\n96 10.4%0.313 0.3550.413 0.3600.3170.3580.318 0.35819210.3%0.405 0.4100.525 0.4160.4090.4140.419 0.414336 7.1%0.392 0.4150.423 0.4340.3970.4200.447 0.452720 8.4%0.410 0.4330.453 0.4580.4120.4340.477 0.476\\nETTm1\\n96 13.5%0.327 0.3670.338 0.3800.465 0.4420.333 0.37119210.6%0.374 0.3950.388 0.4030.483 0.4550.3840.39933611.8%0.401 0.4080.417 0.4310.540 0.4880.4070.413720 7.3%0.479 0.4560.486 0.4720.583 0.5090.4830.469\\nETTm2\\n96 13.9%0.170 0.2550.238 0.2710.179 0.2630.1750.260192 3.8%0.235 0.2960.247 0.3050.2430.3030.244 0.305336 3.3%0.297 0.3350.308 0.3430.3060.3430.308 0.34772013.7%0.401 0.3970.583 0.4190.4050.4040.405 0.404\\n1stCount 30 1 1 0\\nG Compared with Other Methods\\nG.1 Compared with Normalization Methods\\nIn this section, we provide the detailed experimental results of the comparison between TFPS and five\\nstate-of-the-art normalization methods for non-stationary time series forecasting: SIN [17], SAN [34],\\nDish-TS [10], and RevIN [20]. We summarize the forecasting results of TFPS and baseline models in\\nTable 8 and Table 9. Specifically, the results of FEDformer combined with SIN are taken from [17],\\nwhile those of FEDformer with other normalization-based methods are reported by [34]. For a fair\\ncomparison, we additionally rerun all experiments for DLinear combined with each normalization\\nmethod.\\nTable 8 and Table 9 presents the forecasting performance across all prediction lengths for each dataset,\\nalong with the relative improvements of TFPS over existing methods. As shown, TFPS consistently\\nachieves the best performance in the majority of settings, demonstrating its strong adaptability to\\ndistributional and conceptual drifts in time series data.\\nWe attribute this improvement to the accurate identification of pattern groups and the provision\\nof specialized experts for each group, thereby avoiding the over-stationarization problem often\\nassociated with normalization methods.\\nG.2 Compared with MoE-based Methods\\nAs shown in Table 10, unlike MoE-based methods that rely on the Softmax function as a gating\\nmechanism, our approach constructs a pattern recognizer to assign different experts to handle distinct\\npatterns. This results in TFPS achieving relative improvements of 2.3%, 9.0%, 10.6%, and 9.1%\\nacross the four datasets, respectively.\\nG.3 Compared with Distribution Shift Methods\\nAs shown in Table 11, we compare with the methods for distribution shift. This results in TFPS\\nachieving relative improvements of 6.7%, 6.6%, 4.8%, and 5.9% across the four datasets, respectively.\\nH Model Analysis\\nDetailed Results on the Number of Experts.We provide the full results on the number of experts\\nfor the ETTh1 and Weather dataset in Figure 7.\\n28Table 11: Comparison between TFPS and methods for Distribution Shift. The best results are\\nhighlighted inboldand the second best are underlined.\\nModel IMP. TFPS Koopa SOLID OneNet(Our) (2024)) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 7.9%0.3980.4130.3850.4070.440 0.4390.4250.40219210.3%0.423 0.4230.4450.4340.492 0.4660.452 0.443336 4.9%0.484 0.4610.4890.4600.525 0.4810.492 0.482720 4.4%0.488 0.4760.4970.4800.517 0.4960.504 0.496\\nETTh2\\n96 10.6%0.313 0.3550.318 0.3600.3180.3590.382 0.362192 4.7%0.4050.4100.378 0.3980.414 0.4180.435 0.426336 4.8%0.392 0.4150.415 0.4300.398 0.4210.4260.419720 6.8%0.410 0.4330.445 0.4560.424 0.4410.4560.437\\nETTm1\\n96 6.8%0.327 0.3670.3290.3590.329 0.3700.374 0.392192 2.0%0.3740.3950.3800.3930.379 0.4000.385 0.435336 8.7%0.401 0.4080.4010.4110.405 0.4120.473 0.458720 2.0%0.4790.4560.475 0.4480.482 0.4640.496 0.483\\nETTm2\\n96 5.3%0.170 0.2550.179 0.2610.1750.2580.184 0.274192 3.8%0.235 0.2960.246 0.3050.2410.3020.248 0.384336 3.4%0.297 0.3350.310 0.3480.3030.3420.313 0.374720 9.0%0.401 0.3970.4050.4020.456 0.4360.425 0.438\\n1stCount 25 6 0 1\\nFigure 7: Results of expert number experiments for ETTh1 and Weather.\\nIn Figure 7, we set the learning rate to 0.0001 and conducted four sets of experiments on the ETTh1\\nand Weather datasets, Kt = 1, Kf ={1,2,4,8} , to explore the effect of the number of frequency\\nexperts on the results. For example, Kt1Kf 4 means that the TFPS contains 1 time experts and 4\\nfrequency experts. We observed that Kt1Kf 2 outperformed Kt1Kf 4 in most cases, suggesting that\\nincreasing the number of experts does not always lead to better performance.\\nIn addition, we conducted three experiments based on the optimal number of frequency experts to\\nverify the impact of varying the number of time experts on the results. As shown in Figure 7, the best\\nresults for ETTh1 were obtained with Kt4Kf 2, Kt8Kf 4, Kt4Kf 4, Kt4Kf 4, while for Weather, the\\noptimal results were achieved with Kt4Kf 8, Kt4Kf 8, Kt4Kf 4 and Kt4Kf 8. Combined with the\\naverage Wasserstein in Table 5, we attribute this to the fact that, in cases where concept drift is more\\nsevere, such as Weather, more experts are needed, whereas fewer experts are sufficient when the drift\\nis less severe.\\nComparing Inter- and Intra-Cluster Differences via Wasserstein Distance.To assess the ef-\\nfectiveness of the PI module, we replace it with a linear layer and compare the resulting inter- and\\nintra-cluster Wasserstein distance heatmaps in Figure 8. The diagonal elements represent the average\\nWasserstein distances of patches within the same clusters. If these values are small, it indicates that the\\ndifference of patches within the same cluster is relatively similar. The off-diagonal elements represent\\n29(a) Linear layer\\n (b) Pattern Identifier\\nFigure 8: Heatmap showing the Wasserstein distances of inter- and intra-cluster patches on ETTh1.\\n(a) Linear\\n (b) Pattern Identifier\\nFigure 9: Visualization of the embedded representations with t-SNE on ETTh1 for the time domain\\nwith H= 96 . (a) t-SNE visualization with a Linear Layer replacing the Patch Identifier for\\ncomparison. (b) t-SNE visualization of TFPS.\\n(a) ETTh1\\n (b) ETTm1\\n (c) Weather\\nFigure 10: MoE Expert allocation distributions of TFPS: the x-axis corresponds to the 4 experts, and\\nthe y-axis shows the proportion of tokens assigned to each expert.\\nthe average Wasserstein distances between patches from different clusters, where larger values mean\\nsignificant differences between the clusters. We observe that when using PI, the intra-cluster drift is\\nsmaller, while the inter-cluster shift is more pronounced compared to the linear layer. This indicates\\nthat our identifier effectively classifies and distinguishes between different patterns.\\nTFPS produces differentiated token embeddings by adapting to the characteristics of the data.\\nFigure 9 presents the t-SNE visualization of the learned embedded representation on the ETTh1 for\\nthe time domain with H= 96 . In the Figure 9 (a), where the pattern identifier is replaced with a\\nlinear layer, the representation lacks clear clustering structures, resulting in scattered and indistinct\\ngroupings. In contrast, Figure 9 (b) shows the visualization of the representation learned by the\\nproposed method, which effectively captures discriminative features and reveals significantly clearer\\nclustering patterns.\\nTFPS implements dataset-specific token embeddings assignment in a data-driven way, effec-\\ntively improving performance.Figure 10 visualizes the expert allocation distributions across\\nvarious datasets. Notably, ETTh1 and ETTm1 exhibit a high degree of consistency in their expert\\nassignments, underscoring the model’s ability to capture shared patterns. Conversely, the Weather\\ndataset shows a distinctly different allocation pattern, highlighting the method’s sensitivity to unique\\ndataset characteristics and its capability to tailor expert assignments accordingly.\\n30Table 12: The GPU memory (MB) and speed (inference time) of each model.\\nTFPS TSLANet FITS iTransformer TFDNet-IK PatchTST TimesNet DLinear FEDformer\\nMSE 0.4230.448 0.445 0.441 0.458 0.460 0.441 0.434 0.441GPU Memory (MB)9.643 0.481 0.019 3.304 0.246 0.205 2.345 0.142 62.191Average Inference Time (ms)6.114 0.063 1.184 2.571 98.266 4.861 12.306 0.659 136.130\\n(a)α\\n (b)β\\nFigure 11: Parameter sensitivity ofαandβof the proposed method on the ETTh1-96 dataset.\\nI Efficiency Analysis\\nTo make this clearer, we present the results of ETTh1 for a prediction length of 192 from Table 1 and\\ninclude additional results on runtime and computational complexity in Table 12. Due to the sparsity\\nof MoPE, TFPS achieves a balance between performance and efficiency:\\nPerformance Superiority. TFPS achieves an MSE of 0.423, outperforming TSLANet (0.448),\\nFITS (0.445), PatchTST (0.460), and FEDformer (0.441). This represents a 5.6% improvement over\\nTSLANet and a 8.0% improvement over PatchTST, highlighting its significant accuracy gains. While\\nDLinear achieves an MSE of 0.434, TFPS still demonstrates a 2.5% relative improvement, making it\\nthe most accurate model among all baselines.\\nEfficiency Gains. TFPS maintains competitive runtime and memory efficiency.\\n• Runtime: TFPS runs in 6.457 ms, making it 2.8× faster than PatchTST (17.851 ms) and\\n11.2× faster than TimesNet (72.196 ms).\\n• Memory Usage: TFPS uses 9.643 MB of GPU memory, significantly less than FEDformer\\n(62.191 MB) and comparable to iTransformer (3.304 MB). This makes TFPS suitable for\\nresource-constrained applications while maintaining superior performance.\\nBalancing Trade-offs. While lightweight models like DLinear (0.434 MSE, 0.789 ms runtime) are\\nslightly more efficient, TFPS delivers a performance improvement of 2.5%, providing a well-rounded\\nsolution that balances accuracy and efficiency effectively.\\nJ Hyperparameter Sensitivity\\nIn this section, we analysis the impact of the hyperparametersαandβon the performance.\\nSpecifically, we performed a grid search to optimize the hyperparametersαt ={0.0001,0.001,0.01}\\nand αf ={0.0001,0.001,0.01} , as shown in Figure 11 (a). After extensive testing, we ultimately\\nfixed atα t =α f = 10−3 in our experiments.\\nIn addition, we conducted a grid search to optimize the balance factors βt ={0.01,0.05,0.1,0.5,1}\\nand βf ={0.01,0.05,0.1,0.5,1} . The performance under different parameter values is displayed in\\nFigure 11 (b), from which we have the following observations:\\n• Firstly, the performance is affected when the value of β is too low, indicating that the\\nproposed clustering objective plays a crucial role in distinguishing patterns.\\n• Second, an excessive β also has a negative on the performance. One plausible explanation\\nis that the excessive value influences the learning of the inherent structure of original data,\\nresulting in a perturbation of the embedding space.\\n31Table 13: In the table, w/ Imaginary indicates that we incorporate both the real and imaginary parts\\ninto the network.\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\nTFPS 0.3980.423 0.4840.4880.3130.4050.392 0.410\\nw/ Imaginary0.3970.424 0.4870.4860.3120.4060.391 0.399\\nTable 14: Ablation study of PI components. The model variants in our ablation study include the\\nfollowing configurations across both time and frequency branches: (a) inclusion of the Time PI; (b)\\ninclusion of the Frequency PI; (c) exclusion of both. The best results are inbold.\\nTime PI Frequency PI ETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\n✓ ✓ 0.398 0.423 0.484 0.4880.313 0.405 0.392 0.410\\n✓✗ 0.4040.4540.4900.5030.3220.4130.4100.425\\n✗✓ 0.405 0.456 0.493 0.5090.324 0.415 0.412 0.430\\n✗ ✗ 0.407 0.458 0.497 0.5130.328 0.418 0.419 0.435\\n• Overall, we recommend settingβaround 0.1 for optimal performance.\\nK Full Ablation\\nK.1 Impacts of Real/Imaginary Parts\\nTo further validate the robustness of our approach, we adopted similar operations in FreTS to conduct\\nexperiments incorporating both the real and imaginary parts. The results in the Table 13 show that\\nthe performance of TFPS with the real part only is very similar to that when both parts are included,\\nwhile requiring fewer parameters. This further reinforces the conclusion that TFPS remains highly\\neffective even when focusing solely on the real part of the Fourier transform.\\nK.2 Ablation on PI\\nThe PI module plays a crucial role in identifying and characterizing distinct patterns within the time\\nseries data, while the gating network dynamically selects the most relevant experts for each segment.\\nThis collaborative mechanism allows the model to specialize in handling different patterns and adapt\\neffectively to distribution shifts, thus mitigating the overfitting risks that arise from treating all data\\nequally.\\nTo validate the importance of PI empirically, we have conducted the ablation experiments comparing\\nthe model’s performance by replacing the PI module with a linear layer in the Table 2 of main text.\\nIn addition, we supplement some ablation experiments in Table 14 to further verify the effectiveness\\nof PI.\\nK.3 Ablation onR 1 andR 2\\nWe conducted ablation experiments to further verify the important roles of R1 and R2, as shown in\\nTable 15.\\nK.4 Replace MoPE with Alternative Designs\\nHere we provide the complete results of alternative designs for TFPS.\\nAs show in Table 16, we have conducted addition experiments where we replaced the MoPE module\\nwith weighted multi-output predictor and stacked self-attention layers, keeping all other components\\n32Table 15: Ablation study of Loss Constraint. The model variants in our ablation study include\\nthe following configurations across both time and frequency branches: (a) inclusion of the R1; (b)\\ninclusion of theR 2; (c) exclusion of both. The best results are inbold.\\nR1 R2\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\n✓ ✓ 0.398 0.423 0.484 0.488 0.313 0.405 0.392 0.410\\n✓✗ 0.408 0.449 0.500 0.498 0.320 0.418 0.415 0.429\\n✗✓ 0.403 0.434 0.493 0.491 0.316 0.413 0.405 0.418\\n✗ ✗ 0.412 0.456 0.509 0.503 0.328 0.425 0.420 0.435\\nTable 16: Multi-output predictor and a stacked attention layer are used to replace MoPE in ETTh1\\nand ETTh2 datasets.\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\nTFPS 0.398 0.423 0.484 0.4880.313 0.405 0.392 0.410Multi-output Predictor0.403 0.435 0.492 0.4910.317 0.407 0.399 0.425Attention Layers0.399 0.452 0.492 0.5080.334 0.407 0.409 0.451\\nand configurations identical. The results demonstrate that our proposed method significantly out-\\nperforms them, which validates the importance of the Top-K selection and pattern-aware design in\\nenhancing the model’s representation capacity. In contrast, multi-output predictor and self-attention\\ntypically treats all data points uniformly, which may limit its ability to capture subtle distribution\\nshifts or evolving patterns across patches.\\nL Algorithm of TFPS\\nWe provide the pseudo-code of TFPS in Algorithm 1.\\nM Broader Impact\\nReal-world Applications.TFPS addresses the crucial challenge of time series forecasting, which is a\\nvaluable and urgent demand in extensive applications. Our method achieves consistent state-of-the-art\\nperformance in four real-world applications: electricity, weather, exchange rate, illness. Researchers\\nin these fields stand to benefit significantly from the enhanced forecasting capabilities of TFPS. We\\nbelieve that improved time series forecasting holds the potential to empower decision-making and\\nproactively manage risks in a wide array of societal domains.\\nAcademic Research.TFPS draws inspiration from classical time series analysis and stochastic\\nprocess theory, contributing to the field by introducing a novel framework with the assistance pattern\\nrecognition. This innovative architecture and its associated methodologies represent significant\\nadvancements in the field of time series forecasting, enhancing the model’s ability to address\\ndistribution shifts and complex patterns effectively.\\nModel Robustness.Extensive experimentation with TFPS reveals robust performance without\\nexceptional failure cases. Notably, TFPS exhibits impressive results and maintains robustness in\\ndatasets with distribution shifts. The pattern identifier structure within TFPS groups the time series\\ninto distinct patterns and adopts a mixture of pattern experts for further prediction, thereby alleviating\\nprediction difficulties. However, it is essential to note that, like any model, TFPS may face challenges\\nwhen dealing with unpredictable patterns, where predictability is inherently limited. Understanding\\nthese nuances is crucial for appropriately applying and interpreting TFPS’s outcomes.\\nOur work only focuses on the scientific problem, so there is no potential ethical risk.\\n33Algorithm 1Time-Frequency Pattern-Specific architecture - Overall Architecture.\\nInput: Input lookback time series X∈R L×C; input length L; predicted length H; variables number\\nC; patch length P ; feature dimension D; encoder layers number n; random Gaussian distribution-\\ninitialized subspace D= [D (1),D (2),· · ·,D (K)], each D(j) ∈R q×d, where q=C×D and\\nd=q/K. Technically, we setDas 512,nas 2.\\nOutput: The prediction result ˆY.\\n1:X=X.transpose▷ X∈R C×L\\n2:X P E =Patch(X) +Position Embedding▷ X 0\\nt ∈R C×N×D\\n3:▷Time Encoder.\\n4:X 0\\nt =X P E\\n5:forlin{1, . . . , n}:\\n6:forX l−1\\nt =LayerNorm(X l−1\\nt +Self-Attn(X l−1\\nt )).▷ X l−1\\nt ∈R C×N×D\\n7:forX l\\nt =LayerNorm(X l−1\\nt +Feed-Forward(X l−1\\nt )).▷ X l\\nt ∈R C×N×D\\n8:End for\\n9:z t =X l\\nt ▷ z l\\nt ∈R C×N×D\\n10:▷Pattern Identifier for Time Domain.\\n11:s t =Subspace affinity(z t,D)▷Eq. 6 of the papers t ∈R C×N×D\\n12:ˆst =Subspace refinement(s t)▷Eq. 7 of the paperˆs t ∈R C×N×D\\n13:▷Mixture of Temporal Pattern Experts.\\n14:G(s) =Softmax(TopK(s t))\\n15:h t =PK\\nk=1 G(s)MLPk(zt)▷Eq. 10 and Eq. 11 of the paperh t ∈R C×N×D\\n16:▷Frequency Encoder.\\n17:X 0\\nf =X P E ▷Eq. 2 of the paperX 0\\nf ∈R C×N×P\\n18:forlin{1, . . . , n}:\\n19:forX l−1\\nf =LayerNorm(X l−1\\nf +Fourier(X l−1\\nf )).▷ X l−1\\nf ∈R C×N×D\\n20:forX l\\nf =LayerNorm(X l−1\\nf +Feed-Forward(X l−1\\nf )).▷ X l\\nf ∈R C×N×D\\n21:End for\\n22:z f =X l\\nf ▷ z n\\nf ∈R C×N×D\\n23:▷Pattern Identifier for Frequency Domain.\\n24:s f =Subspace affinity(z f ,D)▷Eq. 6 of the papers f ∈R C×N×D\\n25:ˆsf =Subspace refinement(s f )▷Eq. 7 of the paperˆs f ∈R C×N×D\\n26:▷Mixture of Frequency Pattern Experts.\\n27:G(s) =Softmax(TopK(s f ))\\n28:h f =PK\\nk=1 G(s)MLPk(zf)▷Eq. 10 and Eq. 11 of the paperh f ∈R C×N×D\\n29:h=Concat(h t,h f )▷ h∈R C×N×2∗D\\n30:forcin{1, . . . , C}:\\n31:for ˆY=Linear(Flatten(h)).▷Project tokens back to predicted series ˆY∈R C×H\\n32:End for\\n33: ˆY= ˆY.transpose▷ ˆY∈R H×C\\n34:Return ˆY ▷Output the final prediction ˆY∈R H×C\\n34N Limitations\\nThough TFPS demonstrates promising performance on the benchmark dataset, there are still some\\nlimitations of this method. First, the patch length is primarily chosen heuristically, and the current\\ndesign struggles with handling indivisible lengths or multi-period characteristics in time series. While\\nthis approach works well in experiments, it lacks generalizability for real-world applications. Second,\\nthe real-world time series data undergo expansion, implying that the new patterns continually emerge\\nover time, such as an epidemic or outbreak that had not occurred before. Therefore, future work will\\nfocus on developing a more flexible and automatic patch length selection mechanism, as well as an\\nextensible solution to address these evolving distribution shifts.\\n35'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=\"\"\n",
    "for item in pages:\n",
    "    docs+=item.page_content\n",
    "  \n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template=\"\"\"\n",
    "'''\n",
    "{context}\n",
    "'''\n",
    "总结上面的内容\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章标题：基于模式特定专家的时序预测方法：适应分段分布变化的时间序列 forecasting\n",
      "\n",
      "作者：Yanru Sun, Zongxia Xie, Emadeldeen Eldele, Dongyue Chen, Qinghua Hu, Min Wu\n",
      "\n",
      "摘要：\n",
      "时间序列预测，旨在根据历史数据预测未来的值，因其广泛的应用而受到重视。然而，现实世界的时间序列往往表现出不同段的模式演变，如季节变化、 regime切换或上下文变化，这使得准确预测变得困难。现有方法，通常将单个模型训练为捕捉所有这些不同的模式，经常在分段间出现模式 drift，并可能导致泛化不佳。为此，我们提出TFPS（Time Series Pattern-Specific Experts），一种基于模式特定专家的时序预测方法。TFPS通过使用双域编码器来捕获时间域和频率域特征，从而更全面地理解时间动态。然后，它通过动态识别数据段中的不同模式进行子空间聚类。最后，这些模式被建模为专门的专家，允许模型学习多个预测函数。在实验证明中，TFPS在表现出显著分布变化的时间序列数据集上优于现有方法，特别是在这种情况下。数据和代码可用：https://github.com/syrGitHub/TFPS。"
     ]
    }
   ],
   "source": [
    "chain=prompt | model | output_parser\n",
    "for chunk in chain.stream({\"context\":docs[:2000]}):\n",
    "    print(chunk,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 0}, page_content='Learning Pattern-Specific Experts for Time Series\\nForecasting Under Patch-level Distribution Shift\\nYanru Sun, Zongxia Xie∗, Emadeldeen Eldele‡†, Dongyue Chen, Qinghua Hu, Min Wu‡\\nCollege of Intelligence and Computing, Tianjin University, China\\n‡I2R, Agency for Science, Technology and Research, Singapore.\\n†Department of Computer Science, Khalifa University, UAE.\\n{yanrusun, dyuechen, huqinghua}@tju.edu.cn,caddiexie@hotmail.com,\\nemad0002@ntu.edu.sg,wumin@i2r.a-star.edu.sg\\nAbstract\\nTime series forecasting, which aims to predict future values based on historical\\ndata, has garnered significant attention due to its broad range of applications.\\nHowever, real-world time series often exhibit heterogeneous pattern evolution\\nacross segments, such as seasonal variations, regime changes, or contextual shifts,\\nmaking accurate forecasting challenging. Existing approaches, which typically\\ntrain a single model to capture all these diverse patterns, often struggle with the\\npattern drifts between patches and may lead to poor generalization. To address these\\nchallenges, we proposeTFPS, a novel architecture that leverages pattern-specific\\nexperts for more accurate and adaptable time series forecasting. TFPS employs a\\ndual-domain encoder to capture both time-domain and frequency-domain features,\\nenabling a more comprehensive understanding of temporal dynamics. It then\\nperforms subspace clustering to dynamically identify distinct patterns across data\\nsegments. Finally, these patterns are modeled by specialized experts, allowing the\\nmodel to learn multiple predictive functions. Extensive experiments on real-world\\ndatasets demonstrate that TFPS outperforms state-of-the-art methods, particularly\\non datasets exhibiting significant distribution shifts. The data and code are available:\\nhttps://github.com/syrGitHub/TFPS.\\n1 Introduction\\nTime series forecasting plays a critical role in various domains, such as finance [ 18], weather\\n[3, 63, 24], traffic [35, 22], and others [59, 33, 68], by modeling the relationship between historical\\ndata and future outcomes. However, the inherent complexity of time series data, including temporal\\ndependencies and non-stationarity, poses significant challenges in achieving reliable forecasts.\\nRecent Transformer-based models have shown great promise in time series forecasting due to their\\nability to model long-range dependencies [29, 53]. In particular, models like PatchTST [ 44] split\\ncontinuous time series into discrete patches and process them with Transformer blocks. While\\nthese models are effective, a closer examination reveals that patches often exhibit distribution shifts,\\nwhich are frequently associated with concept drift [36]. For example, patches from different regimes,\\nseasons, or operating modes may not only differ in statistical properties [27], but also in the functional\\nrelationships between historical and future values [60, 56]. However, this variability contradicts the\\nassumptions of most existing models [44, 69, 9], which adopt the Uniform Distribution Modeling\\n(UDM) strategy by treating all patches as samples from a single underlying distribution. This\\noversimplified view ignores structural heterogeneity and temporal variation across segments, thereby\\nlimiting the model’s ability to generalize and degrading its forecasting performance [43, 25].\\n∗Corresponding author\\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\\narXiv:2410.09836v2  [cs.LG]  1 Oct 2025'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 1}, page_content='(a) Sudden drift\\n (b) Gradual drift\\nFigure 1: Illustration of distribution shifts between time series patches on the ETTh1 dataset,\\nquantified by Wasserstein distance. The combined time- and frequency-domain views reveal richer\\nand more complementary shift patterns arising from temporal non-stationarity.\\nTo quantify these distributional shifts, we split the ETTh1 dataset into patches and analyze two\\nrepresentative cases: sudden drift and gradual drift, in both time and frequency domains. Specifically,\\nwe compute the Wasserstein distance between patches and visualize the results as heatmaps in Figure 1,\\nwhich clearly illustrate the discrepancies across segments. Notably, sudden drift (Figure 1 (a)) leads\\nto a sharp discrepancy between patches 9 and 10 and the remaining segments, while gradual drift\\n(Figure 1 (b)) reveals that patches 0 to 5 differ from patches 6 to 11, exhibiting a progressive shift that\\nmakes forecasting more challenging. Furthermore, the frequency domain offers a complementary\\nperspective on the shifts [42]. These observations highlight the complex and evolving nature of time\\nseries data, where different segments may follow distinct distributions and exhibit heterogeneous\\ntemporal patterns [55, 27, 17, 56, 52].\\nTo address the challenges posed by distribution shifts in time series data, we propose a novel\\nTime-FrequencyPattern-Specific (TFPS) architecture to effectively model the complex temporal\\npatterns. In particular, TFPS consists of the following three key components. The first is a Dual-\\nDomain Encoder (DDE), which extracts features from both time and frequency domains to provide a\\ncomprehensive representation of the time series data, enabling the model to capture both short-term\\nand long-term dependencies. Second, TFPS addresses the issue of concept drift by incorporating a\\nPattern Identifier (PI), that utilizes a subspace clustering approach to dynamically identify the distinct\\npatterns across patches. This enables the model to effectively handle nonlinear cluster boundaries\\nand accurately assign patches to their corresponding clusters. Finally, TFPS constructs a Mixture\\nof Pattern Experts (MoPE)—a set of specialized expert models, each tailored to a specific pattern\\nidentified by the PI. By dynamically assigning patches to the appropriate experts, TFPS learns\\npattern-specific predictive functions that effectively capture heterogeneous temporal dynamics and\\ndistributional variations. This specialized modeling strategy enhances the model’s adaptability and\\nyields significant forecasting improvements, particularly on datasets with severe distributional drift.\\nIn summary, the key contributions of this work are:\\n• We introduce a novel pattern-specific forecasting paradigm that enables segment-wise expert\\nmodeling based on latent pattern structure, overcoming the limitations of uniform modeling\\nunder distribution shift.\\n• We propose TFPS, a dual-domain framework that integrates time- and frequency-domain\\nrepresentations with subspace clustering and dynamic expert routing, enabling the model to\\nexplicitly adapt to concept drift and capture evolving patterns in non-stationary time series.\\n• We evaluate our approach on nine real-world multivariate time series datasets, demonstrating\\nits effectiveness. Our model achieves top-1 performance in 57 out of 72 settings, showcasing\\nits competitive edge in improving forecasting accuracy.\\n2'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 2}, page_content='2 Related Work\\nTime Series Forecasting Models.In recent years, deep models with elaborately designed archi-\\ntectures have achieved great progress in time series forecasting [ 51, 67, 28, 45]. Approaches like\\nTimesNet [61] and ModernTCN [41] utilize convolutional neural networks with time-series-specific\\nmodifications, making them better suited for forecasting tasks. Additionally, simpler architectures\\nsuch as Multi-Layer Perceptron (MLP)-based models [69, 8] have demonstrated competitive perfor-\\nmance. However, Transformer-based models have gained particular prominence due to their ability\\nto model long-term dependencies in time series [70, 62, 71, 29]. Notably, PatchTST [44] has become\\na widely adopted Transformer variant, introducing a channel-independent patching mechanism to\\nenhance temporal representations. This approach has been further extended by subsequent models\\n[29, 9].\\nWhile previous work has primarily focused on capturing nonlinear dependencies in time series\\nthrough enhanced model structures, our approach addresses the distribution shifts caused by evolving\\npatterns within the data, which is a key limitation of existing methods.\\nNon-stationary Time Series Forecasting.Non-stationarity in time series data complicate predictive\\nmodeling, necessitating effective solutions to handle shifting distributions [ 36, 11]. To address\\nvarying distributions, normalization techniques have emerged as a focal point in recent research,\\naiming to mitigate non-stationary elements and align data with a consistent distribution.\\nFor instance, adaptive norm [46] applies z-score normalization using global statistics and DAIN [47]\\nintroduces a neural layer for adaptively normalizing each input instance. Reversible instance normal-\\nization (RevIN) [20] is proposed to alleviate series shift. Furthermore, Non-stationary transformer\\n[31] points that directly stationarizing time series will damage the model’s capability to capture spe-\\ncific temporal dependencies and introduces an innovative de-stationary attention mechanism within\\nself-attention frameworks. Recent advancement include Dish-TS [10], which identifies both intra-\\nand inter-space distribution shifts in time series data, and SAN [34], which applies normalization at\\nthe slice level, thus opening new avenues for handling non-stationary time series data. Lastly, SIN\\n[17] introduces a novel method to selecting the statistics and learning normalization transformations\\nto capture local invariance in time series data.\\nHowever, normalization methods can only address changes in statistical properties, and over-reliance\\non them may lead to over-stationarization, where meaningful temporal variations are inadvertently\\nsmoothed out [34]. In contrast, our approach preserves the intrinsic non-stationarity of the original\\nseries in the latent representation space, enabling the model to better adapt to evolving regimes by\\ntailoring experts to diverse temporal patterns and distributional structures.\\n3 Method\\n3.1 Preliminaries\\nTime series forecasting aims to uncover relationships between historical time series data and future\\ndata. Let X denote the time series, and xt represent the value at timestep t. Given the historical\\ntime series data X= [x t−L+1,· · ·, x t]∈R L×C, where L is the length of the look-back window\\nand C >1 is the number of features in each timestep, the objective is to predict the future series\\nY= [x t+1,· · ·, x t+H]∈R H×C , whereHis the forecast horizon.\\n3.2 Overall Architecture\\nOur model introduces three novel components: the Dual-Domain Encoder (DDE), the Pattern\\nIdentifier (PI), and the Mixture of Pattern Experts (MoPE), as illustrated in Figure 2. The DDE goes\\nbeyond traditional time-domain encoding by incorporating a frequency encoder that applies Fourier\\nanalysis, transforming time series data into the frequency domain. This enables the model to capture\\nperiodic patterns and frequency-specific features, providing a more comprehensive understanding\\nof the data. The PI is a clustering-based module that distinguishes patches with distinct patterns,\\neffectively addressing the variability in the data. MoPE then utilizes multiple MLP-based experts,\\neach dedicated to modeling a specific pattern, thereby enhancing the model’s ability to adapt to the\\ntemporal dynamics of time series. Collectively, these components form a cohesive framework that\\neffectively handles concept drift between patches, leading to more accurate time series forecasting.\\n3'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 3}, page_content='Figure 2: The structure of our proposed TFPS. The input time series is divided into patches, and\\npositional embeddings are added. These embeddings are processed through two branches: time-\\ndomain branch and frequency-domain branch. Each branch consists of three key components: (1) an\\nencoder to capture patch-wise features, (2) a clustering mechanism to identify patches with similar\\npatterns, and (3) a mixture of pattern experts block to model the patterns of each cluster. Finally, the\\noutputs from both branches are combined for the final prediction.\\n3.3 Embedding Layer\\nFirstly, the input sequence X∈R L×C is divided into patches of length P , resulting in N=\\n⌊ (L−P)\\nS + 2⌋ tokens, where S denotes the stride, defining the non-overlapping region between\\nconsecutive patches. Each patch is denoted as Pi ∈R C×P . These patches are then projected into a\\nnew dimensionD, via a linear transformation, such that,P i → P ′\\ni ∈R C×D.\\nNext, positional embeddings are added to each patch to preserve the temporal ordering disrupted dur-\\ning the segmentation process. The position embedding for the i-th patch, denoted as Ei, is a vector of\\nthe same dimension as the projected patch. The enhanced patch is computed by summing the original\\npatch and its positional embedding: XP Ei =P ′\\ni +E i, and XP E ={X P E1 , XP E2 ,· · ·, X P EN }.\\nNotably, the positional embeddings are learnable parameters, which enables the model to capture\\nthe temporal dependencies in the time series more effectively. As a result, the final enriched patch\\nrepresentations areX P E ∈R C×N×D .\\n3.4 Dual-Domain Encoder\\nAs shown in Figure 1, both time and frequency domains reveal distinct concept drifts that can\\nsignificantly affect the performance of forecasting models. To effectively address these drifts, we\\npropose a Dual-Domain Encoder (DDE) architecture that captures both temporal and frequency\\ndependencies inherent in time series data.\\nWe utilize the patch-based Transformer [44] as an encoder to extract embeddings for each patch,\\ncapturing the global trend feature. The multi-head attention is employed to obtain the attention output\\nOt ∈R N×D :\\nOt =Attention(Q, K, V) =Softmax\\n\\x12 QK T\\n√dk\\n\\x13\\nV,\\nQ=X P EWQ, K=X P EWK, V=X P EWV .\\n(1)\\nThe encoder block also incorporates BatchNorm layers and a feed-forward network with residual\\nconnections, as shown in Figure 2 (b). This process generates the temporal featuresz t ∈R C×N×D .\\nIn parallel with the time encoder, we incorporate a Frequency Encoder by replacing the self-attention\\nsublayer of the Transformer with a Fourier sublayer [26]. This sublayer applies a 2D Fast Fourier\\nTransform (the number of patches, hidden dimension) to the patch representation, expressed as:\\nOf =F patch(Fh(XP E)).(2)\\n4'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 4}, page_content='Figure 3: Illustration of the proposed Pattern Identifier and Mixture of Pattern Experts. The embedded\\nrepresentation z from DDE combines with subspace D to construct the subspace affinity vector,\\nwhich yields the normalized subspace affinity S. Subsequently, the refined subspace affinity ˆS\\nis computed from S to provide self-supervised information. Then, we assign the corresponding\\npatch-wise experts to the embedded representationzaccording toSfor modeling.\\nWe only keep the real part of the result, and hence, we do not modify the feed-forward layers in the\\nTransformer. The structure of the Frequency Encoder is depicted in Figure 2 (c), yielding frequency\\nfeaturesz f ∈R C×N×D .\\nBy modeling data in both the time and frequency domains, the DDE provides a more comprehensive\\nunderstanding of temporal patterns, enabling the model to effectively handle complexities such as\\nconcept drift and evolving dynamics. This dual-domain perspective enhances the model’s robustness\\nand predictive accuracy, offering a versatile foundation for real-world time series forecasting.\\n3.5 Pattern Identifier\\nTo address the complex and evolving patterns in time series data, we introduce a novel Pattern\\nIdentifier (PI) module, an essential innovation within our framework. Unlike traditional approaches\\nthat treat the entire time series uniformly, our PI module dynamically classifies patches based on their\\ndistributional characteristics, enabling a more precise and adaptive modeling strategy.\\nThe core of our approach lies in leveraging subspace clustering to detect concept shifts across multiple\\nsubspaces, as illustrated in Figure 3. The PI module plays a central role by directly analyzing the\\nintrinsic properties of each patch and clustering them into distinct groups based on their latent patterns.\\nIn the time domain, PI enables TFPS to identify shifts in temporal characteristics such as seasonality\\nand trends. In the frequency domain, it captures shifts associated with frequency-specific structures,\\nlike periodic behaviors and spectral changes, providing a comprehensive perspective on evolving\\npatterns throughout the series.\\nTo provide clarity, Figure 3 showcases an application of the PI module exclusively within the\\ntime domain. However, the insights and methodology seamlessly extend to the frequency domain,\\npresenting a unified solution to the challenge of concept shifts.\\nThe PI module iteratively refines subspace bases, which in turn improve representation learning and\\nenable more accurate modeling of evolving patterns. It operates through the following three steps.\\nConstruction of Subspace Bases.We define a new variable D= [D (1),D (2),· · ·,D (K)] to\\nrepresent the bases of K subspaces, where D consists of K blocks, each D(j) ∈R q×d,\\n\\r\\r\\rD(j)\\nu\\n\\r\\r\\r =\\n1, u= 1,· · ·d, j= 1,· · ·, K . To control the column sizes of D, we impose the following constraint:\\nR1 = 1\\n2\\n\\r\\rDT D⊙I−I\\n\\r\\r2\\nF ,(3)\\nwhere⊙denotes the Hadamard product, andIis an identity matrix of sizeKd×Kd.\\n5'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 5}, page_content='Subspaces Differentiation.To ensure the dissimilarity between different subspaces, we introduce\\nthe second constraint:\\nR2 = 1\\n2\\n\\r\\r\\rD(j)T D(l)\\n\\r\\r\\r\\n2\\nF\\n, j̸=l,\\n= 1\\n2\\n\\r\\rDT D⊙O\\n\\r\\r2\\nF ,\\n(4)\\nwhere O is a matrix with all off-diagonal d-size blocks set to 1 and diagonal blocks set to 0.\\nCombiningR 1 andR 2 yields the regularization term forR:\\nR=α(R 1 +R 2),(5)\\nwhereαis a tuning parameters, fixed at10 −3 in this work.\\nSubspace Affinity and Refinement.We propose a novel subspace affinity measure S to assess\\nthe relationship between the embedded representation z from DDE and the subspace bases D. The\\naffinity sij, representing the probability that the embedded zi belongs to the j-th subspace, is defined\\nas:\\nsij =\\n\\r\\rzT\\ni D(j)\\r\\r2\\nF +ηd\\nP\\nj(\\n\\r\\rzT\\ni D(j)\\n\\r\\r2\\nF +ηd)\\n,(6)\\nwhere η is a parameter controlling the smoothness, fixed to the same value as d. To emphasize more\\nconfident assignments, we introduce a refined subspace affinityˆsij:\\nˆsij = s2\\nij/P\\ni sij\\nP\\nj(s2\\nij/P\\ni sij) .(7)\\nThis refinement sharpens the clustering by weighting high-confidence assignments more. The\\nsubspace clustering objective based on the Kullback-Leibler divergence is:\\nLsub =KL( ˆS∥S) =\\nX\\ni\\nX\\nj\\nˆsijlog ˆsij\\nsij\\n.(8)\\nThe clustering loss is defined as:\\nLP I =R+βL sub,(9)\\nwhere β is a hyperparameter balancing the regularization and subspace clustering terms. A detailed\\nsensitivity analysis ofαandβis presented in Appendix J.\\n3.6 Mixture of Pattern Experts\\nTraditional time series forecasting methods often rely on a uniform distribution modeling (UDM)\\napproach, which struggles to adapt to the complexities of diverse and evolving patterns in real-world\\ndata. To address this limitation, we introduce the Mixture of Pattern Experts module (MoPE), which\\nassigns specialized experts to patches based on their unique underlying patterns, enabling more\\nprecise and adaptive forecasting.\\nGiven the cluster assignments s obtained from the PI module, we apply the Patch-wise MoPE to the\\nfeature tensorz∈R C×N×D . The MoPE module consists of the following key components:\\nGating Network.The gating network G calculates the gating weights for each expert based on the\\ncluster assignmentsand selects the topkexperts. The gating weights are computed as:\\nG(s) =Softmax(TopK(s)).(10)\\nHere, the top k logits are selected and normalized using the Softmax function to produce the gating\\nweights.\\nExpert Networks.The MoPE contains K expert networks, denoted as E1, . . . , EK. Each expert\\nnetwork is modeled as an MLP consisting of two linear layers and a ReLU activation. Given a\\npatch-wise featurez, each expert networkE k processes the input to generate its respective output.\\n6'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 6}, page_content='Output Aggregation.The final output h of the MoPE module is a weighted sum of the outputs from\\nall the selected experts, with the weights provided by the gating network:\\nh=\\nKX\\nk=1\\nG(s)Ek(z).(11)\\nAfter the frequency branch is processed by the inverse Fast Fourier transform, the time-frequency\\noutputsh t andh f , are concatenated to formh=concat(h t, hf)∈R C×N×2D .\\nFinally, a linear transformation is applied to the disentangled and pattern-specific representations h to\\ngenerate the prediction: ˆY=Linear(h)∈R H×C .\\nThis approach ensures that the MoPE dynamically assigns and aggregates contributions from various\\nexperts based on evolving patterns, improving the model’s adaptability and accuracy.\\n3.7 Loss Function\\nFollowing [44], we use the Mean Squared Error (MSE) loss to quantify the discrepancy between\\npredicted values ˆY and ground truth values Y : LM SE = ( ˆY−Y) 2. In addition to the MSE loss, we\\nincorporate the clustering regularization loss from the PI module, yielding the final loss function:\\nL=L M SE +L P It +L P If .(12)\\nThis combined loss ensures that the model not only minimizes forecasting errors but also accurately\\nidentifies and maintains the integrity of pattern clusters across time. The algorithm is provided in the\\nAppendix L.\\n4 Experiments\\n4.1 Experimental Setup\\nDatasets and Baselines.We conducted our experiments on nine publicly available real-world\\nmultivariate time series datasets, i.e., ETT (ETTh1, ETTh2, ETTm1, ETTm2), Exchange, Weather,\\nElectricity, Traffic, and ILI. These datasets are provided in [62] for time series forecasting. More\\ndetails about these datasets are included in Appendix A.\\nWe employed a diverse set of state-of-the-art forecasting models as baselines, categorized based on\\nthe type of information they utilize as follows.(1) Time-domain methods:PatchTST [ 44], DLinear\\n[69], TimesNet [61] and iTransformer [29];(2) Frequency-domain methods:FEDformer [ 71] and\\nFITS [64];(3) Time-frequency methods:TFDNet-IK [ 42] and TSLANet [ 9]. We rerun all the\\nexperiments with codes provided by their official implementation.\\nIn addition, we compare TFPS with recent foundation models, including AutoTimes [30], Moment\\n[16], and Timer [32]. We rerun all experiments for a fair comparison: AutoTimes is reproduced using\\nits official implementation, while Moment and Timer are evaluated based on the OpenLTM [32].\\nWe further include comparisons with normalization techniques, MoE-based architectures, and meth-\\nods designed to address distribution shifts. Comprehensive results are presented in Appendix G.\\nExperiments Details.Following previous works [ 44], we used ADAM [ 21] as the default opti-\\nmizer across all the experiments. We employed the MSE and mean absolute error (MAE) as the\\nevaluation metrics, where lower values indicate better performance. A detailed explanation is pro-\\nvided in Appendix E. TFPS was implemented by PyTorch [ 48] and trained on a single NVIDIA\\nRTX 3090 24GB GPU. We conducted grid search to optimize the following three parameters, i.e.,\\nlearning rate={0.0001,0.0005,0.001,0.005,0.01,0.05} , the number of experts in the time domain\\nKt ={1,2,4,8}, and the number of experts in the frequency domainK f ={1,2,4,8}.\\n4.2 Overall Performance Comparison\\nTable 1 highlights the consistent superiority of TFPS across multiple datasets and prediction horizons,\\nsecuring the top performance in 57 out of 72 experimental configurations. In particular, TFPS\\ndemonstrates significant improvements over time-domain methods, with an overall improvement of\\n7'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 7}, page_content='Table 1: Multivariate long-term forecasting results with prediction lengthsH∈ {24,36,48,60}for\\nILI and H∈ {96,192,336,720} for others. The input lengths are L= 104 for ILI and L= 96 for\\nothers. The best results are highlighted inboldand the second best are underlined.\\nModelIMP. TFPS TSLANetFITS iTransformerTFDNet-IKPatchTSTTimesNetDLinearFEDformer(Our) (2024) (2024) (2024) (2023) (2023) (2023) (2023) (2022)MetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -1.1%0.398 0.4130.387 0.4050.3950.4030.3870.4050.396 0.4090.413 0.4190.389 0.4120.398 0.4100.3850.4251924.8%0.423 0.4230.448 0.4360.445 0.4320.441 0.4360.451 0.4410.460 0.4450.441 0.4420.4340.4270.441 0.4613361.8%0.484 0.4610.491 0.4870.489 0.4630.491 0.4630.4950.4620.497 0.4630.491 0.4670.499 0.4770.491 0.4737203.0%0.488 0.4760.505 0.4860.496 0.4850.509 0.4940.4920.4820.501 0.4860.512 0.4910.508 0.5030.501 0.499\\nETTh2\\n96 -2.0%0.313 0.3550.290 0.3450.2950.3440.301 0.3500.289 0.3370.299 0.3480.324 0.3680.315 0.3740.342 0.383192-2.9%0.405 0.4100.362 0.3910.382 0.3960.380 0.3990.3790.3950.383 0.3980.393 0.4100.432 0.4470.434 0.44033610.5%0.392 0.4150.4010.4190.416 0.4250.424 0.4320.416 0.4220.424 0.4310.429 0.4370.486 0.4810.512 0.49772012.6%0.410 0.4330.419 0.4390.4180.4370.430 0.4470.424 0.4410.429 0.4450.433 0.4480.732 0.6140.467 0.476\\nETTm1\\n96 4.1%0.327 0.3670.3290.3680.354 0.3750.342 0.3770.331 0.3690.331 0.3700.337 0.3770.346 0.3740.360 0.4061922.6%0.3740.3950.3760.3830.392 0.3930.383 0.3960.3760.3810.374 0.3950.395 0.4060.382 0.3920.395 0.4273364.2%0.401 0.4080.403 0.4140.425 0.4150.418 0.4180.4050.4100.402 0.4120.433 0.4320.414 0.4140.448 0.458720-0.7%0.479 0.4560.4450.4380.486 0.4490.487 0.4570.4710.4370.466 0.4460.484 0.4580.478 0.4550.491 0.479\\nETTm2\\n96 6.9%0.170 0.2550.179 0.2610.183 0.2660.186 0.2720.176 0.2670.1770.2600.182 0.2620.184 0.2760.193 0.2851927.1%0.235 0.2960.243 0.3030.247 0.3050.254 0.3140.2450.3020.248 0.3060.252 0.3070.282 0.3570.256 0.3243364.6%0.297 0.3350.308 0.3450.307 0.3420.316 0.3510.3030.3400.303 0.3410.312 0.3460.324 0.3640.321 0.3647203.6%0.401 0.3970.403 0.4000.407 0.4010.414 0.4070.4050.3990.405 0.4030.417 0.4040.441 0.4540.434 0.426\\nExchange\\n96 12.7%0.083 0.2050.085 0.2060.088 0.2100.086 0.2060.0840.2050.089 0.2060.105 0.2330.089 0.2190.136 0.26519211.2%0.174 0.2970.178 0.3000.181 0.3040.181 0.3040.1760.2990.178 0.3020.219 0.3420.180 0.3190.279 0.38433610.4%0.310 0.3980.329 0.4150.324 0.4130.338 0.4220.3210.4090.326 0.4110.353 0.4330.313 0.4230.465 0.504720-13.3%1.011 0.7560.850 0.6930.846 0.6960.853 0.6960.835 0.6890.840 0.6900.912 0.7240.8370.6901.169 0.826\\nWeather\\n96 15.6%0.154 0.2020.176 0.2160.167 0.2140.176 0.2160.1650.2090.177 0.2190.168 0.2180.197 0.2570.236 0.32519210.6%0.205 0.2490.226 0.2580.215 0.2570.225 0.2570.2140.2520.225 0.2590.226 0.2670.237 0.2940.268 0.3373369.1%0.262 0.2890.279 0.2990.270 0.2990.281 0.2990.2670.2980.278 0.2980.283 0.3050.283 0.3320.366 0.4027204.1%0.344 0.3420.355 0.3550.3470.3450.358 0.3500.347 0.3460.351 0.3460.355 0.3530.347 0.3820.407 0.422\\nElectricity\\n96 14.6%0.149 0.2360.155 0.2490.200 0.2780.1510.2410.171 0.2540.166 0.2520.168 0.2720.195 0.2770.189 0.30419212.0%0.162 0.2530.170 0.2640.200 0.2810.1670.2580.189 0.2690.174 0.2610.186 0.2890.194 0.2810.198 0.3123360.2%0.200 0.3100.197 0.2820.214 0.2950.179 0.2710.205 0.2840.1900.2770.197 0.2980.207 0.2960.212 0.3267207.2%0.2200.3200.2240.3180.256 0.3280.229 0.3190.247 0.3180.2300.3120.225 0.3220.243 0.3300.242 0.351\\nTraffic\\n96 21.1%0.4270.2960.475 0.3070.651 0.3880.4280.2950.519 0.3140.446 0.2840.586 0.3160.650 0.3970.575 0.35719217.7%0.445 0.2980.478 0.3060.603 0.3640.4480.3020.513 0.3140.453 0.2850.618 0.3230.600 0.3720.613 0.38133617.0%0.459 0.3070.494 0.3120.610 0.3660.4650.3110.525 0.3190.467 0.2910.634 0.3370.606 0.3740.622 0.38072015.1%0.496 0.3130.528 0.3310.648 0.3870.5010.3330.561 0.3360.501 0.4920.659 0.3490.646 0.3960.630 0.383\\nILI\\n24 40.9%1.349 0.7601.749 0.8983.489 1.3732.443 1.0781.8240.8241.614 0.8351.699 0.8712.239 1.0413.217 1.24636 43.6%1.239 0.7521.754 0.9123.530 1.3702.455 1.0861.6990.8131.475 0.8591.733 0.9132.238 1.0492.688 1.07448 40.4%1.461 0.8012.050 0.9843.671 1.3913.437 1.3311.7620.8311.642 0.8802.272 0.9992.252 1.0642.540 1.05760 39.8%1.458 0.8362.240 1.0394.030 1.4622.734 1.1551.7580.8631.608 0.8851.998 0.9742.236 1.0572.782 1.1361stCount 57 3 1 3 6 1 0 0 1\\nTable 2: Ablation study of TFPS components. The model variants in our ablation study include the\\nfollowing configurations across both time and frequency branches: (a) inclusion of the encoder, PI\\nand MoPE; (b) PI replaced with Linear; (c) only the encoder. The best results are inbold.\\nTime Branch Frequency Branch ETTh1 ETTh2\\nEncoder PI MoPE Encoder PI MoPE 96 192 336 720 96 192 336 720\\n✓ ✓ ✓ ✓ ✓ ✓ 0.398 0.423 0.484 0.488 0.313 0.405 0.392 0.410\\n✓ ✓ ✓ 0.401 0.459 0.486 0.492 0.318 0.409 0.400 0.428\\n✓Linear✓ 0.401 0.451 0.494 0.509 0.325 0.411 0.400 0.434\\n✓ 0.414 0.460 0.501 0.500 0.339 0.411 0.426 0.431\\n✓ ✓ ✓ 0.455 0.507 0.539 0.576 0.324 0.407 0.417 0.436\\n✓Linear✓ 0.503 0.535 0.558 0.583 0.398 0.446 0.457 0.444\\n✓ 0.552 0.583 0.591 0.594 0.371 0.426 0.418 0.463\\n9.5% in MSE and 6.4% in MAE. Compared to frequency-domain methods, TFPS shows even more\\npronounced enhancements, with MSE improved by 16.9% and MAE by 12.4%.\\nWhile the time-frequency methods like TSLANet and TFDNet perform competitively on several\\ndatasets, TFPS still outperforms them, showing improvement of 5.2% in MSE and 2.2% in MAE.\\nThese substantial improvements can be attributed to the integration of both time- and frequency-\\ndomain information, combined with our innovative approach to modeling distinct patterns with\\nspecialized experts. By addressing the underlying concept shifts and capturing complex, evolving\\npatterns in time series data, TFPS achieves more accurate predictions than other baselines.\\n8'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 8}, page_content='Table 3: Compared with foundation models.\\nModelIMP. TFPS AutoTimesMoment Timer(Our) (2024) (2024) (2024)\\nMetricMSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh10.2%0.4010.4120.396 0.4280.415 0.4390.3940.417ETTh210.9%0.335 0.3860.3630.4060.381 0.4120.382 0.418ETTm12.4%0.343 0.3740.364 0.3890.348 0.3860.3440.378ETTm27.3%0.248 0.3080.273 0.3270.265 0.3250.2640.321Traffic-3.6%0.398 0.2680.3790.2650.395 0.2730.3790.255Electricity3.7%0.159 0.2490.168 0.2610.163 0.2630.1650.258\\nTable 4: Compared with normalization methods.\\nModel IMP. TFPS DLinear\\nSIN SAN Dish-TS RevIN\\nETTh11.5% 0.448 0.454 0.456 0.461 0.451ETTh22.4% 0.380 0.386 0.388 0.392 0.390ETTm12.3% 0.395 0.4050.399 0.406 0.409ETTm23.3% 0.276 0.2830.280 0.293 0.284Weather5.3% 0.241 0.2530.249 0.263 0.254\\n4.3 Ablation Study\\nTable 2 presents the MSE results of TFPS and its variants with different combinations of encoders,\\nPI, and MoPE.1) Best Result.The full TFPS model, i.e., both the time and frequency branches,\\nalong with their respective encoders, PI, and MoPE are included, performs the best across all the\\nforecast horizons for both datasets.2) Linear vs. PI.We replace PI with a linear layer and find that it\\ngenerally results in higher MSE in most cases, indicating that accurately capturing specific patterns\\nis crucial.3) Impact of Pattern-aware Modeling.Additionally, when comparing the results with\\nthe encoder-only configuration, two variants with MoPE in each branch achieved improved MSE,\\nfurther supporting the necessity of patter-aware modeling.4) Importance of DDE.Furthermore, we\\nfind that both the time encoder and frequency encoder alone yield worse performance, with the time\\nencoder playing a more significant role. In summary, incorporating both branches with PI and MoPE\\nprovides the best performance, while simpler configurations result in higher MSE. See Appendix K\\nfor an in-depth analysis of each component’s contribution.\\n4.4 Comparsion with Foundation Models\\nTo ensure a fair comparison with foundation models, we searched input lengths among 96, 192, 336,\\nand 512. The average results across all forecasting lengths are included in Table 3, with detailed\\nresults provided in Appendix F.2. As shown in Table 3, TFPS consistently outperforms recent\\nfoundation models. Notably, on challenging datasets such as ETTh2 and ETTm2, TFPS achieves\\nsubstantial improvements in MSE by 10.9% and 7.3%, respectively. Although TFPS performs slightly\\nworse on the Traffic dataset, we attribute this to the relatively mild distribution shift observed in\\nTraffic (see Table 5), which may reduce the benefit of our pattern-specific modeling. These results\\nsuggest that TFPS not only matches but often surpasses large-scale foundation models in forecasting\\naccuracy, benefiting from its expert-based design that explicitly captures distributional heterogeneity.\\n4.5 Comparsion with Normalization Methods\\nNormalization methods can reduce fluctuations to enhance performance and are widely used for\\nnon-stationary time series forecasting [17, 34, 10, 31, 20]. We compare our TFPS with these state-of-\\nthe-art normalization methods and Table 4 presents the average MSE across all forecasting lengths for\\neach dataset. While normalization improves stability by enforcing distributional consistency, TFPS\\nretains the intrinsic non-stationarity and models diverse patterns through distribution-specific experts,\\nachieving better adaptability and forecasting accuracy. Detailed results are provided in Appendix G.1.\\n4.6 Visualization\\nWe visualize the prediction curves for ETTh1 withH= 192 . Given that DLinear exhibits competitive\\nperformance in Table 1, we compare its results with those of TFPS in Figure 4 under two scenarios:\\n(a) sudden drift caused by external factors or random events, and (b) gradual drift where the trend is\\ndominant. It is evident that DLinear struggles to achieve accurate predictions in both scenarios. In\\ncontrast, our TFPS consistently produces accurate forecasts despite these challenges, demonstrating\\nits robustness in dealing with various concept dynamics.\\n4.7 Analysis of Experts\\nQualitative Visualizations of Pattern Identifier.Through training, pattern experts in MoPE\\nspontaneously specialize, and we present two examples in Figure 5. We visualize the expert with the\\nhighest score as the routed expert for each instance pair. In the provided examples, we observe that\\n9'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 9}, page_content='(a) Sudden Drift\\n (b) Gradual Drift\\nFigure 4: Visualizations of DLinear and TFPS on the ETTh1 dataset whenH= 192.\\n(a) Expert 0\\n (b) Expert 4\\nFigure 5: Interpretable patterns via PI. Expert-0\\nspecializes in downward trends, while Expert-4\\nfocuses on parabolic trends.\\n(a) ETTh1\\n (b) ETTh2\\nFigure 6: Experiments on the number of experts\\nwhen H= 96 . Further analysis of expert be-\\nhavior is provided in Appendix H.\\nexpert-0 specialize in downward-related concepts, while expert-4 focuses on parabolic trend. These\\nexamples also demonstrate the interpretability of MoPE.\\nNumber of Experts.In Figure 6, we set the learning rate to 0.0001 and conducted four sets of\\nexperiments on the ETTh1 and Weather datasets, Kt = 1, Kf ={1,2,4,8} , to explore the effect of\\nthe number of frequency experts on the results. For example, Kt1Kf 4 means that the TFPS contains\\n1 time experts and 4 frequency experts. We observed that Kt1Kf 2 outperformed Kt1Kf 4 in both\\ncases, suggesting that increasing the number of experts does not always lead to better performance.\\nIn addition, we conducted three experiments based on the optimal number of frequency experts to\\nverify the impact of varying the number of time experts on the results. As shown in Figure 6, the\\nbest results for ETTh1 were obtained with Kt4Kf 2, while for Weather, the optimal results were\\nachieved with Kt4Kf 8. Combined with the average Wasserstein distance in Table 5, we attribute\\nthis to the fact that, in cases where concept drift is more severe, such as Weather, more experts are\\nneeded, whereas fewer experts are sufficient when the drift is less severe.\\n5 Conclusion\\nIn this paper, we propose a novel pattern-aware time series forecasting framework, TFPS, which\\nincorporates a dual-domain mixture of pattern experts approach. Our TFPS framework aims to\\naddress the distribution shift across time series patches and effectively assigns pattern-specific experts\\nto model them. Experimental results across eight diverse datasets demonstrate that TFPS surpasses\\nstate-of-the-art methods in both quantitative metrics and visualizations. Future work will focus on\\ninvestigating evolving distribution shifts, particularly those introduced by the emergence of new\\npatterns, such as unforeseen epidemics or outbreaks.\\n6 Acknowledgments and Disclosure of Funding\\nThis work was supported in part by the National Natural Science Foundation of China under Grants\\nU23B2049, 62376194, 61925602, 62406219, and 62436001; in part by the China Postdoctoral\\nScience Foundation - Tianjin Joint Support Program under Grant 2023T014TJ; and in part by the\\nChina Scholarship Council under Grant 202406250137.\\n10'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 10}, page_content='References\\n[1] Khaled Alkilane, Yihang He, and Der-Horng Lee. Mixmamba: Time series modeling with\\nadaptive expertise.Information Fusion, 112:102589, 2024.\\n[2] Quentin Gregory Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba:\\nMixture of experts for state-space models. InICLR 2024 Workshop on Mathematical and\\nEmpirical Understanding of Foundation Models, 2024.\\n[3] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate\\nmedium-range global weather forecasting with 3d neural networks.Nature, 619(7970):533–538,\\n2023.\\n[4] Yushu Chen, Shengzhuo Liu, Jinzhe Yang, Hao Jing, Wenlai Zhao, and Guangwen Yang. A joint\\ntime-frequency domain transformer for multivariate time series forecasting.Neural Networks,\\n176:106334, 2024.\\n[5] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li,\\nWangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization\\nin mixture-of-experts language models.arXiv preprint arXiv:2401.06066, 2024.\\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. InProceedings of the 2019 conference of\\nthe North American chapter of the association for computational linguistics: human language\\ntechnologies, volume 1 (long and short papers), pages 4171–4186, 2019.\\n[7] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of\\nlanguage models with mixture-of-experts. InInternational Conference on Machine Learning,\\npages 5547–5569. PMLR, 2022.\\n[8] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.\\nTsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. InProceedings\\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages\\n459–469, 2023.\\n[9] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. TSLANet:\\nRethinking transformers for time series representation learning. InForty-first International\\nConference on Machine Learning, 2024.\\n[10] Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou, and Yanjie Fu.\\nDish-ts: a general paradigm for alleviating distribution shift in time series forecasting. In\\nProceedings of the AAAI conference on artificial intelligence, volume 37, pages 7522–7529,\\n2023.\\n[11] Wei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Jiang Bian, and Yanjie Fu. Addressing\\ndistribution shift in time series forecasting with instance normalization flows.arXiv preprint\\narXiv:2401.16777, 2024.\\n[12] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\nparameter models with simple and efficient sparsity.Journal of Machine Learning Research,\\n23(120):1–39, 2022.\\n[13] Yunpeng Gong, Liqing Huang, and Lifei Chen. Eliminate deviation with deviation for data\\naugmentation and a general multi-modal data learning method.arXiv preprint arXiv:2101.08533,\\n2021.\\n[14] Yunpeng Gong, Liqing Huang, and Lifei Chen. Person re-identification method based on color\\nattack and joint defence. InProceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 4313–4322, 2022.\\n[15] Yunpeng Gong, Zhun Zhong, Yansong Qu, Zhiming Luo, Rongrong Ji, and Min Jiang. Cross-\\nmodality perturbation synergy attack for person re-identification.Advances in Neural Informa-\\ntion Processing Systems, 37:23352–23377, 2024.\\n11'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 11}, page_content='[16] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.\\nMoment: A family of open time-series foundation models. InInternational Conference on\\nMachine Learning, pages 16115–16152. PMLR, 2024.\\n[17] Lu Han, Han-Jia Ye, and De-Chuan Zhan. SIN: Selective and interpretable normalization for\\nlong-term time series forecasting. InForty-first International Conference on Machine Learning,\\n2024.\\n[18] Hongbin Huang, Minghua Chen, and Xiao Qiao. Generative learning for financial time series\\nwith irregular and scale-invariant patterns. InThe Twelfth International Conference on Learning\\nRepresentations, 2024.\\n[19] Yushan Jiang, Kanghui Ning, Zijie Pan, Xuyang Shen, Jingchao Ni, Wenchao Yu, Anderson\\nSchneider, Haifeng Chen, Yuriy Nevmyvaka, and Dongjin Song. Multi-modal time series\\nanalysis: A tutorial and survey. InProceedings of the 31st ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining V . 2, pages 6043–6053, 2025.\\n[20] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.\\nReversible instance normalization for accurate time-series forecasting against distribution shift.\\nInInternational Conference on Learning Representations, 2021.\\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint\\narXiv:1412.6980, 2014.\\n[22] Weiyang Kong, Ziyu Guo, and Yubao Liu. Spatio-temporal pivotal graph neural networks\\nfor traffic flow forecasting. InProceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 38, pages 8627–8635, 2024.\\n[23] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term\\ntemporal patterns with deep neural networks. InThe 41st international ACM SIGIR conference\\non research & development in information retrieval, pages 95–104, 2018.\\n[24] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,\\nFerran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning\\nskillful medium-range global weather forecasting.Science, 382(6677):1416–1421, 2023.\\n[25] Sanghyun Lee and Chanyoung Park. Continual traffic forecasting via mixture of experts.arXiv\\npreprint arXiv:2406.03140, 2024.\\n[26] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens\\nwith fourier transforms. InProceedings of the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, pages\\n4296–4313, 2022.\\n[27] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An\\ninvestigation on linear mapping.arXiv preprint arXiv:2305.10721, 2023.\\n[28] Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, and Bin Yang.\\nRethinking irregular time series forecasting: A simple yet effective baseline.arXiv preprint\\narXiv:2505.11250, 2025.\\n[29] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng\\nLong. itransformer: Inverted transformers are effective for time series forecasting. InThe\\nTwelfth International Conference on Learning Representations, 2024.\\n[30] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Autotimes: Au-\\ntoregressive time series forecasters via large language models.Advances in Neural Information\\nProcessing Systems, 37:122154–122184, 2024.\\n[31] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:\\nExploring the stationarity in time series forecasting.Advances in Neural Information Processing\\nSystems, 35:9881–9893, 2022.\\n12'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 12}, page_content='[32] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long.\\nTimer: generative pre-trained transformers are large time series models. InProceedings of the\\n41st International Conference on Machine Learning, pages 32369–32399, 2024.\\n[33] Zhanyu Liu, Guanjie Zheng, and Yanwei Yu. Cross-city few-shot traffic forecasting via traffic\\npattern bank. InProceedings of the 32nd ACM International Conference on Information and\\nKnowledge Management, pages 1451–1460, 2023.\\n[34] Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, and Enhong Chen.\\nAdaptive normalization for non-stationary time series forecasting: A temporal slice perspective.\\nAdvances in Neural Information Processing Systems, 36, 2023.\\n[35] Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, and Yuanchun Zhou. Un-\\nveiling delay effects in traffic forecasting: A perspective from spatial-temporal delay differential\\nequations. InProceedings of the ACM on Web Conference 2024, pages 1035–1044, 2024.\\n[36] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under\\nconcept drift: A review.IEEE transactions on knowledge and data engineering, 31(12):2346–\\n2363, 2018.\\n[37] Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, and Adams Wai-Kin\\nKong. Does flux already know how to perform physically plausible image composition?arXiv\\npreprint arXiv:2509.21278, 2025.\\n[38] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free\\ncross-domain image composition. InProceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 2294–2305, 2023.\\n[39] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept\\nerasure in diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 6430–6440, 2024.\\n[40] Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust water-\\nmarking using generative priors against image editing: From benchmarking to advances.arXiv\\npreprint arXiv:2410.18775, 2024.\\n[41] Donghao Luo and Xue Wang. Moderntcn: A modern pure convolution structure for general\\ntime series analysis. InThe Twelfth International Conference on Learning Representations,\\n2024.\\n[42] Yuxiao Luo, Ziyu Lyu, and Xingyu Huang. Tfdnet: Time-frequency enhanced decomposed\\nnetwork for long-term time series forecasting.arXiv preprint arXiv:2308.13386, 2023.\\n[43] Ronghao Ni, Zinan Lin, Shuaiqi Wang, and Giulia Fanti. Mixture-of-linear-experts for long-\\nterm time series forecasting. InInternational Conference on Artificial Intelligence and Statistics,\\npages 4672–4680. PMLR, 2024.\\n[44] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth\\n64 words: Long-term forecasting with transformers. InThe Eleventh International Conference\\non Learning Representations, 2023.\\n[45] Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y Zhang, Kashif Rasul, Anderson\\nSchneider, Lintao Ma, Yuriy Nevmyvaka, and Dongjin Song. Ts-rag: Retrieval-augmented\\ngeneration based time series foundation models are stronger zero-shot forecaster.arXiv preprint\\narXiv:2503.07649, 2025.\\n[46] Eduardo Ogasawara, Leonardo C Martinez, Daniel De Oliveira, Geraldo Zimbrão, Gisele L\\nPappa, and Marta Mattoso. Adaptive normalization: A novel data normalization approach for\\nnon-stationary time series. InThe 2010 International Joint Conference on Neural Networks\\n(IJCNN), pages 1–8. IEEE, 2010.\\n[47] Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and Alexandros\\nIosifidis. Deep adaptive input normalization for time series forecasting.IEEE transactions on\\nneural networks and learning systems, 31(9):3760–3765, 2019.\\n13'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 13}, page_content='[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\\nstyle, high-performance deep learning library.Advances in neural information processing\\nsystems, 32, 2019.\\n[49] Maciej Pióro, Kamil Ciebiera, Krystian Król, Jan Ludziejewski, Michał Krutul, Jakub Krajewski,\\nSzymon Antoniak, Piotr Miło´s, Marek Cygan, and Sebastian Jaszczur. Moe-mamba: Efficient\\nselective state space models with mixture of experts. InICLR 2024 Workshop on Mathematical\\nand Empirical Understanding of Foundation Models, 2024.\\n[50] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft\\nmixtures of experts.arXiv preprint arXiv:2308.00951, 2023.\\n[51] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo,\\nAoying Zhou, Christian S. Jensen, Zhenli Sheng, and Bin Yang. Tfb: Towards comprehensive\\nand fair benchmarking of time series forecasting methods.Proc. VLDB Endow., 17(9):2363–\\n2377, 2024.\\n[52] Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu, and Bin Yang. Duet: Dual\\nclustering enhanced multivariate time series forecasting. InProceedings of the 31st ACM\\nSIGKDD Conference on Knowledge Discovery and Data Mining, pages 1185–1196, 2025.\\n[53] Xiangfei Qiu, Yuhan Zhu, Zhengyu Li, Hanyin Cheng, Xingjian Wu, Chenjuan Guo, Bin Yang,\\nand Jilin Hu. Dag: A dual causal network for time series forecasting with exogenous variables.\\narXiv preprint arXiv:2509.14933, 2025.\\n[54] Aaditya Ramdas, Nicolás García Trillos, and Marco Cuturi. On wasserstein two-sample testing\\nand related families of nonparametric tests.Entropy, 19(2):47, 2017.\\n[55] Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer\\nthe embedding space for metric learning. InProceedings of the ieee/cvf conference on computer\\nvision and pattern recognition, pages 471–480, 2019.\\n[56] Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao, Tao\\nSun, Guangyin Jin, Xin Cao, et al. Exploring progress in multivariate time series forecasting:\\nComprehensive benchmarking and heterogeneity analysis.IEEE Transactions on Knowledge\\nand Data Engineering, 2024.\\n[57] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\\nexperts layer. InInternational Conference on Learning Representations, 2017.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information\\nprocessing systems, 30, 2017.\\n[59] Hao Wang, Zhiyu Wang, Yunlong Niu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, and\\nXinggao Liu. An accurate and interpretable framework for trustworthy process monitoring.\\nIEEE Transactions on Artificial Intelligence, 2023.\\n[60] Qingsong Wen, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan, et al.\\nOnenet: Enhancing time series forecasting models under concept drift by online ensembling.\\nAdvances in Neural Information Processing Systems, 36:69949–69980, 2023.\\n[61] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:\\nTemporal 2d-variation modeling for general time series analysis. InThe Eleventh International\\nConference on Learning Representations, 2023.\\n[62] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-\\nformers with auto-correlation for long-term series forecasting.Advances in neural information\\nprocessing systems, 34:22419–22430, 2021.\\n14'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 14}, page_content='[63] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting\\nfor worldwide stations with a unified deep model.Nature Machine Intelligence, 5(6):602–611,\\n2023.\\n[64] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: Modeling time series with $10k$ parameters. In\\nThe Twelfth International Conference on Learning Representations, 2024.\\n[65] Kaiwen Yan, Chen Long, Huisi Wu, and Zhenkun Wen. Multi-resolution expansion of analysis\\nin time-frequency domain for time series forecasting.IEEE Transactions on Knowledge and\\nData Engineering, 2024.\\n[66] Zhangjing Yang, Weiwu Yan, Xiaolin Huang, and Lin Mei. Adaptive temporal-frequency\\nnetwork for time-series forecasting.IEEE Transactions on Knowledge and Data Engineering,\\n34(4):1576–1587, 2020.\\n[67] Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Zhulin An,\\nQi Wang, and Yongjun Xu. Ginar+: A robust end-to-end framework for multivariate time series\\nforecasting with missing values.IEEE Transactions on Knowledge and Data Engineering,\\n2025.\\n[68] Chengqing Yu, Fei Wang, Yilun Wang, Zezhi Shao, Tao Sun, Di Yao, and Yongjun Xu. Mgsf-\\nformer: A multi-granularity spatiotemporal fusion transformer for air quality prediction.Infor-\\nmation Fusion, 113:102607, 2025.\\n[69] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\\nforecasting? InProceedings of the AAAI conference on artificial intelligence, volume 37, pages\\n11121–11128, 2023.\\n[70] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\\nProceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106–11115,\\n2021.\\n[71] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\\nFrequency enhanced decomposed transformer for long-term series forecasting. InInternational\\nconference on machine learning, pages 27268–27286. PMLR, 2022.\\n15'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 15}, page_content='NeurIPS Paper Checklist\\n1.Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\\npaper’s contributions and scope?\\nAnswer: [Yes]\\nJustification: The main claims in the abstract and introduction accurately reflect the contribu-\\ntions and scope of the paper. They are supported by the proposed methodology, theoretical\\nmotivation, and extensive empirical results across diverse datasets.\\nGuidelines:\\n• The answer NA means that the abstract and introduction do not include the claims\\nmade in the paper.\\n• The abstract and/or introduction should clearly state the claims made, including the\\ncontributions made in the paper and important assumptions and limitations. A No or\\nNA answer to this question will not be perceived well by the reviewers.\\n• The claims made should match theoretical and experimental results, and reflect how\\nmuch the results can be expected to generalize to other settings.\\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\\nare not attained by the paper.\\n2.Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes]\\nJustification: The paper discusses the limitations of the proposed method in Section N,\\nincluding its assumptions about fixed patch length and potential challenges under evolving\\ndistributions.\\nGuidelines:\\n• The answer NA means that the paper has no limitation while the answer No means that\\nthe paper has limitations, but those are not discussed in the paper.\\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\\n• The paper should point out any strong assumptions and how robust the results are to\\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\\nmodel well-specification, asymptotic approximations only holding locally). The authors\\nshould reflect on how these assumptions might be violated in practice and what the\\nimplications would be.\\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\\nonly tested on a few datasets or with a few runs. In general, empirical results often\\ndepend on implicit assumptions, which should be articulated.\\n• The authors should reflect on the factors that influence the performance of the approach.\\nFor example, a facial recognition algorithm may perform poorly when image resolution\\nis low or images are taken in low lighting. Or a speech-to-text system might not be\\nused reliably to provide closed captions for online lectures because it fails to handle\\ntechnical jargon.\\n• The authors should discuss the computational efficiency of the proposed algorithms\\nand how they scale with dataset size.\\n• If applicable, the authors should discuss possible limitations of their approach to\\naddress problems of privacy and fairness.\\n• While the authors might fear that complete honesty about limitations might be used by\\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\\nlimitations that aren’t acknowledged in the paper. The authors should use their best\\njudgment and recognize that individual actions in favor of transparency play an impor-\\ntant role in developing norms that preserve the integrity of the community. Reviewers\\nwill be specifically instructed to not penalize honesty concerning limitations.\\n3.Theory assumptions and proofs\\n16'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 16}, page_content='Question: For each theoretical result, does the paper provide the full set of assumptions and\\na complete (and correct) proof?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not include theoretical results.\\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\\nreferenced.\\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\\n• The proofs can either appear in the main paper or the supplemental material, but if\\nthey appear in the supplemental material, the authors are encouraged to provide a short\\nproof sketch to provide intuition.\\n• Inversely, any informal proof provided in the core of the paper should be complemented\\nby formal proofs provided in appendix or supplemental material.\\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4.Experimental result reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: The paper provides sufficient details regarding the model architecture, training\\nsettings, data preprocessing, and evaluation metrics, enabling reproduction of the main\\nexperimental results and verification of the core claims.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• If the paper includes experiments, a No answer to this question will not be perceived\\nwell by the reviewers: Making the paper reproducible is important, regardless of\\nwhether the code and data are provided or not.\\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\\nto make their results reproducible or verifiable.\\n• Depending on the contribution, reproducibility can be accomplished in various ways.\\nFor example, if the contribution is a novel architecture, describing the architecture fully\\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\\nbe necessary to either make it possible for others to replicate the model with the same\\ndataset, or provide access to the model. In general. releasing code and data is often\\none good way to accomplish this, but reproducibility can also be provided via detailed\\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\\nof a large language model), releasing of a model checkpoint, or other means that are\\nappropriate to the research performed.\\n• While NeurIPS does not require releasing code, the conference does require all submis-\\nsions to provide some reasonable avenue for reproducibility, which may depend on the\\nnature of the contribution. For example\\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\\nto reproduce that algorithm.\\n(b) If the contribution is primarily a new model architecture, the paper should describe\\nthe architecture clearly and fully.\\n(c) If the contribution is a new model (e.g., a large language model), then there should\\neither be a way to access this model for reproducing the results or a way to reproduce\\nthe model (e.g., with an open-source dataset or instructions for how to construct\\nthe dataset).\\n(d) We recognize that reproducibility may be tricky in some cases, in which case\\nauthors are welcome to describe the particular way they provide for reproducibility.\\nIn the case of closed-source models, it may be that access to the model is limited in\\nsome way (e.g., to registered users), but it should be possible for other researchers\\nto have some path to reproducing or verifying the results.\\n17'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 17}, page_content='5.Open access to data and code\\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\\ntions to faithfully reproduce the main experimental results, as described in supplemental\\nmaterial?\\nAnswer: [Yes]\\nJustification: The source code is included in the supplementary materials to facilitate\\nreproducibility and further research.\\nGuidelines:\\n• The answer NA means that paper does not include experiments requiring code.\\n• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\\npublic/guides/CodeSubmissionPolicy) for more details.\\n• While we encourage the release of code and data, we understand that this might not be\\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\\nincluding code, unless this is central to the contribution (e.g., for a new open-source\\nbenchmark).\\n• The instructions should contain the exact command and environment needed to run to\\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n• The authors should provide instructions on data access and preparation, including how\\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n• The authors should provide scripts to reproduce all experimental results for the new\\nproposed method and baselines. If only a subset of experiments are reproducible, they\\nshould state which ones are omitted from the script and why.\\n• At submission time, to preserve anonymity, the authors should release anonymized\\nversions (if applicable).\\n• Providing as much information as possible in supplemental material (appended to the\\npaper) is recommended, but including URLs to data and code is permitted.\\n6.Experimental setting/details\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\\nresults?\\nAnswer: [Yes]\\nJustification: Training and testing details are described in Appendix A. The hyperparameter\\nsearch ranges and sensitivity analysis are provided in Appendix J.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The experimental setting should be presented in the core of the paper to a level of detail\\nthat is necessary to appreciate the results and make sense of them.\\n• The full details can be provided either with the code, in appendix, or as supplemental\\nmaterial.\\n7.Experiment statistical significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiments?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\\ndence intervals, or statistical significance tests, at least for the experiments that support\\nthe main claims of the paper.\\n• The factors of variability that the error bars are capturing should be clearly stated (for\\nexample, train/test split, initialization, random drawing of some parameter, or overall\\nrun with given experimental conditions).\\n18'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 18}, page_content='• The method for calculating the error bars should be explained (closed form formula,\\ncall to a library function, bootstrap, etc.)\\n• The assumptions made should be given (e.g., Normally distributed errors).\\n• It should be clear whether the error bar is the standard deviation or the standard error\\nof the mean.\\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\\nof Normality of errors is not verified.\\n• For asymmetric distributions, the authors should be careful not to show in tables or\\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\\nerror rates).\\n• If error bars are reported in tables or plots, The authors should explain in the text how\\nthey were calculated and reference the corresponding figures or tables in the text.\\n8.Experiments compute resources\\nQuestion: For each experiment, does the paper provide sufficient information on the com-\\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\\nthe experiments?\\nAnswer: [Yes]\\nJustification: The paper provides sufficient details on the computational resources used,\\nincluding the type of GPU, memory, and training time, in Section 4.1. This information\\nallows readers to estimate the resources required for reproduction.\\nGuidelines:\\n• The answer NA means that the paper does not include experiments.\\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\\nor cloud provider, including relevant memory and storage.\\n• The paper should provide the amount of compute required for each of the individual\\nexperimental runs as well as estimate the total compute.\\n• The paper should disclose whether the full research project required more compute\\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\\ndidn’t make it into the paper).\\n9.Code of ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the\\nNeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: We have read the NeurIPS Code of Ethics, and we are certain that the paper\\nconform to it.\\nGuidelines:\\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n• If the authors answer No, they should explain the special circumstances that require a\\ndeviation from the Code of Ethics.\\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\\neration due to laws or regulations in their jurisdiction).\\n10.Broader impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative\\nsocietal impacts of the work performed?\\nAnswer: [Yes]\\nJustification: The paper discusses both potential positive and negative societal impacts of\\nthe proposed method in Appendix M.\\nGuidelines:\\n• The answer NA means that there is no societal impact of the work performed.\\n19'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 19}, page_content='• If the authors answer NA or No, they should explain why their work has no societal\\nimpact or why the paper does not address societal impact.\\n• Examples of negative societal impacts include potential malicious or unintended uses\\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\\ngroups), privacy considerations, and security considerations.\\n• The conference expects that many papers will be foundational research and not tied\\nto particular applications, let alone deployments. However, if there is a direct path to\\nany negative applications, the authors should point it out. For example, it is legitimate\\nto point out that an improvement in the quality of generative models could be used to\\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\\nthat a generic algorithm for optimizing neural networks could enable people to train\\nmodels that generate Deepfakes faster.\\n• The authors should consider possible harms that could arise when the technology is\\nbeing used as intended and functioning correctly, harms that could arise when the\\ntechnology is being used as intended but gives incorrect results, and harms following\\nfrom (intentional or unintentional) misuse of the technology.\\n• If there are negative societal impacts, the authors could also discuss possible mitigation\\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\\nfeedback over time, improving the efficiency and accessibility of ML).\\n11.Safeguards\\nQuestion: Does the paper describe safeguards that have been put in place for responsible\\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\\nimage generators, or scraped datasets)?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper poses no such risks.\\n• Released models that have a high risk for misuse or dual-use should be released with\\nnecessary safeguards to allow for controlled use of the model, for example by requiring\\nthat users adhere to usage guidelines or restrictions to access the model or implementing\\nsafety filters.\\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\\nshould describe how they avoided releasing unsafe images.\\n• We recognize that providing effective safeguards is challenging, and many papers do\\nnot require this, but we encourage authors to take this into account and make a best\\nfaith effort.\\n12.Licenses for existing assets\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected?\\nAnswer: [Yes]\\nJustification: All external assets used in the paper, including datasets and code, are properly\\ncredited. Their licenses and terms of use are explicitly acknowledged and respected.\\nGuidelines:\\n• The answer NA means that the paper does not use existing assets.\\n• The authors should cite the original paper that produced the code package or dataset.\\n• The authors should state which version of the asset is used and, if possible, include a\\nURL.\\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n• For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n20'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 20}, page_content='• If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com/datasets\\nhas curated licenses for some datasets. Their licensing guide can help determine the\\nlicense of a dataset.\\n• For existing datasets that are re-packaged, both the original license and the license of\\nthe derived asset (if it has changed) should be provided.\\n• If this information is not available online, the authors are encouraged to reach out to\\nthe asset’s creators.\\n13.New assets\\nQuestion: Are new assets introduced in the paper well documented and is the documentation\\nprovided alongside the assets?\\nAnswer: [Yes]\\nJustification: The new assets introduced in this paper, including the implementation code\\nand model components, are well documented. Documentation is provided alongside the\\ncode in the supplementary materials.\\nGuidelines:\\n• The answer NA means that the paper does not release new assets.\\n• Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n• The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n• At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14.Crowdsourcing and research with human subjects\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\\ninclude the full text of instructions given to participants and screenshots, if applicable, as\\nwell as details about compensation (if any)?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n• Including this information in the supplemental material is fine, but if the main contribu-\\ntion of the paper involves human subjects, then as much detail as possible should be\\nincluded in the main paper.\\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\\nor other labor should be paid at least the minimum wage in the country of the data\\ncollector.\\n15. Institutional review board (IRB) approvals or equivalent for research with human\\nsubjects\\nQuestion: Does the paper describe potential risks incurred by study participants, whether\\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\\napprovals (or an equivalent approval/review based on the requirements of your country or\\ninstitution) were obtained?\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\\nmay be required for any human subjects research. If you obtained IRB approval, you\\nshould clearly state this in the paper.\\n21'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 21}, page_content='• We recognize that the procedures for this may vary significantly between institutions\\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\\nguidelines for their institution.\\n• For initial submissions, do not include any information that would break anonymity (if\\napplicable), such as the institution conducting the review.\\n16.Declaration of LLM usage\\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\\nnon-standard component of the core methods in this research? Note that if the LLM is used\\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\\nscientific rigorousness, or originality of the research, declaration is not required.\\nAnswer: [NA]\\nGuidelines:\\n• The answer NA means that the core method development in this research does not\\ninvolve LLMs as any important, original, or non-standard components.\\n• Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM)\\nfor what should or should not be described.\\n22'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 22}, page_content='A Dataset\\nWe evaluate the performance of TFPS on eight widely used datasets, including four ETT datasets\\n(ETTh1, ETTh2, ETTm1 and ETTm2), Exchange, Weather, Electricity, and ILI. This subsection\\nprovides a summary of the datasets:\\n• ETT 2 [70] (Electricity Transformer Temperature) dataset contains two electric transformers,\\nETT1 and ETT2, collected from two separate counties. Each of them has two versions of\\nsampling resolutions (15min & 1h). Thus, there are four ETT datasets:ETTm1,ETTm2,\\nETTh1, andETTh2.\\n• Exchange-Rate 3 [23] the exchange-rate dataset contains the daily exchange rates of eight\\nforeign countries including Australia, British, Canada, Switzerland, China, Japan, New\\nZealand, and Singapore ranging from 1990 to 2016.\\n• Weather 4 [62] dataset contains 21 meteorological indicators in Germany, such as humidity\\nand air temperature.\\n• Electricity 5 [62] is a dataset that describes 321 customers’ hourly electricity consumption.\\n• Traffic 6 [62] is a dataset featuring hourly road occupancy rates from 862 sensors along the\\nfreeways in the San Francisco Bay area.\\n• ILI 7 [62] dataset collects the number of patients and influenza-like illness ratio in a weekly\\nfrequency.\\nFor the data split, we follow [69] and split the data into training, validation, and testing by a ratio of\\n6:2:2 for the ETT datasets and 7:1:2 for the others. Details are shown in Table 5. The best parameters\\nare selected based on the lowest validation loss and then applied to the test set for performance\\nevaluation. The data and codes are available:https://github.com/syrGitHub/TFPS.\\nTable 5: The statistics of the datasets.\\nDatasets Variates Prediction Length Timesteps GranularityAverage Wasserstein*\\n(Time Domain)Average Wasserstein*\\n(Frequency Domain)\\nETTh1 7 {96, 192, 336, 720} 17,420 1 hour 9.268 11.561ETTh2 7 {96, 192, 336, 720} 17,420 1 hour 13.221 18.970ETTm1 7 {96, 192, 336, 720} 69,680 15 min 9.336 10.660ETTm2 7 {96, 192, 336, 720} 69,680 15 min 13.606 16.574Exchange-Rate8 {96, 192, 336, 720} 7,588 1 day 0.132 0.144Weather 21 {96, 192, 336, 720} 52,696 10 min 39.742 77.422Electricity 321 {96, 192, 336, 720} 26,304 1 hour 520.162 1018.311Traffic 862 {96, 192, 336, 720} 17,451 1 hour 0.011 0.028ILI 7 {24, 36, 48, 60} 966 1 week 258881.714 381377.494\\n*A large Wasserstein indicates a more severe drift.\\nB Related Work\\nDeep learning has achieved remarkable success across diverse domains such as computer vision\\n[14, 38, 39, 40, 37], natural language processing [58, 6], and multi-modality [13, 15], and has also\\nadvanced the state of the art in time series modeling [56, 19].\\nThe Combination of Time and Frequency Domains.Time-domain models excel at capturing\\nsequential trends, while frequency-domain models are essential for identifying periodic and oscillatory\\npatterns. Recent research has increasingly emphasized integrating information from both domains to\\nbetter interpret underlying patterns. For instance, ATFN [66] demonstrates the advantage of frequency\\ndomain methods for forecasting strongly periodic time series through a time–frequency adaptive\\n2https://github.com/zhouhaoyi/ETDataset\\n3https://github.com/laiguokun/multivariate-time-series-data\\n4https://www.bgc-jena.mpg.de/wetter/\\n5https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\\n6http://pems.dot.ca.gov\\n7https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\\n23'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 23}, page_content='network. TFDNet [42] adopts a branching structure to capture long-term latent patterns and temporal\\nperiodicity from both domains. Similarly, JTFT [4] utilizes the frequency domain representation to\\nextract multi-scale dependencies while enhancing local relationships modeling through time domain\\nrepresentation. TFMRN [65] expands data in both domains to capture finer details that may not be\\nevident in the original data. Recently, TSLANet [9] leverages Fourier analysis to enhance feature\\nrepresentation and capture both long-term and short-term interactions.\\nBuilding on these approaches, our proposed method, TFPS, introduces a novel Dual-Domain Encoder\\nthat effectively combines time and frequency domain information to capture both trend and periodic\\npatterns. By integrating time-frequency features, TFPS significantly advances the field in addressing\\nthe complexities inherent in time series forecasting.\\nMixture-of-Experts.Mixture-of-Experts (MoE) models have gained attention for their ability to\\nscale efficiently by activating only a subset of experts for each input, as first introduced by [ 57].\\nDespite their success, challenges such as training instability, expert redundancy, and limited expert\\nspecialization have been identified [50, 5]. These issues hinder the full potential of MoE models in\\nreal-world tasks.\\nRecent advances have integrated MoE with Transformers to improve scalability and efficiency. For\\nexample, GLaM [7] and Switch Transformer [12] interleave MoE layers with Transformer blocks,\\nreducing computational costs. Other models like state space models (SSMs) [49, 2], [1] combines\\nMoE with alternative architectures for enhanced scalability and inference speed.\\nIn contrast, our approach introduces MoE into time series forecasting by assigning experts to specific\\ntime-frequency patterns, enabling more effective, patch-level adaptation. This approach represents a\\nsignificant innovation in time series forecasting, offering a more targeted and effective way to handle\\nvarying patterns across both time and frequency domains.\\nC Wasserstein Distance\\nThe Wasserstein distance, also called the Earth mover’s distance or the optimal transport distance, is a\\nsimilarity metric between two probability distributions. In the discrete case, the Wasserstein distance\\ncan be understood as the cost of an optimal transport plan to convert one distribution into the other.\\nThe cost is calculated as the product of the amount of probability mass being moved and the distance\\nit is being moved.\\nGiven two one-dimensional probability mass functions,u and v, the first Wasserstein distance between\\nthem is defined as:\\nl1(u, v) = inf\\nπ∈Γ(u,v)\\nZ\\nR×R\\n|x−y|dπ(x, y),(13)\\nwhere Γ(u, v) is the set of (probability) distributions on R×R whose marginals are u and v on the\\nfirst and second factors respectively. Here, u(x) represents the probability of u at position x, with the\\nsame interpretation forv(x).\\nIn the special case of one-dimensional distributions, the Wasserstein distance can be equivalently\\nexpressed using their cumulative distribution functions (CDFs),UandV, as:\\nl1(u, v) =\\nZ +∞\\n−∞\\n|U−V|.(14)\\nThis equivalence is rigorously proved in [54].\\nThe input distributions can be empirical, therefore coming from samples whose values are effectively\\ninputs of the function, or they can be seen as generalized functions, in which case they are weighted\\nsums of Dirac delta functions located at the specified values.\\nD Distribution Shifts in both Time and Frequency Domains\\nThe time seriesX is segmented intoN patches, where each patchPn ={x n1, xn2, . . . , xnP } consists\\nof P consecutive timesteps for n= 1,2,· · ·, N . For the frequency domain, we apply a Fourier\\ntransformFto each patchP n, obtaining its frequency-domain representation as ˆPn =F(P n).\\n24'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 24}, page_content='Each patch’s probability distribution in the time domain is denoted as pt(Pn), representing the\\nstatistical properties of Pn, while its frequency domain distribution, denoted as pf( ˆPn), captures its\\nspectral characteristics.\\nThe distribution shifts between two patches Pi and Pj are characterized by the comparing their\\nprobability distributions in both time and frequency domains. These shifts are defined as:\\nDt(Pi,P j) =|d(p t(Pi), pt(Pj))|> θ,(15)\\nDf( ˆPi, ˆPj) =|d(p f( ˆPi), pf( ˆPj))|> θ,(16)\\nwhere d is a distance metric, such as Wasserstein distance or Kullback-Leibler divergence, andθ is\\na threshold indicating a significant distribution shift. If Dt(Pi,P j) or Df( ˆPi, ˆPj) exceeds θ, this\\nimplies a significant distribution shift between the two patches in either domain. It is important to\\nnote that θ serves only as a conceptual threshold for defining distribution shifts in the analysis and\\ndoes not participate in the modeling or training process of TFPS.\\nE Metric Illustration\\nWe use mean square error (MSE) and mean absolute error (MAE) as our metrics for evaluation of all\\nforecasting models. Then calculation of MSE and MAE can be described as:\\nMSE= 1\\nH\\nL+HX\\ni=L+1\\n( ˆYi −Y i)2,(17)\\nMAE= 1\\nH\\nL+HX\\ni=L+1\\n\\x0c\\x0c\\x0c ˆYi −Y i\\n\\x0c\\x0c\\x0c ,(18)\\nwhere ˆYis predicted vector withHfuture values, whileYis the ground truth.\\nIn addition, we report the IMP (Improvement) metric, which is defined as:\\nIMP= Avg MSE of baselines−MSE of TFPS\\nAvg MSE of baselines ×100%.(19)\\nThis metric quantifies the relative percentage improvement of TFPS over the average MSE of all\\nbaseline methods. A higher IMP value indicates better overall performance of TFPS compared to the\\nbaselines.\\nF Hyperparameter-search Results\\nF.1 Comparsion with Specific Models\\nTo ensure a fair comparison between models, we conducted experiments using unified parameters\\nand reported results in the main text.\\nIn addition, considering that the reported results in different papers are mostly obtained through\\nhyperparameter search, we provide the experiment results with the full version of the parameter\\nsearch. We searched for input length among 96, 192, 336, and 512. The results are included in\\nTable 6. All baselines are reproduced by their official code.\\nWe can find that the relative promotion of TFPS over TFDNet is smaller under comprehensive\\nhyperparameter search than the unified hyperparameter setting. It is worth noticing that TFPS runs\\nmuch faster than TFDNet according to the efficiency comparison in Table 12. Therefore, considering\\nperformance, hyperparameter-search cost and efficiency, we believe TFPS is a practical model in\\nreal-world applications and is valuable to deep time series forecasting community.\\nF.2 Comparsion with Foundation Models\\nTable 7 presents the detailed results of TFPS compared to recent foundation models (AutoTimes\\n[30], Moment [ 16], and Timer [ 32]) across six datasets and four forecasting lengths. Consistent\\n25'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 25}, page_content='Table 6: Experiment results under hyperparameter searching for the long-term forecasting task. The\\nbest results are highlighted inboldand the second best are underlined.\\nModelIMP. TFPS TSLANetFITS iTransformerTFDNet-IKPatchTSTTimesNetDlinearFEDformer(Our) (2024) (2024) (2024) (2023) (2023) (2023) (2023) (2022)MetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 1.5%0.372 0.4040.3680.3940.374 0.3950.387 0.4050.360 0.3870.375 0.4000.389 0.4120.384 0.4050.385 0.4251925.7%0.401 0.4100.413 0.4180.407 0.4140.441 0.4360.4030.4120.414 0.4210.441 0.4420.443 0.4500.441 0.4613369.8%0.409 0.4020.4120.4160.429 0.4280.491 0.4630.434 0.4290.432 0.4360.491 0.4670.447 0.4480.491 0.47372011.2%0.423 0.4330.473 0.4770.4250.4460.509 0.4940.437 0.4520.450 0.4660.512 0.4910.504 0.5150.501 0.499\\nETTh2\\n96 9.3%0.268 0.3250.283 0.3440.274 0.3370.301 0.3500.2710.3290.278 0.3360.324 0.3680.290 0.3530.342 0.38319210.4%0.3290.3760.3310.3780.337 0.3770.380 0.3990.3330.3720.339 0.3800.393 0.4100.388 0.4220.434 0.44033617.7%0.329 0.4010.319 0.3770.360 0.3980.424 0.4320.361 0.3960.3360.3800.429 0.4370.463 0.4730.512 0.4977209.0%0.412 0.4410.407 0.4490.386 0.4230.430 0.4470.3820.4180.3820.4210.433 0.4480.733 0.6060.467 0.476\\nETTm1\\n9610.2%0.281 0.3290.291 0.3530.303 0.3450.342 0.3770.2830.3300.288 0.3420.337 0.3770.301 0.3450.360 0.4061928.5%0.324 0.3540.329 0.3720.337 0.3650.383 0.3960.3270.3560.334 0.3720.395 0.4060.336 0.3660.395 0.4273368.2%0.359 0.4040.3570.3920.3720.3850.418 0.4180.3610.3750.367 0.3930.433 0.4320.372 0.3890.448 0.4587208.2%0.409 0.4080.423 0.4250.428 0.4160.487 0.4570.4110.4090.417 0.4220.484 0.4580.427 0.4230.491 0.479\\nETTm2\\n96 8.9%0.158 0.2430.167 0.2560.165 0.2550.186 0.2720.1580.2440.164 0.2530.182 0.2620.172 0.2670.193 0.2851925.7%0.222 0.3020.221 0.2940.2200.2910.254 0.3140.219 0.2820.221 0.2920.252 0.3070.237 0.3140.256 0.3243368.5%0.268 0.3160.277 0.3290.274 0.3260.316 0.3510.2730.3170.277 0.3290.312 0.3460.295 0.3590.321 0.36472012.0%0.344 0.3730.356 0.3820.367 0.3830.414 0.4070.3460.3740.365 0.3840.417 0.4040.427 0.4390.434 0.426\\nTraffic\\n9617.8%0.370 0.2500.375 0.2600.398 0.2850.428 0.2950.3770.2530.373 0.2590.586 0.3160.413 0.2870.575 0.35719217.0%0.390 0.2580.395 0.2720.408 0.2880.448 0.3020.3910.2600.395 0.2730.618 0.3230.424 0.2900.613 0.38133617.2%0.4010.2710.402 0.2720.420 0.2920.465 0.3110.4080.2660.402 0.2740.634 0.3370.438 0.2990.622 0.38072015.7%0.432 0.2940.431 0.2880.448 0.3100.501 0.3330.4510.2910.435 0.2930.659 0.3490.466 0.3160.630 0.383\\nElectricity\\n9610.3%0.128 0.2200.137 0.2290.135 0.2310.148 0.2390.1300.2220.130 0.2230.168 0.2720.140 0.2370.188 0.30319211.9%0.145 0.2350.153 0.2420.149 0.2440.167 0.2580.1460.2370.149 0.2400.186 0.2890.154 0.2500.197 0.3113366.8%0.1660.2580.165 0.2630.165 0.2600.178 0.2710.162 0.2540.168 0.2620.196 0.2970.169 0.2680.212 0.3277206.9%0.198 0.2830.206 0.2940.204 0.2930.211 0.3000.2010.2870.204 0.2890.235 0.3290.204 0.3000.243 0.3521stCount 32 5 0 0 10 1 0 0 0\\nTable 7: Detailed results of the comparison between TFPS and foundation models. The best results\\nare highlighted inboldand the second best are underlined.\\nModel IMP. TFPS AutoTimesMoment Timer(Our) (2024) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -2.2%0.3720.4040.365 0.4050.369 0.4060.359 0.392192-1.3%0.4010.4100.392 0.4230.405 0.4310.3910.413336 0.5%0.4090.4020.4060.4330.420 0.4410.4070.424720 3.1%0.4230.4330.423 0.4500.466 0.4790.4210.441\\nETTh2\\n96 6.3%0.268 0.3250.286 0.3490.287 0.3470.2850.344192 9.2%0.329 0.3760.3510.3930.371 0.4010.365 0.40033617.2%0.329 0.4010.3770.4170.404 0.4250.412 0.440720 9.8%0.412 0.4410.4390.4640.463 0.4760.467 0.487\\nETTm1\\n96 1.3%0.2810.3290.297 0.3500.281 0.3430.2760.335192 1.2%0.3240.3540.344 0.3770.3180.3680.3230.365336 1.6%0.359 0.4040.380 0.3980.3560.3910.3580.388720 4.8%0.409 0.4080.433 0.4310.438 0.4410.4190.423\\nETTm2\\n96 8.6%0.158 0.2430.181 0.2660.170 0.2580.1670.254192 5.5%0.2220.3020.241 0.3060.2330.3010.2290.297336 6.9%0.268 0.3160.295 0.3410.287 0.3400.2820.335720 8.1%0.344 0.3730.3760.3930.371 0.3990.376 0.398\\nTraffic\\n96 -5.5%0.370 0.2500.3470.2490.360 0.2540.345 0.237192-5.3%0.390 0.2580.3660.2580.381 0.2650.365 0.247336-3.1%0.401 0.2710.3830.2670.404 0.2770.381 0.256720-1.1%0.432 0.2940.4200.2860.438 0.2970.4240.280\\nElectricity\\n96 3.4%0.128 0.2200.135 0.2300.133 0.2360.1300.224192 4.3%0.145 0.2350.153 0.2470.152 0.2540.1490.243336 1.5%0.166 0.2580.172 0.2660.166 0.2650.1680.263720 5.2%0.198 0.2830.212 0.3000.2020.2980.213 0.303\\n1stCount 30 2 2 14\\nwith Table 6, for TFPS, we searched input lengths among 96, 192, 336, and 512, and report the\\nbest-performing configuration for each forecasting length.\\nTFPS achieves the best performance in30 out of 48settings, significantly outperforming AutoTimes\\nand Moment, which only achieve 2 best scores each. Although Timer demonstrates competitive\\n26'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 26}, page_content='Table 8: Detailed results of the comparison between TFPS and normalization-based methods using\\nFEDformer. The best results are highlighted inboldand the second best are underlined.\\nFEDformerTFPS + SIN + SAN + Dish-TS+ RevINModelIMP. (Our) (2024) (2023) (2023) (2021)\\nMetricMSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\n96 -0.9%0.398 0.4130.4130.3720.3830.4090.390 0.4240.392 0.413192 3.7%0.4230.4230.4430.4170.431 0.4380.441 0.4580.443 0.444336-0.5%0.484 0.4610.465 0.4480.4710.4560.495 0.4860.495 0.467\\nETTh1720 4.8%0.488 0.4760.509 0.4900.5040.4880.519 0.5090.520 0.498\\n96 34.0%0.3130.3550.412 0.3570.3000.3550.806 0.5890.380 0.40219228.3%0.4050.4100.472 0.4530.3920.4130.936 0.6590.457 0.44333638.2%0.392 0.4150.527 0.5270.4590.4621.039 0.7020.515 0.479\\nETTh272041.4%0.410 0.4330.593 0.6390.4620.4721.237 0.7590.507 0.487\\n96 4.5%0.327 0.3670.3730.3200.3110.3550.348 0.3970.340 0.385192 2.9%0.374 0.3950.3940.3660.3510.3830.406 0.4280.390 0.411336 4.4%0.401 0.4080.4180.4050.3900.4070.438 0.4500.432 0.436\\nETTm1720-0.8%0.4790.4560.4510.4750.4560.4440.497 0.4810.497 0.466\\n96 37.5%0.1700.2550.3260.2110.175 0.2660.394 0.3950.192 0.27219235.9%0.235 0.2960.402 0.3160.2460.3150.552 0.4720.270 0.32033638.6%0.297 0.3350.465 0.3990.3150.3620.808 0.6010.348 0.367\\nETTm272040.2%0.401 0.3970.555 0.5470.412 0.4221.282 0.7710.4300.415\\n96 30.7%0.154 0.2020.2800.2150.179 0.2390.244 0.3170.187 0.23419225.6%0.205 0.2490.3140.2640.234 0.2960.320 0.3800.235 0.27233622.0%0.262 0.2890.3290.2930.304 0.3480.424 0.4520.287 0.307\\nWeather72021.3%0.344 0.3420.3820.3700.400 0.4040.604 0.5530.361 0.353\\n1stCount 24 9 7 0 0\\nTable 9: Detailed results of the comparison between TFPS and normalization-based methods using\\nDLinear. The best results are highlighted inboldand the second best are underlined.\\nDLinearTFPS + SIN + SAN + Dish-TS+ RevINModel IMP. (Our) (2024) (2023) (2023) (2021)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -1.6%0.398 0.4130.401 0.4150.385 0.3950.3890.3990.393 0.4161922.9%0.423 0.4230.438 0.4560.4320.4230.443 0.4330.431 0.428336-0.4%0.484 0.4610.462 0.4460.490 0.4630.4870.4560.488 0.4837204.6%0.488 0.4760.515 0.5000.516 0.5040.523 0.5080.4930.482\\nETTh2\\n96 4.9%0.313 0.3550.3590.3590.319 0.3640.317 0.3650.322 0.3611921.1%0.405 0.4100.409 0.4240.407 0.4390.4080.4200.412 0.4243363.6%0.392 0.4150.398 0.4290.4110.4250.416 0.4260.403 0.4277202.7%0.410 0.4330.419 0.4420.417 0.4410.4280.4390.422 0.446\\nETTm1\\n96 4.9%0.327 0.3670.350 0.3830.333 0.3740.343 0.3750.3520.3691921.9%0.3740.3950.383 0.3960.374 0.3960.3810.3910.388 0.3963363.0%0.401 0.4080.413 0.4160.406 0.4180.416 0.4170.4190.4147200.0%0.479 0.4560.4730.4520.4830.4510.482 0.4580.478 0.463\\nETTm2\\n96 5.5%0.170 0.2550.172 0.2830.179 0.2720.1890.2640.179 0.2691924.4%0.235 0.2960.2490.3010.239 0.3160.249 0.3020.248 0.3023361.2%0.297 0.3350.2990.3390.301 0.3530.305 0.3490.299 0.3457203.2%0.401 0.3970.412 0.4210.404 0.4080.4290.4020.411 0.402\\nWeather\\n96 4.5%0.154 0.2020.162 0.2230.1570.2150.173 0.2410.154 0.2431927.6%0.205 0.2490.216 0.2590.2140.2580.225 0.2630.233 0.2653366.8%0.262 0.2890.2790.2910.275 0.2920.289 0.3050.282 0.2937203.0%0.344 0.3420.3550.3410.349 0.3510.366 0.3690.348 0.362\\n1stCount 33 3 3 1 0\\nperformance (with 14 best scores), TFPS consistently achieves lower error in datasets with higher\\ndistribution shifts, such as ETTh2 and ETTm2, indicating its advantage in handling pattern hetero-\\ngeneity.\\nThese results reinforce the effectiveness of TFPS’s pattern-specific modeling strategy, especially in\\nscenarios where traditional large-scale foundation models struggle to generalize.\\n27'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 27}, page_content='Table 10: Comparison between TFPS and MoE-based methods. The best results are highlighted in\\nboldand the second best are underlined.\\nModel IMP. TFPS MoLE MoU KAN4TSF(Our) (2024) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 -4.3%0.398 0.4130.3830.3920.3810.4030.3820.400192 1.7%0.423 0.4230.4340.4260.429 0.4300.430 0.426336 1.6%0.484 0.4610.489 0.4780.4880.4630.498 0.467720 8.2%0.488 0.4760.602 0.5450.499 0.4840.4940.479\\nETTh2\\n96 10.4%0.313 0.3550.413 0.3600.3170.3580.318 0.35819210.3%0.405 0.4100.525 0.4160.4090.4140.419 0.414336 7.1%0.392 0.4150.423 0.4340.3970.4200.447 0.452720 8.4%0.410 0.4330.453 0.4580.4120.4340.477 0.476\\nETTm1\\n96 13.5%0.327 0.3670.338 0.3800.465 0.4420.333 0.37119210.6%0.374 0.3950.388 0.4030.483 0.4550.3840.39933611.8%0.401 0.4080.417 0.4310.540 0.4880.4070.413720 7.3%0.479 0.4560.486 0.4720.583 0.5090.4830.469\\nETTm2\\n96 13.9%0.170 0.2550.238 0.2710.179 0.2630.1750.260192 3.8%0.235 0.2960.247 0.3050.2430.3030.244 0.305336 3.3%0.297 0.3350.308 0.3430.3060.3430.308 0.34772013.7%0.401 0.3970.583 0.4190.4050.4040.405 0.404\\n1stCount 30 1 1 0\\nG Compared with Other Methods\\nG.1 Compared with Normalization Methods\\nIn this section, we provide the detailed experimental results of the comparison between TFPS and five\\nstate-of-the-art normalization methods for non-stationary time series forecasting: SIN [17], SAN [34],\\nDish-TS [10], and RevIN [20]. We summarize the forecasting results of TFPS and baseline models in\\nTable 8 and Table 9. Specifically, the results of FEDformer combined with SIN are taken from [17],\\nwhile those of FEDformer with other normalization-based methods are reported by [34]. For a fair\\ncomparison, we additionally rerun all experiments for DLinear combined with each normalization\\nmethod.\\nTable 8 and Table 9 presents the forecasting performance across all prediction lengths for each dataset,\\nalong with the relative improvements of TFPS over existing methods. As shown, TFPS consistently\\nachieves the best performance in the majority of settings, demonstrating its strong adaptability to\\ndistributional and conceptual drifts in time series data.\\nWe attribute this improvement to the accurate identification of pattern groups and the provision\\nof specialized experts for each group, thereby avoiding the over-stationarization problem often\\nassociated with normalization methods.\\nG.2 Compared with MoE-based Methods\\nAs shown in Table 10, unlike MoE-based methods that rely on the Softmax function as a gating\\nmechanism, our approach constructs a pattern recognizer to assign different experts to handle distinct\\npatterns. This results in TFPS achieving relative improvements of 2.3%, 9.0%, 10.6%, and 9.1%\\nacross the four datasets, respectively.\\nG.3 Compared with Distribution Shift Methods\\nAs shown in Table 11, we compare with the methods for distribution shift. This results in TFPS\\nachieving relative improvements of 6.7%, 6.6%, 4.8%, and 5.9% across the four datasets, respectively.\\nH Model Analysis\\nDetailed Results on the Number of Experts.We provide the full results on the number of experts\\nfor the ETTh1 and Weather dataset in Figure 7.\\n28'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 28}, page_content='Table 11: Comparison between TFPS and methods for Distribution Shift. The best results are\\nhighlighted inboldand the second best are underlined.\\nModel IMP. TFPS Koopa SOLID OneNet(Our) (2024)) (2024) (2024)\\nMetric MSEMSE MAEMSE MAEMSE MAEMSE MAE\\nETTh1\\n96 7.9%0.3980.4130.3850.4070.440 0.4390.4250.40219210.3%0.423 0.4230.4450.4340.492 0.4660.452 0.443336 4.9%0.484 0.4610.4890.4600.525 0.4810.492 0.482720 4.4%0.488 0.4760.4970.4800.517 0.4960.504 0.496\\nETTh2\\n96 10.6%0.313 0.3550.318 0.3600.3180.3590.382 0.362192 4.7%0.4050.4100.378 0.3980.414 0.4180.435 0.426336 4.8%0.392 0.4150.415 0.4300.398 0.4210.4260.419720 6.8%0.410 0.4330.445 0.4560.424 0.4410.4560.437\\nETTm1\\n96 6.8%0.327 0.3670.3290.3590.329 0.3700.374 0.392192 2.0%0.3740.3950.3800.3930.379 0.4000.385 0.435336 8.7%0.401 0.4080.4010.4110.405 0.4120.473 0.458720 2.0%0.4790.4560.475 0.4480.482 0.4640.496 0.483\\nETTm2\\n96 5.3%0.170 0.2550.179 0.2610.1750.2580.184 0.274192 3.8%0.235 0.2960.246 0.3050.2410.3020.248 0.384336 3.4%0.297 0.3350.310 0.3480.3030.3420.313 0.374720 9.0%0.401 0.3970.4050.4020.456 0.4360.425 0.438\\n1stCount 25 6 0 1\\nFigure 7: Results of expert number experiments for ETTh1 and Weather.\\nIn Figure 7, we set the learning rate to 0.0001 and conducted four sets of experiments on the ETTh1\\nand Weather datasets, Kt = 1, Kf ={1,2,4,8} , to explore the effect of the number of frequency\\nexperts on the results. For example, Kt1Kf 4 means that the TFPS contains 1 time experts and 4\\nfrequency experts. We observed that Kt1Kf 2 outperformed Kt1Kf 4 in most cases, suggesting that\\nincreasing the number of experts does not always lead to better performance.\\nIn addition, we conducted three experiments based on the optimal number of frequency experts to\\nverify the impact of varying the number of time experts on the results. As shown in Figure 7, the best\\nresults for ETTh1 were obtained with Kt4Kf 2, Kt8Kf 4, Kt4Kf 4, Kt4Kf 4, while for Weather, the\\noptimal results were achieved with Kt4Kf 8, Kt4Kf 8, Kt4Kf 4 and Kt4Kf 8. Combined with the\\naverage Wasserstein in Table 5, we attribute this to the fact that, in cases where concept drift is more\\nsevere, such as Weather, more experts are needed, whereas fewer experts are sufficient when the drift\\nis less severe.\\nComparing Inter- and Intra-Cluster Differences via Wasserstein Distance.To assess the ef-\\nfectiveness of the PI module, we replace it with a linear layer and compare the resulting inter- and\\nintra-cluster Wasserstein distance heatmaps in Figure 8. The diagonal elements represent the average\\nWasserstein distances of patches within the same clusters. If these values are small, it indicates that the\\ndifference of patches within the same cluster is relatively similar. The off-diagonal elements represent\\n29'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 29}, page_content='(a) Linear layer\\n (b) Pattern Identifier\\nFigure 8: Heatmap showing the Wasserstein distances of inter- and intra-cluster patches on ETTh1.\\n(a) Linear\\n (b) Pattern Identifier\\nFigure 9: Visualization of the embedded representations with t-SNE on ETTh1 for the time domain\\nwith H= 96 . (a) t-SNE visualization with a Linear Layer replacing the Patch Identifier for\\ncomparison. (b) t-SNE visualization of TFPS.\\n(a) ETTh1\\n (b) ETTm1\\n (c) Weather\\nFigure 10: MoE Expert allocation distributions of TFPS: the x-axis corresponds to the 4 experts, and\\nthe y-axis shows the proportion of tokens assigned to each expert.\\nthe average Wasserstein distances between patches from different clusters, where larger values mean\\nsignificant differences between the clusters. We observe that when using PI, the intra-cluster drift is\\nsmaller, while the inter-cluster shift is more pronounced compared to the linear layer. This indicates\\nthat our identifier effectively classifies and distinguishes between different patterns.\\nTFPS produces differentiated token embeddings by adapting to the characteristics of the data.\\nFigure 9 presents the t-SNE visualization of the learned embedded representation on the ETTh1 for\\nthe time domain with H= 96 . In the Figure 9 (a), where the pattern identifier is replaced with a\\nlinear layer, the representation lacks clear clustering structures, resulting in scattered and indistinct\\ngroupings. In contrast, Figure 9 (b) shows the visualization of the representation learned by the\\nproposed method, which effectively captures discriminative features and reveals significantly clearer\\nclustering patterns.\\nTFPS implements dataset-specific token embeddings assignment in a data-driven way, effec-\\ntively improving performance.Figure 10 visualizes the expert allocation distributions across\\nvarious datasets. Notably, ETTh1 and ETTm1 exhibit a high degree of consistency in their expert\\nassignments, underscoring the model’s ability to capture shared patterns. Conversely, the Weather\\ndataset shows a distinctly different allocation pattern, highlighting the method’s sensitivity to unique\\ndataset characteristics and its capability to tailor expert assignments accordingly.\\n30'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 30}, page_content='Table 12: The GPU memory (MB) and speed (inference time) of each model.\\nTFPS TSLANet FITS iTransformer TFDNet-IK PatchTST TimesNet DLinear FEDformer\\nMSE 0.4230.448 0.445 0.441 0.458 0.460 0.441 0.434 0.441GPU Memory (MB)9.643 0.481 0.019 3.304 0.246 0.205 2.345 0.142 62.191Average Inference Time (ms)6.114 0.063 1.184 2.571 98.266 4.861 12.306 0.659 136.130\\n(a)α\\n (b)β\\nFigure 11: Parameter sensitivity ofαandβof the proposed method on the ETTh1-96 dataset.\\nI Efficiency Analysis\\nTo make this clearer, we present the results of ETTh1 for a prediction length of 192 from Table 1 and\\ninclude additional results on runtime and computational complexity in Table 12. Due to the sparsity\\nof MoPE, TFPS achieves a balance between performance and efficiency:\\nPerformance Superiority. TFPS achieves an MSE of 0.423, outperforming TSLANet (0.448),\\nFITS (0.445), PatchTST (0.460), and FEDformer (0.441). This represents a 5.6% improvement over\\nTSLANet and a 8.0% improvement over PatchTST, highlighting its significant accuracy gains. While\\nDLinear achieves an MSE of 0.434, TFPS still demonstrates a 2.5% relative improvement, making it\\nthe most accurate model among all baselines.\\nEfficiency Gains. TFPS maintains competitive runtime and memory efficiency.\\n• Runtime: TFPS runs in 6.457 ms, making it 2.8× faster than PatchTST (17.851 ms) and\\n11.2× faster than TimesNet (72.196 ms).\\n• Memory Usage: TFPS uses 9.643 MB of GPU memory, significantly less than FEDformer\\n(62.191 MB) and comparable to iTransformer (3.304 MB). This makes TFPS suitable for\\nresource-constrained applications while maintaining superior performance.\\nBalancing Trade-offs. While lightweight models like DLinear (0.434 MSE, 0.789 ms runtime) are\\nslightly more efficient, TFPS delivers a performance improvement of 2.5%, providing a well-rounded\\nsolution that balances accuracy and efficiency effectively.\\nJ Hyperparameter Sensitivity\\nIn this section, we analysis the impact of the hyperparametersαandβon the performance.\\nSpecifically, we performed a grid search to optimize the hyperparametersαt ={0.0001,0.001,0.01}\\nand αf ={0.0001,0.001,0.01} , as shown in Figure 11 (a). After extensive testing, we ultimately\\nfixed atα t =α f = 10−3 in our experiments.\\nIn addition, we conducted a grid search to optimize the balance factors βt ={0.01,0.05,0.1,0.5,1}\\nand βf ={0.01,0.05,0.1,0.5,1} . The performance under different parameter values is displayed in\\nFigure 11 (b), from which we have the following observations:\\n• Firstly, the performance is affected when the value of β is too low, indicating that the\\nproposed clustering objective plays a crucial role in distinguishing patterns.\\n• Second, an excessive β also has a negative on the performance. One plausible explanation\\nis that the excessive value influences the learning of the inherent structure of original data,\\nresulting in a perturbation of the embedding space.\\n31'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 31}, page_content='Table 13: In the table, w/ Imaginary indicates that we incorporate both the real and imaginary parts\\ninto the network.\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\nTFPS 0.3980.423 0.4840.4880.3130.4050.392 0.410\\nw/ Imaginary0.3970.424 0.4870.4860.3120.4060.391 0.399\\nTable 14: Ablation study of PI components. The model variants in our ablation study include the\\nfollowing configurations across both time and frequency branches: (a) inclusion of the Time PI; (b)\\ninclusion of the Frequency PI; (c) exclusion of both. The best results are inbold.\\nTime PI Frequency PI ETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\n✓ ✓ 0.398 0.423 0.484 0.4880.313 0.405 0.392 0.410\\n✓✗ 0.4040.4540.4900.5030.3220.4130.4100.425\\n✗✓ 0.405 0.456 0.493 0.5090.324 0.415 0.412 0.430\\n✗ ✗ 0.407 0.458 0.497 0.5130.328 0.418 0.419 0.435\\n• Overall, we recommend settingβaround 0.1 for optimal performance.\\nK Full Ablation\\nK.1 Impacts of Real/Imaginary Parts\\nTo further validate the robustness of our approach, we adopted similar operations in FreTS to conduct\\nexperiments incorporating both the real and imaginary parts. The results in the Table 13 show that\\nthe performance of TFPS with the real part only is very similar to that when both parts are included,\\nwhile requiring fewer parameters. This further reinforces the conclusion that TFPS remains highly\\neffective even when focusing solely on the real part of the Fourier transform.\\nK.2 Ablation on PI\\nThe PI module plays a crucial role in identifying and characterizing distinct patterns within the time\\nseries data, while the gating network dynamically selects the most relevant experts for each segment.\\nThis collaborative mechanism allows the model to specialize in handling different patterns and adapt\\neffectively to distribution shifts, thus mitigating the overfitting risks that arise from treating all data\\nequally.\\nTo validate the importance of PI empirically, we have conducted the ablation experiments comparing\\nthe model’s performance by replacing the PI module with a linear layer in the Table 2 of main text.\\nIn addition, we supplement some ablation experiments in Table 14 to further verify the effectiveness\\nof PI.\\nK.3 Ablation onR 1 andR 2\\nWe conducted ablation experiments to further verify the important roles of R1 and R2, as shown in\\nTable 15.\\nK.4 Replace MoPE with Alternative Designs\\nHere we provide the complete results of alternative designs for TFPS.\\nAs show in Table 16, we have conducted addition experiments where we replaced the MoPE module\\nwith weighted multi-output predictor and stacked self-attention layers, keeping all other components\\n32'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 32}, page_content='Table 15: Ablation study of Loss Constraint. The model variants in our ablation study include\\nthe following configurations across both time and frequency branches: (a) inclusion of the R1; (b)\\ninclusion of theR 2; (c) exclusion of both. The best results are inbold.\\nR1 R2\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\n✓ ✓ 0.398 0.423 0.484 0.488 0.313 0.405 0.392 0.410\\n✓✗ 0.408 0.449 0.500 0.498 0.320 0.418 0.415 0.429\\n✗✓ 0.403 0.434 0.493 0.491 0.316 0.413 0.405 0.418\\n✗ ✗ 0.412 0.456 0.509 0.503 0.328 0.425 0.420 0.435\\nTable 16: Multi-output predictor and a stacked attention layer are used to replace MoPE in ETTh1\\nand ETTh2 datasets.\\nETTh1 ETTh2\\n96 192 336 720 96 192 336 720\\nTFPS 0.398 0.423 0.484 0.4880.313 0.405 0.392 0.410Multi-output Predictor0.403 0.435 0.492 0.4910.317 0.407 0.399 0.425Attention Layers0.399 0.452 0.492 0.5080.334 0.407 0.409 0.451\\nand configurations identical. The results demonstrate that our proposed method significantly out-\\nperforms them, which validates the importance of the Top-K selection and pattern-aware design in\\nenhancing the model’s representation capacity. In contrast, multi-output predictor and self-attention\\ntypically treats all data points uniformly, which may limit its ability to capture subtle distribution\\nshifts or evolving patterns across patches.\\nL Algorithm of TFPS\\nWe provide the pseudo-code of TFPS in Algorithm 1.\\nM Broader Impact\\nReal-world Applications.TFPS addresses the crucial challenge of time series forecasting, which is a\\nvaluable and urgent demand in extensive applications. Our method achieves consistent state-of-the-art\\nperformance in four real-world applications: electricity, weather, exchange rate, illness. Researchers\\nin these fields stand to benefit significantly from the enhanced forecasting capabilities of TFPS. We\\nbelieve that improved time series forecasting holds the potential to empower decision-making and\\nproactively manage risks in a wide array of societal domains.\\nAcademic Research.TFPS draws inspiration from classical time series analysis and stochastic\\nprocess theory, contributing to the field by introducing a novel framework with the assistance pattern\\nrecognition. This innovative architecture and its associated methodologies represent significant\\nadvancements in the field of time series forecasting, enhancing the model’s ability to address\\ndistribution shifts and complex patterns effectively.\\nModel Robustness.Extensive experimentation with TFPS reveals robust performance without\\nexceptional failure cases. Notably, TFPS exhibits impressive results and maintains robustness in\\ndatasets with distribution shifts. The pattern identifier structure within TFPS groups the time series\\ninto distinct patterns and adopts a mixture of pattern experts for further prediction, thereby alleviating\\nprediction difficulties. However, it is essential to note that, like any model, TFPS may face challenges\\nwhen dealing with unpredictable patterns, where predictability is inherently limited. Understanding\\nthese nuances is crucial for appropriately applying and interpreting TFPS’s outcomes.\\nOur work only focuses on the scientific problem, so there is no potential ethical risk.\\n33'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 33}, page_content='Algorithm 1Time-Frequency Pattern-Specific architecture - Overall Architecture.\\nInput: Input lookback time series X∈R L×C; input length L; predicted length H; variables number\\nC; patch length P ; feature dimension D; encoder layers number n; random Gaussian distribution-\\ninitialized subspace D= [D (1),D (2),· · ·,D (K)], each D(j) ∈R q×d, where q=C×D and\\nd=q/K. Technically, we setDas 512,nas 2.\\nOutput: The prediction result ˆY.\\n1:X=X.transpose▷ X∈R C×L\\n2:X P E =Patch(X) +Position Embedding▷ X 0\\nt ∈R C×N×D\\n3:▷Time Encoder.\\n4:X 0\\nt =X P E\\n5:forlin{1, . . . , n}:\\n6:forX l−1\\nt =LayerNorm(X l−1\\nt +Self-Attn(X l−1\\nt )).▷ X l−1\\nt ∈R C×N×D\\n7:forX l\\nt =LayerNorm(X l−1\\nt +Feed-Forward(X l−1\\nt )).▷ X l\\nt ∈R C×N×D\\n8:End for\\n9:z t =X l\\nt ▷ z l\\nt ∈R C×N×D\\n10:▷Pattern Identifier for Time Domain.\\n11:s t =Subspace affinity(z t,D)▷Eq. 6 of the papers t ∈R C×N×D\\n12:ˆst =Subspace refinement(s t)▷Eq. 7 of the paperˆs t ∈R C×N×D\\n13:▷Mixture of Temporal Pattern Experts.\\n14:G(s) =Softmax(TopK(s t))\\n15:h t =PK\\nk=1 G(s)MLPk(zt)▷Eq. 10 and Eq. 11 of the paperh t ∈R C×N×D\\n16:▷Frequency Encoder.\\n17:X 0\\nf =X P E ▷Eq. 2 of the paperX 0\\nf ∈R C×N×P\\n18:forlin{1, . . . , n}:\\n19:forX l−1\\nf =LayerNorm(X l−1\\nf +Fourier(X l−1\\nf )).▷ X l−1\\nf ∈R C×N×D\\n20:forX l\\nf =LayerNorm(X l−1\\nf +Feed-Forward(X l−1\\nf )).▷ X l\\nf ∈R C×N×D\\n21:End for\\n22:z f =X l\\nf ▷ z n\\nf ∈R C×N×D\\n23:▷Pattern Identifier for Frequency Domain.\\n24:s f =Subspace affinity(z f ,D)▷Eq. 6 of the papers f ∈R C×N×D\\n25:ˆsf =Subspace refinement(s f )▷Eq. 7 of the paperˆs f ∈R C×N×D\\n26:▷Mixture of Frequency Pattern Experts.\\n27:G(s) =Softmax(TopK(s f ))\\n28:h f =PK\\nk=1 G(s)MLPk(zf)▷Eq. 10 and Eq. 11 of the paperh f ∈R C×N×D\\n29:h=Concat(h t,h f )▷ h∈R C×N×2∗D\\n30:forcin{1, . . . , C}:\\n31:for ˆY=Linear(Flatten(h)).▷Project tokens back to predicted series ˆY∈R C×H\\n32:End for\\n33: ˆY= ˆY.transpose▷ ˆY∈R H×C\\n34:Return ˆY ▷Output the final prediction ˆY∈R H×C\\n34'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.09836v2.pdf', 'page': 34}, page_content='N Limitations\\nThough TFPS demonstrates promising performance on the benchmark dataset, there are still some\\nlimitations of this method. First, the patch length is primarily chosen heuristically, and the current\\ndesign struggles with handling indivisible lengths or multi-period characteristics in time series. While\\nthis approach works well in experiments, it lacks generalizability for real-world applications. Second,\\nthe real-world time series data undergo expansion, implying that the new patterns continually emerge\\nover time, such as an epidemic or outbreak that had not occurred before. Therefore, future work will\\nfocus on developing a more flexible and automatic patch length selection mechanism, as well as an\\nextensible solution to address these evolving distribution shifts.\\n35'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 0}, page_content='Preprint\\nXLSTM-M IXER : M ULTIVARIATE TIME SERIES\\nFORECASTING BY MIXING VIA SCALAR MEMORIES\\nMaurice Kraus∗\\nAI & ML Group, TU Darmstadt\\nFelix Divo\\nAI & ML Group, TU Darmstadt\\nDevendra Singh Dhami\\nUncertainty in AI Group, TU Eindhoven\\nHessian Center for AI (hessian.AI)\\nKristian Kersting\\nAI & ML Group, TU Darmstadt\\nCentre for Cognitive Science, TU Darmstadt\\nHessian Center for AI (hessian.AI)\\nGerman Research Center for AI (DFKI)\\nABSTRACT\\nTime series data is prevalent across numerous fields, necessitating the development\\nof robust and accurate forecasting models. Capturing patterns both within and\\nbetween temporal and multivariate components is crucial for reliable predictions.\\nWe introduce xLSTM-Mixer, a model designed to effectively integrate temporal\\nsequences, joint time-variate information, and multiple perspectives for robust\\nforecasting. Our approach begins with a linear forecast shared across variates,\\nwhich is then refined by xLSTM blocks. They serve as key elements for modeling\\nthe complex dynamics of challenging time series data. xLSTM-Mixer ultimately\\nreconciles two distinct views to produce the final forecast. Our extensive evalu-\\nations demonstrate its superior long-term forecasting performance compared to\\nrecent state-of-the-art methods. A thorough model analysis provides further in-\\nsights into its key components and confirms its robustness and effectiveness. This\\nwork contributes to the resurgence of recurrent models in time series forecasting.\\n1 I NTRODUCTION\\nTime series are an essential data modality ubiquitous in many critical fields of application, such\\nas medicine (Hosseini et al., 2021), manufacturing (Essien & Giannetti, 2020), logistics (Seyedan\\n& Mafakheri, 2020), traffic management (Lippi et al., 2013), finance (Lin et al., 2012), audio\\nprocessing (Latif et al., 2023), and weather modeling (Lam et al., 2023). While significant progress\\nin time series forecasting has been made over the decades, the field is still far from being solved.\\nThe regular appearance of yet better models and improved combinations of existing approaches\\nexemplifies this. Further increasing the forecast quality obtained from machine learning models\\npromises a manifold of improvements, such as higher efficiency in manufacturing and transportation\\nas well as more accurate medical treatments.\\nHistorically, recurrent neural networks (RNNs) and their powerful successors were natural choices\\nfor deep learning-based time series forecasting (Hochreiter & Schmidhuber, 1997; Cho et al., 2014).\\nToday, large Transformers (Vaswani et al., 2017) are applied extensively to time series tasks, including\\nforecasting. Many improvements to the vanilla architecture have since been proposed, including\\npatching (Nie et al., 2023), decompositions (Zeng et al., 2023), and tokenization inversions (Liu et al.,\\n2023). However, some of their limitations are yet to be lifted. For instance, they typically require\\nlarge datasets to train successfully, restricting their use to only a subset of conceivable applications.\\nFurthermore, they are inefficient when applied to long sequences due to the cost of the attention\\nmechanism being quadratic in the number of variates and time steps, depending on the specific choice\\nof tokenization. Therefore, recurrent and state space models (SSMs) (Patro & Agneeswaran, 2024)\\nare experiencing a resurgence of interest in overcoming such limitations. Specifically, Beck et al.\\n(2024) revisited recurrent models by borrowing insights gained from Transformers applied to many\\n∗Contact: maurice.kraus@cs.tu-darmstadt.de.\\n1\\narXiv:2410.16928v2  [cs.LG]  23 Oct 2024'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 1}, page_content='Preprint\\nNLinear\\nTime\\nInitial\\nForecast\\nReverse\\nView\\nInitial\\nEmbedding\\nVariate\\nNLinear\\nNLinear\\nNLinear\\nForecast\\nTranspose\\nShared Weights\\nTime Series\\nTime Mixing1 Joint Mixing2 View Mixing3\\nc\\nc c c c\\nc c c\\nsLSTM Stack\\nc\\nc\\nc c c\\nc c c\\nsLSTM Stack\\nFigure 1: The xLSTM-Mixer architecture consists of three stages: (1) An initial NLinear forecast\\nassuming channel independence and performing time mixing; (2) subsequent joint mixing, which\\nmixes variate and time information through crucial applications of sLSTM blocks in two views;\\nand (3) a final view mixing, where the two latent forecast views are reconciled into a coherent final\\nforecast.\\ndomains, specifically to natural language processing. They propose Extended Long Short-Term\\nMemory (xLSTM) models as a viable alternative to current sequence models.\\nWe propose xLSTM-Mixer1, a new state-of-the-art method for time series forecasting using recurrent\\ndeep learning methods. Specifically, we augment the highly expressive xLSTM architecture with\\ncarefully crafted time, variate, and multi-view mixing. These operations regularize the training and\\nlimit the model parameters by weight-sharing, effectively improving the learning of features necessary\\nfor accurate forecasting. xLSTM-Mixer initially computes a channel-independent linear forecast\\nshared over the variates. It is then up-projected to a higher hidden dimension and subsequently refined\\nby an xLSTM stack. It performs multi-view forecasting by producing a forecast from the original and\\nreversed up-projected embedding. The powerful xLSTM cells thereby jointly mix time and variate\\ninformation to capture complex patterns from the data. Both forecasts are eventually reconciled by a\\nlearned linear projection into the final prediction, again by mixing time. An overview of our method\\nis shown in Figure 1.\\nOverall, we make the following contributions:\\n(i) We investigate time and variate mixing in the context of recurrent models and propose a joint\\nmultistage approach that is highly effective for multivariate time series forecasting. We argue\\nthat marching over the variates instead of the temporal axis yields better results if suitably\\ncombined with temporal mixing.\\n(ii) We propose xLSTM-Mixer, a state-of-the-art method for time series forecasting using recurrent\\ndeep learning methods.\\n(iii) We extensively compare xLSTM-Mixer with existing methods for multivariate long-term time\\nseries forecasting and perform in-depth model analyses. The experiments demonstrate that\\nxLSTM-Mixer consistently achieves state-of-the-art performance in a wide range of bench-\\nmarks.\\nThe following work is structured as follows: In the upcoming Sec. 2, we introduce preliminaries to\\nthen motivate and explain xLSTM-Mixer in Sec. 3. We then present comprehensive experiments on\\nits effectiveness and inner workings in Sec. 4. We finally contextualize the findings within the related\\nwork in Sec. 5 and close with a conclusion and outlook in Sec. 6.\\n2 B ACKGROUND\\nAfter introducing the notation used throughout this work, we review xLSTM blocks and discuss\\nleveraging channel mixing or their independence in time series models.\\n1https://github.com/mauricekraus/xLSTM-Mixer\\n2'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 2}, page_content='Preprint\\n2.1 N OTATION\\nIn multivariate time series forecasting, the model is presented with a time seriesX = (x1, . . . ,xT ) ∈\\nRV ×T consisting of T time steps with V variates each. Given this context, the forecaster shall predict\\nthe future values Y = ( xT +1, . . . ,xT +H ) ∈ RV ×H up to a horizon H. A variate (also called a\\nchannel) can be any scalar measurement, such as the occupancy of a road or the oil temperature in\\na power plant. The measurements are assumed to be carried out jointly, such that the T + H time\\nsteps reflect a regularly sampled multivariate signal. A time series dataset consists of N such pairs\\x08\\x00\\nX (i), Y (i)\\x01\\t\\ni∈{1,...,N } divided into train, validation, and test portions.\\n2.2 E XTENDED LONG SHORT-T ERM MEMORY (XLSTM)\\nBeck et al. (2024) propose xLSTM architectures consisting of two building blocks, namely the\\nsLSTM and mLSTM modules. To harness the full expressivity of xLSTMs within each step and\\nacross the computation sequence, we employ a stack of sLSTM blocks without any mLSTM blocks.\\nThe latter are less suited for joint mixing due to their independent treatment of the sequence elements,\\nmaking it impossible to learn any relationships between them directly. We will continue by recalling\\nthe construction of sLSTM cells.\\nThe standard LSTM architecture of Hochreiter & Schmidhuber (1997) involves updating the cell state\\nct through a combination of input, forget, and output gates, which regulate the flow of information\\nacross tokens. sLSTM blocks enhance this by incorporating exponential gating and memory mix-\\ning (Greff et al., 2017) to handle complex temporal and cross-variate dependencies more effectively.\\nThe sLSTM updates the cell ct and hidden state ht using three gates as follows:\\nct = ft ⊙ ct−1 + it ⊙ zt cell state (1)\\nnt = ft · nt−1 + it normalizer state (2)\\nht = ot ⊙ ct ⊙ n−1\\nt hidden state (3)\\nzt = tanh\\n\\x00\\nWzxt + Rzht−1 + bz\\n\\x01\\ncell input (4)\\nit = exp\\n\\x00˜it − mt\\n\\x01 ˜it = Wixt + Riht−1 + bi input gate (5)\\nft = exp\\n\\x00 ˜ft + mt−1 − mt\\n\\x01 ˜ft = Wf xt + Rf ht−1 + bf forget gate (6)\\not = σ\\n\\x00\\nWoxt + Roht−1 + bo\\n\\x01\\noutput gate (7)\\nmt = max\\n\\x00 ˜ft + mt−1,˜it\\n\\x01\\nstabilizer state (8)\\nIn this setup, the matrices Wz, Wi, Wf , and Wo are input weights mapping the input token xt to\\nthe cell input zt, input gate, forget gate, and output gate, respectively. The states nt and mt serve as\\nnecessary normalization and training stabilization, respectively.\\nAs Beck et al. have shown, it is beneficial to restrict the memory mixing performed by the recurrent\\nweight matrices Rz, Ri, Rf , and Ro to individual heads, inspired by the multi-head setup of\\nTransformers (Zeng et al., 2023), yet more restricted and therefore more efficient to compute. In\\nparticular, each token gets broken up into separate pieces, where the input weightsWz,i,f,o act across\\nall of them, but the recurrence matrices Rz,i,f,o are implemented as block-diagonals and therefore\\nonly act within each piece. This permits specialization of the individual heads to patterns specific to\\nthe respective section of the tokens and empirically does not sacrifice expressivity.\\n2.3 C HANNEL INDEPENDENCE AND MIXING IN TIME SERIES MODELS\\nMultiple works have investigated whether it is beneficial to learn representations of the time and\\nvariate dimensions jointly or separately. Intuitively, because joint mixing is strictly more expressive,\\none might think it should always be preferred. It is indeed used by many methods such as Temporal\\nConvolutional Networks (TCN) (Lea et al., 2016), N-BEATS (Oreshkin et al., 2019), N-HiTS (Challu\\net al., 2023), and many Transformers (Vaswani et al., 2017), including Temporal Fusion Trans-\\nformer (TFT) (Lim et al., 2021), Autoformer (Wu et al., 2021), and FEDFormer (Zhou et al., 2022).\\nHowever, treating slices of the input data independently assumes an invariance to temporal or variate\\npositions and serves as a strong regularization against overfitting, reminiscent of kernels in CNNs.\\n3'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 3}, page_content='Preprint\\nProminent models implementing some aspects of channel independence in multivariate time series\\nforecasting are PatchTST (Nie et al., 2023) and iTransformer (Liu et al., 2023). TiDE (Das et al.,\\n2023), on the other hand, contains a time-step shared feature projection and temporal decoder but\\ntreats variates jointly. As Tolstikhin et al. (2021) have shown with MLP-Mixer, interleaving mixing\\nof all channels in each token and all tokens per channel does not empirically sacrifice any expressivity\\nand instead improves performance. This idea has since been applied to time series, too, namely in\\narchitectures such as TimeMixer (Chen et al., 2023c) and TSMixer (Chen et al., 2023c), and is one of\\nthe key components of our method xLSTM-Mixer.\\n3 XLSTM-M IXER\\nNow we have everything at hand to introduce xLSTM-Mixer as depicted in Fig. 1. It carefully\\nintegrates three key components: (1) an initial linear forecast with time mixing, (2) joint mixing using\\npowerful sLSTM modules, and (3) an eventual combination of two views by a final fully connected\\nlayer. The transposing steps between the key components enable capturing complex temporal and\\nintra-variate patterns while facilitating easy trainability and limiting parameter counts. The sLSTM\\nblock, in particular, can learn intricate non-linear relationships hidden within the data along both\\nthe time and variate dimensions. The xLSTM-Mixer architecture is furthermore equipped with\\nnormalization layers and skip connections to improve training stability and overall effectiveness.\\n3.1 K EY COMPONENT 1: N ORMALIZATION AND INITIAL LINEAR FORECAST\\nNormalization has become an essential ingredient of modern deep learning architectures (Huang\\net al., 2023). For time series in particular, reversible instance norm (RevIN) (Kim et al., 2022) is a\\ngeneral recipe for improving forecasting performance, where each time series instance is normalized\\nby its mean and variance and furthermore scaled and offset by learnable scalars γ and β:\\nxnorm\\nt = RevIN(xt) = γ\\n \\nxt − E [x]p\\nVar [x] + ϵ\\n!\\n+ β.\\nWe apply it as part of xLSTM-Mixer, and at the end of the entire pipeline, we invert the RevIN\\noperation to obtain the final prediction. In the case of xLSTM-Mixer, the typical skip connections\\nfound in mixer acrchitectures (Tolstikhin et al., 2021; Chen et al., 2023c) are taken up by RevIN, the\\nnormalization in the NLinear forecast explained shortly, and the integral skip connections within each\\nsLSTM block.\\nIt has been shown previously that simple linear models equipped with appropriate normalization\\nschemes are, already by themselves, decent long-term forecasters (Zeng et al., 2023; Li et al., 2023).\\nOur observations confirm this finding. Therefore, we first process each variate separately by an\\nNLinear model by computing:\\nxinitial = NLinear(xnorm) = FC (xnorm\\n1:T − xnorm\\nT ) + xnorm\\nT ,\\nwhere FC(·) denotes a fully-connected linear layer with bias term. Sharing this model across variates\\nlimits parameter counts, and the weight-tying serves as a useful regularization. The quality of this\\ninitial forecast will be investigated in Sec. 4.1 and 4.2.\\n3.2 K EY COMPONENT 2: SLSTM R EFINEMENT\\nWhile the NLinear forecast xinitial captures the basic patterns between the historic and future time\\nsteps, its quality alone is insufficient for today’s challenging time series datasets. We, therefore, refine\\nit using powerful sLSTM blocks. As a first step, it is crucial to increase the embedding dimension\\nof the data to provide enough latent dimensions D for the sLSTM cells: xup = FC up\\x00\\nxinitial\\x01\\n.\\nThis pre-up-projection is similar to what is commonly performed in SSMs (Beck et al., 2024). We\\nweight-share FCup across variates to perform time-mixing similar to the initial forecast. Note that\\nthis step does not maintain the temporal ordering within the embedding token dimensions, as was the\\ncase up until this step, and instead embeds it into a higher latent dimension.\\nThe stack of M sLSTM blocks S(·) transforms xup as defined in Eq. 1 to 8. The recurrent model\\nstrides over the data in variate order, i.e., where each token represents all time steps from a single\\n4'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 4}, page_content='Preprint\\nTable 1: The long-term forecasting benchmark datasets and their key properties.\\nDataset Source Domain Horizons Sampling #Variates\\nWeather Zhou et al. (2021) Weather 96–720 10 min 21\\nElectricity Zhou et al. (2021) Power Usage 96–720 1 hour 321\\nTraffic Wu et al. (2021) Traffic Load 96–720 1 hour 862\\nETT Zhou et al. (2021) Power Production 96–720 15&60 min 7\\nvariate as in the work of Liu et al. (2023). The sLSTM blocks learn intricate non-linear relationships\\nhidden within the data along both the time and variate dimensions. The mixing of the hidden\\nstate is still limited to blocks of consecutive dimensions, aiding efficient learning and inference\\nwhile allowing for effective cross-variate interaction during the recurrent processing. Striding over\\nvariates has the benefit of linear runtime scaling in the number of variates at a constant number of\\nparameters. It, however, comes at the cost of possibly fixing a suboptimal order of variates. While\\nthis is empirically not a significant limitation, we leave investigations into how to find a suitable\\nordering for future work. In addition to a large embedding dim, we observed a high number of heads\\nbeing crucial for effective forecasting.\\nThe sLSTM cells’ first hidden state ht−1 must be initialized before each sequence of tokens can\\nbe processed. Extending the initial description of these blocks, we propose learning a single initial\\nembedding token η ∈ RD that gets prepended to each encoded time series xup. These initial\\nembeddings draw from recent advances in Large Language Models, where learnable \"soft prompt\"\\ntokens are used to condition models and improve their ability to generate coherent outputs (Lester\\net al., 2021; Li & Liang, 2021; Chen et al., 2023a;b). Recent research has extended the application of\\nsoft prompts to LLM-based time series forecasting (Cao et al., 2023; Sun et al., 2024), emphasizing\\ntheir adaptability and effectiveness in improving model performance across modalities. These\\ntokens enable greater flexibility and conditioning, allowing the model to adapt its initial memory\\nrepresentation to specific dataset characteristics and to dynamically interact with the time and variate\\ndata. Soft prompts can be readily optimized through back-propagation with very little overhead.\\n3.3 K EY COMPONENT 3: M ULTI-V IEW MIXING\\nTo further regularize the training of the sLSTM as with the linear projections, we compute forecasts\\nfrom the original embedding xup as well as the reversed embeddingbxup, where the order of the latent\\ndimensions including the representation of η is inverted. Learning forecasts y′ and y′′ for both views\\nwhile sharing weights helps learn better representations. Such multi-task learning settings are known\\nto benefit training (Zhang & Yang, 2022). The final forecast is obtained by a linear projection FCview\\nof the two concatenated forecasts, again per-variate. Specifically, we compute:\\nynorm = FCview (y′, y′′) , where y′ = S(xup) and y′′ = S(bxup).\\nThe final forecast is obtained after de-normalizing the reconciled forecasts as y = RevIN −1(ynorm).\\n4 E XPERIMENTAL EVALUATION\\nOur intention here is to evaluate the forecasting capabilities of xLSTM-Mixer, aiming to provide\\ncomprehensive insights into its performance. To this end, we conducted a series of experiments with\\nthe primary focus on long-term forecasting, following the work of Das et al. (2023) and Chen et al.\\n(2023c). An evaluation of xLSTM-Mixer’s competitiveness in short-term forecasting on the PEMS\\ndataset is provided in App. A.2. Additionally, we perform an extensive model analysis consisting of\\nan ablation study to identify the contributions of individual components of xLSTM-Mixer, followed\\nby an inspection of the initial embedding tokens, a hyperparameter sensitivity analysis, and an\\ninvestigation into its robustness.\\nDatasets. We generally follow the established benchmark procedure of Wu et al. (2021) and Zhou\\net al. (2021) for best backward and future comparability. The datasets we thus used are summarized\\nin Tab. 1.\\n5'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 5}, page_content='Preprint\\nTable 2: xLSTM-Mixer is effective in long-term forecasting. The results are averaged from 4\\ndifferent prediction lengths {96, 192, 336, 720}. A lower MSE or MAE indicates a better prediction.\\nThe best result for each dataset is highlighted in bold red, while the second-best result is blue and\\nunderlined. Wins for each model out of all 28 settings are shown at the bottom.\\nModels\\nRecurrent MLP Transformer Convolutional\\nxLSTM-MixerxLSTMTimeLSTMTimeMixerTSMixerDLinearTiDE PatchTSTiTransformerFEDFormerAutoformerMICNTimesNet\\n(Ours) 2024 1997a 2024a 2023c 2023 2023 2023 2023 2022 2021 2022 2022\\nDataset MSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAE\\nWeather0.2190.2500.2220.2550.4440.4540.2220.2620.2250.2640.2460.3000.2360.2820.2410.2640.2580.2780.3090.3600.3380.3820.2420.2990.2590.287\\nElectricity0.1530.2450.1570.2500.5590.5490.1560.2460.1600.2560.1660.2640.1590.2570.1590.2530.1780.2700.2140.3210.2270.3380.1860.2950.1920.295\\nTraffic 0.3920.2530.3910.2611.0110.5410.3870.2620.4080.2840.4340.2950.3560.2610.3910.2640.4280.2820.6090.3760.6280.3790.5410.3150.6200.336\\nETTh1 0.3970.4200.4080.4281.1980.8210.4110.4230.4120.4280.4230.4370.4190.4300.4130.4340.4540.4480.4400.4600.4960.4870.5580.5350.4580.450\\nETTh2 0.3400.3820.3460.3863.0951.3520.3160.3840.3550.4010.4310.4470.3450.3940.3240.3810.3830.4070.4330.4470.4530.4620.5880.5250.4140.427\\nETTm10.3390.3660.3470.3721.1420.7820.3480.3750.3470.3750.3570.3790.3550.3780.3530.3820.4070.4100.4480.4520.5880.5170.3920.4130.4000.406\\nETTm20.2480.3070.2540.3102.3951.1770.2560.3150.2670.3220.2670.3320.2490.3120.2560.3170.2880.3320.3040.3490.3240.3680.3280.3820.2910.333\\nWins 18 23 1 2 0 0 3 5 0 0 0 0 5 1 2 1 0 0 0 0 0 0 0 0 0 0\\naTaken from Wu et al. (2022).\\n1220\\n1240Weather\\nxLSTM-Mixer xLSTM-Mixer (Init) xLSTMTime TimeMixer PatchTST\\n0 96 192\\n2\\n3\\n4\\nETTm1\\n0 96 192 0 96 192 0 96 192 0 96 192\\nLookback T arget Prediction\\nFigure 2: Example Forecasts Across Models and Datasets. This figure shows example forecasts on\\nthe Weather and ETTm1 datasets for multiple models. The blue lines represent the ground truth target,\\nand the orange lines show the predictions. The first panel illustrates the forecast from xLSTM-Mixer,\\nwhile the second panel shows the forecast extracted before the up-projection step, highlighting the\\neffectiveness of our added components. Comparisons with xLSTMTime, TimeMixer, and PatchTST\\nconfirm the favorable performance of xLSTM-Mixer relative to these baseline models.\\nTraining. We follow standard practice in the forecasting literature by evaluating long-term forecasts\\nusing the mean squared error (MSE) and the mean absolute error (MAE). Based on our experiments,\\nwe used the MAE as the training loss function since it yielded the best results. The datasets were\\nstandardized for consistency across features. Further details on hyperparameter selection, metrics,\\nand the implementation can be found in App. A.1.\\nBaseline Models. We compare xLSTM-Mixer to the recurrent models xLSTMTime (Alharthi &\\nMahmood, 2024) and LSTM (Hochreiter & Schmidhuber, 1997); multi-perceptron (MLP) based\\nmodels TimeMixer (Wang et al., 2024a), TSMixer (Chen et al., 2023c), DLinear (Zeng et al., 2023),\\nand TiDE (Das et al., 2023); the Transformers PatchTST (Nie et al., 2023), iTransformer (Liu et al.,\\n2023), FEDFormer (Zhou et al., 2022), and Autoformer (Wu et al., 2021); and the convolutional\\narchitectures MICN (Wang et al., 2022) and TimesNet (Wu et al., 2022).\\n4.1 L ONG -T ERM TIME SERIES FORECASTING\\nWe present the performance of xLSTM-Mixer compared to prior models in Tab. 2. As shown,\\nxLSTM-Mixer consistently delivers highly accurate forecasts across a wide range of datasets. It\\nachieves the best results in 18 out of 28 cases for MSE and 22 out of 28 cases for MAE, demonstrating\\nits superior performance in long-term forecasting. In particular, xLSTM-Mixer exhibits exceptional\\nforecasting accuracy, as evidenced particularly by its strong MAE performance across all datasets.\\nNotably, on Weather, xLSTM-Mixer reduces the MAE by 2% compared to xLSTMTime and 4.6%\\n6'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 6}, page_content='Preprint\\nTable 3: Ablation Study on the Weather and ETTm1 Datasets. The MSE (↓) and MAE (↓) are\\nreported for each numbered configuration across all four forecast horizons. The best results are\\nhighlighted in bold red, while the second-best results are blue and underlined.\\n#1 (full) #2 #3 #4 #5 #6 #7 #8 #9 #10\\nMix Time ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗\\nsLSTM Variates Time Variates Variates Variates None Variates Variates Variates Variates\\nInit. Token ✓ ✓ ✗ ✓ ✗ ✗ ✓ ✗ ✓ ✗\\nMix View ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗\\nHorizonMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAE\\nWeather\\n96 0.1430.1840.1480.1940.1450.1860.1440.1850.1440.1860.1730.2230.1490.1930.1510.1950.1490.1920.1520.195\\n192 0.1860.2260.1960.2390.1880.2280.1860.2260.1880.2280.2190.2570.1920.2330.1920.2340.1910.2340.1930.236\\n336 0.2370.2660.2520.2810.2390.2670.2410.2700.2420.2700.2610.2880.2400.2710.2420.2730.2420.2730.2440.274\\n720 0.3100.3240.3150.3280.3100.3240.3090.3230.3090.3230.3200.3340.3200.3290.3190.3290.3220.3300.3190.328\\nETTm1\\n96 0.2750.3280.2980.3480.2770.3290.2780.3310.2790.3330.2950.3380.2820.3390.2850.3410.2810.3370.2840.339\\n192 0.3190.3540.3370.3690.3210.3540.3210.3560.3220.3580.3290.3570.3290.3640.3300.3650.3370.3670.3350.366\\n336 0.3530.3740.3680.3880.3540.3750.3550.3770.3570.3790.3590.3760.3670.3850.3670.3850.3660.3840.3660.385\\n720 0.4090.4070.4200.4160.4110.4080.4130.4110.4140.4110.4120.4070.4220.4120.4220.4130.4170.4100.4180.411\\ncompared to TimeMixer. Similarly, for ETTm1, xLSTM-Mixer outperforms TimeMixer by 2.4% in\\nMAE and shows a strong competitive edge over xLSTMTime. Although xLSTM-Mixer performs\\nslightly less well on the Traffic and ETTh2 datasets, where it encounters challenges with handling\\noutliers, it remains highly competitive and outperforms the majority of baseline models. This suggests\\nthat despite these few cases, xLSTM-Mixer can consistently deliver state-of-the-art performance\\nin long-term forecasting. A qualitative inspection of several baseline models, including the initial\\nforecast extracted before the sLSTM refinement, is shown in Fig. 2. In this comparison, the lookback\\nwindow and forecasting horizon are both fixed at 96.\\n4.2 M ODEL ANALYSIS\\nAblation Study. To assess the contributions of each component in xLSTM-Mixer to its strong\\noverall forecast performance, we conducted an extensive ablation study with the results listed in Tab. 3.\\nEach configuration represents a different combination of the four key components: mixing time with\\nNLinear, using sLSTM blocks, learning an initial embedding token, and multi-view mixing. We\\nevaluated the performance using the MSE and MAE across the prediction lengths{96, 192, 336, 720}.\\nThe full version of xLSTM-Mixer (#1), which integrates all components, achieves the best perfor-\\nmance overall. However, we also observe that some configurations of xLSTM-Mixer, which exclude\\nspecific components, remain competitive. For instance, #3, which excludes the initial embedding\\ntoken, still performs reasonably well. This suggests that while it contributes positively to the over-\\nall performance, the model can sometimes still achieve competitive results without it. In general,\\nremoving any specific component leads to a performance drop. For example, removing the time\\nmixing (#7) increases the MAE by 3.4% on ETTm1 at length 96 or 2.8% at length 192, highlighting\\nits critical role in capturing intratemporal dependencies. When we now omit everything except for\\ntime mixing on Weather at 192, we suffer a 13.7% performance decrease. In summary, the ablation\\nstudy confirms that all components of xLSTM-Mixer contribute to its effectiveness, with the full\\nconfiguration yielding the best results. Furthermore, we identified the sLSTM blocks and time-mixing\\nas critical components for ensuring high accuracy across datasets and prediction lengths.\\nInitial Token Embedding. We qualitatively inspect decodings of the initial embedding tokensη on\\nmultiple datasets to further understand and interpret the initializations learned by xLSTM-Mixer. η\\nare decoded to a forecasty by transforming them through the sLSTM stackS and applying multi-view\\nmixing. The resulting output of FCview can then be interpreted as the conditioning forecast used to\\ninitialize the sLSTM blocks. Fig. 3 shows the dataset-specific patterns the initial embedding tokens\\nhave learned on Weather, ETTm1, and ETTh2 for various prediction horizons. With increasing\\nprediction horizons, we observe longer spans of time, eventually revealing underlying seasonal\\npatterns and respective dataset dynamics.\\n7'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 7}, page_content='Preprint\\n2\\n1\\n0\\n1\\nWeather\\nPrediction Length 96 Prediction Length 192 Prediction Length 338 Prediction Length 720\\n1\\n0\\n1\\nETTm1\\n0 24 48 72 96\\nTime\\n2\\n1\\n0\\n1\\nETTh2\\n0 48 96 144 192\\nTime\\n0 85 170 255 338\\nTime\\n0 180 360 540 720\\nTime\\nFigure 3: Initial tokens capture dataset characteristics. The plot illustrates the learned tokens\\nacross multiple datasets and prediction lengths. The lookback length is set to 96 for all evaluations.\\nFor clarity and the high noise levels in ETTm1 and ETTh2, we only show three seeds for Weather.\\n0.12\\n0.14\\n0.16MSE\\nPrediction Length: 96\\n0.14\\n0.16\\n0.18\\nPrediction Length: 192\\n25 27 29 211\\nD\\n0.16\\n0.18MSE\\nPrediction Length: 336\\n25 27 29 211\\nD\\n0.18\\n0.20\\n0.22 Prediction Length: 720\\nFigure 4: Sensitivity analysis of the xLSTM hidden dimen-\\nsion D. The shaded area indicates one standard deviation\\nfrom the mean MSE (↓). Increasing the up-projection dimen-\\nsion becomes beneficial with increasing prediction length.\\nSensitivity to xLSTM Hidden\\nDimension. In Fig. 4, we visualize\\nthe performance of xLSTM-Mixer on\\nthe Electricity dataset with increasing\\nsLSTM embedding (hidden) dimen-\\nsion realized by FCup. The results\\nindicate that larger hidden dimensions\\nconsistently enhance the model’s per-\\nformance, particularly for longer fore-\\ncast horizons. This suggests that a\\nlarger embedding dimension enables\\nxLSTM-Mixer to capture better the\\nhigher complexity of the time series\\ndata over extended horizons, leading\\nto improved forecasting accuracy.\\nRobustness to Lookback Length.\\nFig. 5 illustrates the performance of xLSTM-Mixer across varying lookback lengths and prediction\\nhorizons. We observe that xLSTM-Mixer can effectively utilize longer lookback windows than\\nthe baselines, especially when compared to transformer-based models. This advantage stems from\\nxLSTM-Mixer’s avoidance of self-attention, allowing it to handle extended lookback lengths ef-\\nficiently. Additionally, xLSTM-Mixer demonstrates stable and consistent performance with low\\nvariance. These results confirm that increasing the lookback length improves forecasting accuracy\\nand enhances robustness, particularly for longer prediction horizons.\\n8'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 8}, page_content='Preprint\\n0.28\\n0.30\\n0.32\\n0.34\\n0.36MSE\\nPrediction Length: 96\\n0.325\\n0.350\\n0.375\\n0.400\\n0.425\\nPrediction Length: 192\\n96128 256 512 768 1024\\nLookback Length\\n0.350\\n0.375\\n0.400\\n0.425\\n0.450\\n0.475MSE\\nPrediction Length: 336\\n96128 256 512 768 1024\\nLookback Length\\n0.400\\n0.425\\n0.450\\n0.475\\n0.500\\n0.525\\nPrediction Length: 720\\nModel\\nPatchTST\\nTimeMixer\\nTimesNet\\niTransformer\\nxLSTM-Mixer\\nFigure 5: Increasing the lookback window increases forecasting performance, with xLSTM-\\nMixer virtually always providing the best results. Shows the mean MSE (↓) and std. dev. on ETTm1.\\n5 R ELATED WORK\\nTime Series Forecasting. A long line of machine learning research led from early statistical\\nmethods like ARIMA (Box & Jenkins, 1976) to contemporary models based on deep learning,\\nwhere four architectural families take center stage: The ones based on recurrence, convolutions,\\nMultilayer Perceptrons (MLPs), and Transformers. While all of them are used by practitioners\\ntoday, the research focus is gradually shifting over time. Initially, the naturally sequential recurrent\\nmodels such as Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) and Gated\\nRecurrent Units (GRUs) (Cho et al., 2014) were used for time series analysis. Their main benefits\\nare the high inference efficiency and arbitrary input and output lengths due to their autoregressive\\nnature. While their effectiveness has historically been constrained by a limited ability to capture\\nlong-range dependencies, active research continues to alleviate these limitations (Salinas et al., 2020),\\nincluding the xLSTM architecture presented in Sec. 2 (Beck et al., 2024; Alharthi & Mahmood, 2024).\\nSimilarly efficient as RNNs, yet more restricted in their output length, are the location-invariant\\nCNNs (Li et al., 2022; Lara-Benítez et al., 2021), such as TCN (Lea et al., 2016), TimesNet (Wu\\net al., 2022), and MICN (Wang et al., 2022). Recently, some MLP-based architectures have also\\nshown good success, including the simplistic DLinear and NLinear models (Zeng et al., 2023), the\\nencoder-decoder architecture of TiDE (Das et al., 2023), the mixing architectures TimeMixer (Wang\\net al., 2024a) and TSMixer (Chen et al., 2023c), as well as the hierarchical N-BEATS Oreshkin et al.\\n(2019) and N-HiTS (Challu et al., 2023) models. Finally, a lot of models have been proposed based\\non Transformers (Vaswani et al., 2017), such as Autoformer (Wu et al., 2021), TFT (Lim et al., 2021),\\nFEDFormer (Zhou et al., 2022), PatchTST (Nie et al., 2023), and iTransformer (Liu et al., 2023).\\nxLSTM Models for Time Series. Some initial experiments of applying xLSTMs (Beck et al.,\\n2024) to time series were already performed by Alharthi & Mahmood (2024) with their proposed xL-\\nSTMTime model. While it showed promising forecasting performance, these initial soundings did not\\nsurpass stronger recent models such as TimeMixer (Wang et al., 2024a) on multivariate benchmarks,\\nand the reported performance is challenging to reproduce. We ensure that our method xLSTM-Mixer\\nis well suited as a foundation for further research by providing extensive model analysis, including an\\nablation study with ten variants, and ensuring that results are readily reproducible. Our methodology\\ndraws from xLSTMTime yet improves on it by several key components. Most importantly, our\\nnovel multi-view mixing consistently enhances forecasting performance. Furthermore, we find the\\ntrend-seasonality decomposition to be redundant and a simple NLinear normalization scheme (Zeng\\net al., 2023) to suffice.\\n9'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 9}, page_content='Preprint\\n6 C ONCLUSION\\nIn this work, we introduced xLSTM-Mixer, a method that combines a linear forecast with further\\nrefinement using xLSTM blocks. Our architecture effectively integrates time, joint, and view mixing\\nto capture complex dependencies. In long-term forecasting, xLSTM-Mixer consistently achieved\\nstate-of-the-art performance, outperforming previous methods in 41 out of 56 cases. Furthermore,\\nour detailed model analysis provided valuable insights into the contribution of each component and\\ndemonstrated its robustness to varying hyperparameter settings.\\nWhile xLSTM-Mixer has shown extraordinary performance in long-term forecasting, it should be\\nnoted that due to the transpose of the input, i.e., processing the variates as sequence elements, the\\nnumber of variates may limit the overall performance. To overcome this, we plan to explore how\\ndifferent variate orderings influence performance and whether incorporating more than two views\\ncould lead to further improvements. This study focused on long-term forecasting, yet extending\\nxLSTM-Mixer to tasks such as short-term forecasting, time series classification, or imputation offers\\npromising directions for future research.\\nETHICS STATEMENT\\nOur research advances machine learning by enhancing the capabilities of long-term forecasting in time\\nseries models, significantly improving both accuracy and efficiency. By developing xLSTM-Mixer,\\nwe introduce a robust framework that can be applied across various industries, including finance,\\nhealthcare, energy, and logistics. The improved forecasting accuracy enables better decision-making\\nin critical areas, such as optimizing resource allocation, predicting market trends, and managing risk.\\nHowever, we also recognize the potential risks associated with the misuse of these advanced models.\\nTime series forecasting models could be leveraged for malicious purposes, especially when applied at\\nscale. For example, in the financial sector, adversarial agents might manipulate forecasts to create\\nmarket instability. In political or social contexts, these models could be exploited to predict and\\ninfluence public opinion or destabilize economies. Additionally, the application of these models in\\nsensitive domains like healthcare and security may lead to unintended consequences if not carefully\\nregulated and ethically deployed.\\nTherefore, it is essential that the use of xLSTM-Mixer, like all machine learning technologies, is\\nguided by responsible practices and ethical considerations. We encourage stakeholders to adopt\\nrigorous evaluation processes to ensure fairness, transparency, and accountability in its deployment,\\nand to remain vigilant to the broader societal implications of time series forecasting technologies.\\nREPRODUCIBILITY STATEMENT\\nAll implementation details, including dataset descriptions, metric calculations, and experiment\\nconfigurations, are provided in Sec. 4 and App. A.1. We make sure to exclusively use openly available\\nsoftware and datasets and provide the source code for full reproducibility at\\nhttps://github.com/mauricekraus/xLSTM-Mixer.\\nACKNOWLEDGMENTS\\nThis work received funding from the EU project EXPLAIN, under the Federal Ministry of Education\\nand Research (BMBF) (grant 01—S22030D). Furthermore, it was funded by the ACATIS Investment\\nKVG mbH project “Temporal Machine Learning for Long-Term Value Investing” and the BMBF\\nKompAKI project within the “The Future of Value Creation – Research on Production, Services\\nand Work” program (funding number 02L19C150), managed by the Project Management Agency\\nKarlsruhe (PTKA). The author of Eindhoven University of Technology received support from their\\nDepartment of Mathematics and Computer Science and the Eindhoven Artificial Intelligence Systems\\nInstitute. Furthermore, this work benefited from the HMWK project “The Third Wave of Artificial\\nIntelligence - 3AI”.\\n10'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 10}, page_content='Preprint\\nREFERENCES\\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A\\nNext-generation Hyperparameter Optimization Framework. In Proceedings of the ACM SIGKDD\\nConference on Knowledge Discovery and Data Mining (KDD), 2019.\\nMusleh Alharthi and Ausif Mahmood. xLSTMTime: Long-Term Time Series Forecasting with\\nxLSTM. MDPI AI, 5(3):1482–1495, 2024.\\nMaximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova,\\nMichael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM: Extended\\nLong Short-Term Memory. ArXiv:2405.04517, 2024.\\nGeorge E. P. Box and Gwilym M. Jenkins.Time series analysis: forecasting and control. Holden-Day\\nseries in time series analysis and digital processing. Holden-Day, San Francisco, rev. ed. edition,\\n1976. ISBN 0-8162-1104-3.\\nDefu Cao, Furong Jia, Sercan O. Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO:\\nPrompt-based Generative Pre-trained Transformer for Time Series Forecasting. In The Twelfth\\nInternational Conference on Learning Representations, 2023.\\nCristian Challu, Kin G. Olivares, Boris Oreshkin, Federico Ramirez, Max Canseco, and Artur\\nDubrawski. NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Proceedings\\nof the AAAI Conference on Artificial Intelligence, 37:6989–6997, 2023.\\nJiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you need for\\nin-context learning? Findings of the Association for Computational Linguistics, EMNLP 2023:\\n11149–11159, 2023a.\\nLichang Chen, Heng Huang, and Minhao Cheng. Ptp: Boosting stability and performance of prompt\\ntuning with perturbation-based regularizer. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing , pp. 13512–13525. Association for Computational\\nLinguistics, 2023b.\\nSi-An Chen, Chun-Liang Li, Nathanael C Yoder, Sercan Ö Arık, and Tomas Pfister. TSMixer: An\\nAll-MLP Architecture for Time Series Forecasting. Transactions on Machine Learning Research,\\n2023c.\\nKyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder for\\nStatistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), pp. 1724–1734, Doha, Qatar, 2014. Association for\\nComputational Linguistics.\\nAbhimanyu Das, Weihao Kong, Andrew Leach, Shaan K. Mathur, Rajat Sen, and Rose Yu. Long-term\\nForecasting with TiDE: Time-series Dense Encoder. Transactions on Machine Learning Research,\\n2023. ISSN 2835-8856.\\nAniekan Essien and Cinzia Giannetti. A Deep Learning Model for Smart Manufacturing Using\\nConvolutional LSTM Neural Network Autoencoders. IEEE Transactions on Industrial Informatics,\\n16(9):6069–6078, 2020.\\nKlaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmidhuber.\\nLSTM: A Search Space Odyssey. IEEE Transactions on Neural Networks and Learning Systems,\\n28(10):2222–2232, 2017.\\nSepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):\\n1735–1780, 1997.\\nMohammad-Parsa Hosseini, Amin Hosseini, and Kiarash Ahi. A Review on Machine Learning for\\nEEG Signal Processing in Bioengineering. IEEE Reviews in Biomedical Engineering, 14:204–218,\\n2021.\\n11'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 11}, page_content='Preprint\\nLei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization Techniques in Training\\nDNNs: Methodology, Analysis and Application. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 45(8):10173 – 10196, 2023.\\nTaesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible\\nInstance Normalization for Accurate Time-Series Forecasting against Distribution Shift. In\\nInternational Conference on Learning Representations, 2022.\\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. ArXiv:1412.6980,\\n2017.\\nRemi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran\\nAlet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan\\nHoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and\\nPeter Battaglia. Learning skillful medium-range global weather forecasting. Science, 382(6677):\\n1416–1421, 2023.\\nPedro Lara-Benítez, Manuel Carranza-García, and José C. Riquelme. An Experimental Review on\\nDeep Learning Architectures for Time Series Forecasting. International Journal of Neural Systems,\\n31(03):2130001, 2021.\\nSiddique Latif, Heriberto Cuayáhuitl, Farrukh Pervez, Fahad Shamshad, Hafiz Shehbaz Ali, and\\nErik Cambria. A survey on deep reinforcement learning for audio-based applications. Artificial\\nIntelligence Review, 56(3):2193–2240, 2023.\\nColin Lea, René Vidal, Austin Reiter, and Gregory D. Hager. Temporal Convolutional Networks: A\\nUnified Approach to Action Segmentation. In Gang Hua and Hervé Jégou (eds.), Computer Vision\\n– ECCV 2016 Workshops, volume 9915, pp. 47–54, Cham, 2016.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt\\nTuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.\\n3045–3059. Association for Computational Linguistics, 2021.\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, volume\\nV olume 1: Long Papers, pp. 4582–4597. Association for Computational Linguistics, 2021.\\nZewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A Survey of Convolutional Neural\\nNetworks: Analysis, Applications, and Prospects. IEEE Transactions on Neural Networks and\\nLearning Systems, 33(12):6999–7019, 2022.\\nZhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting Long-term Time Series Forecasting: An\\nInvestigation on Linear Mapping. ArXiv:2305.10721, 2023.\\nBryan Lim, Sercan Ö. Arık, Nicolas Loeff, and Tomas Pfister. Temporal Fusion Transformers for\\ninterpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4):\\n1748–1764, 2021.\\nWei-Yang Lin, Ya-Han Hu, and Chih-Fong Tsai. Machine Learning in Financial Crisis Prediction: A\\nSurvey. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),\\n42(4):421–436, 2012.\\nMarco Lippi, Matteo Bertini, and Paolo Frasconi. Short-Term Traffic Flow Forecasting: An Exper-\\nimental Comparison of Time-Series Analysis and Supervised Learning. IEEE Transactions on\\nIntelligent Transportation Systems, 14(2):871–882, 2013.\\nYong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.\\niTransformer: Inverted Transformers Are Effective for Time Series Forecasting. In The Twelfth\\nInternational Conference on Learning Representations, 2023.\\nYuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A Time Series is Worth 64\\nWords: Long-term Forecasting with Transformers. In Proceedings of the International Conference\\non Learning Representations (ICLR), 2023.\\n12'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 12}, page_content='Preprint\\nBoris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural\\nbasis expansion analysis for interpretable time series forecasting. In International Conference on\\nLearning Representations, 2019.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance\\nDeep Learning Library. In Proceedings of the Conference on Neural Information Processing\\nSystems (NeurIPS), 2019.\\nBadri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of State Space Models\\nas Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges.\\nArXiv:2404.16112, 2024.\\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic\\nforecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):\\n1181–1191, 2020.\\nMahya Seyedan and Fereshteh Mafakheri. Predictive big data analytics for supply chain demand\\nforecasting: methods, applications, and research opportunities. Journal of Big Data, 7(1):53, 2020.\\nChenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. TEST: Text prototype aligned embedding\\nto activate LLM’s ability for time series. In The twelfth international conference on learning\\nrepresentations, 2024.\\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\\nterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey\\nDosovitskiy. MLP-Mixer: An all-MLP Architecture for Vision. In Advances in Neural Information\\nProcessing Systems, volume 34, pp. 24261–24272. Curran Associates, Inc., 2021.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is All you Need. In Proceedings of the Conference on\\nNeural Information Processing Systems (NeurIPS), 2017.\\nHuiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. MICN: Multi-\\nscale Local and Global Context Modeling for Long-term Series Forecasting. In The Eleventh\\nInternational Conference on Learning Representations, 2022.\\nShiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y . Zhang, and\\nJun Zhou. TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting. In The\\nTwelfth International Conference on Learning Representations, 2024a.\\nYuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang. Deep\\nTime Series Models: A Comprehensive Survey and Benchmark, July 2024b.\\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition Transform-\\ners with Auto-Correlation for Long-Term Series Forecasting. In Advances in Neural Information\\nProcessing Systems, 2021.\\nHaixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. TimesNet:\\nTemporal 2D-Variation Modeling for General Time Series Analysis. In The Eleventh International\\nConference on Learning Representations, 2022.\\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are Transformers Effective for Time Series\\nForecasting? In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2023.\\nYu Zhang and Qiang Yang. A Survey on Multi-Task Learning. IEEE Transactions on Knowledge\\nand Data Engineering, 34(12):5586–5609, 2022.\\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\nInformer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. Proceedings\\nof the AAAI Conference on Artificial Intelligence, 35(12):11106–11115, 2021.\\n13'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 13}, page_content='Preprint\\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency\\nEnhanced Decomposed Transformer for Long-term Series Forecasting. In Proceedings of the 39th\\nInternational Conference on Machine Learning, volume 162, 2022.\\nA A PPENDIX\\nA.1 I MPLEMENTATION DETAILS\\nExperimental Details Our codebase is implemented in Python 3.11, leveraging PyTorch 2.4\\n(Paszke et al., 2019) in combination with Lightning 2.42 for model training and optimization. We used\\nthe custom CUDA implementation of Beck et al. (2024) for sLSTM3 which relies on the NVIDIA\\nCompute Capability 8.0 or higher. Thus, our experiments were conducted on a single NVIDIA\\nA100 80GB GPU. The majority of our baseline implementations, along with data loading and\\npreprocessing steps, are adapted from the Time-Series-Library4 of Wang et al. (2024b). Additionally,\\nfor xLSTMTime we used code based on the official repository5 of Alharthi & Mahmood (2024).\\nTraining and Hyperparameters We optimized xLSTM-Mixer for up to 60 epochs with a cosine-\\nannealing scheduler with the Adam optimizer (Kingma & Ba, 2017), using β1 = 0.9 and β2 = 0.999\\nand no weight decay. Hyperparameter tuning was conducted using Optuna (Akiba et al., 2019) with\\nthe choices provided in Tab. 4. We optimized for the L1 forecast error, also known as the Mean\\nAbsolute Error (MAE). To further stabilize the training process, gradient clipping with a maximum\\nnorm of 1.0 was applied. All experiments were run with three different random seeds {2021, 2022,\\n2023}.\\nTable 4: Hyperparameters and their choices.\\nHyperparameter Choices\\nBatch size {16, 32, 64, 128, 256, 512}\\nInitial learning rate { 1 · 10−2, 3 · 10−3, 1 · 10−3, 5 · 10−4, 2 · 10−4, 1 · 10−4}\\nScheduler warmup steps {5, 10, 15}\\nLookback length T {96, 256, 512, 768, 1024, 2048}\\nEmbedding dimension D {32, 64, 128, 256, 512, 768, 1024}\\nsLSTM conv. kernel width {disabled, 2, 4}\\nsLSTM dropout rate {0.1, 0.25}\\n# sLSTM blocks in S {1, 2, 3, 4}\\n# sLSTM heads {4, 8, 16, 32}\\nMetrics We follow common practice in the literature (Wu et al., 2021; Wang et al., 2024a) for\\nmaximum comparability and, therefore, evaluate long-term forecasting of all models on the mean\\nabsolute error (MAE), mean squared error (MSE), and for short-term forecasting, using the MAE,\\nroot mean squared error (RMSE), and mean absolute percentage error (MAPE). The metrics are\\naveraged over all variates and computed as:\\nMAE(y, ˆy) =\\nHX\\ni=1\\n|yi − ˆyi| MSE(y, ˆy) =\\nHX\\ni=1\\n(yi − ˆyi)2\\nRMSE(y, ˆy) =\\np\\nMSE(y, ˆy) MAPE( y, ˆy) = 100\\nH\\nHX\\ni=1\\n|yi − ˆyi|\\n|yi| + ϵ ,\\nwhere y are the targets, ˆy the predictions, and ϵ a small constant added for numerical stability.\\n2https://lightning.ai/pytorch-lightning\\n3https://github.com/NX-AI/xlstm\\n4https://github.com/thuml/Time-Series-Library\\n5https://github.com/muslehal/xLSTMTime\\n14'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 14}, page_content='Preprint\\nA.2 O UTLOOK : S HORT-T ERM TIME SERIES FORECASTING\\nHaving shown superior long-term forecasting accuracies in Sec. 4.1, we also provide an initial\\nexploration of the effectiveness of xLSTM-Mixer to short-term forecasts. To this end, we compare\\nit to applicable baselines on PEMS datasets with input lengths uniformly set to 96 and prediction\\nlengths to 12. The results in Tab. 5 show that the performance of xLSTM-Mixer is competitive with\\nexisting methods. We provide the MAE, MAPE, and RMSE as is common practice.\\nTable 5: Short-term forecasting evaluation of xLSTM-Mixer and baselines on the multivariate\\nPEMS datasets. A lower MAE, MAPE, or RMSE indicates a better prediction. The best result for\\neach dataset is highlighted in bold red, while the second-best result is blue and underlined.\\nModels\\nRecurrent MLP Transformer Convolutional\\nxLSTM-MixerxLSTMTimeLSTMTimeMixerDLinearPatchTSTFEDFormerAutoformerMICNTimesNet\\n(Ours) 2024 1997a 2024a 2023 2023 2022 2021 2022 2022\\nPEMS03\\nMAE 15.71 16.59 18.65 14.63 19.70 18.95 19.00 18.08 15.71 16.41\\nMAPE 14.92 15.31 17.39 14.54 18.35 17.29 18.57 18.75 15.67 15.17\\nRMSE 24.82 26.47 31.73 23.28 32.35 30.15 30.05 27.82 24.55 26.72\\nPEMS08\\nMAE 16.56 17.44 20.34 15.22 20.26 20.35 20.56 20.47 17.76 19.01\\nMAPE 10.24 10.58 13.05 9.67 12.09 13.15 12.41 12.27 10.76 11.83\\nRMSE 26.65 28.13 31.90 24.26 32.38 31.04 32.97 31.52 27.26 30.65\\na Configuration following Wu et al. (2021).\\n15'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.16928v2.pdf', 'page': 15}, page_content='Preprint\\nA.3 F ULL RESULTS FOR LONG -T ERM FORECASTING\\nTab. 6 shows the full results for long-term forecasting on all four separate forecast horizons.\\nTable 6: Full long-term forecasting results for Tab. 2. Avg is averaged from all four prediction\\nlengths {96, 192, 336, 720}. A lower MSE or MAE indicates a better prediction. The best result for\\neach dataset is highlighted in bold red, while the second-best result is blue and underlined. Wins for\\neach model are shown at the bottom.\\nModels\\nRecurrent MLP Transformer Convolutional\\nxLSTM-MixerxLSTMTimeLSTMTimeMixerTSMixerDLinearTiDE PatchTSTiTransformerFEDFormerAutoformerMICNTimesNet\\n(Ours) 2024 1997a 2024a 2023c 2023 2023 2023 2023 2022 2021 2022 2022\\nMetricMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAE\\nWeather\\n96 0.1430.1840.1440.1870.3690.4060.1470.1970.1450.1980.1760.2370.1660.2220.1490.1980.1740.2140.2170.2960.2660.3360.1610.2290.1720.220\\n1920.1860.2260.1920.2360.4160.4350.1890.2390.1910.2420.2200.2820.2090.2630.1940.2410.2210.2540.2760.3360.3070.3670.2200.2810.2190.261\\n3360.2360.2660.2370.2720.4550.4540.2410.2800.2420.2800.2650.3190.2540.3010.3060.2820.2780.2960.3390.3800.3590.3950.2780.3310.2800.306\\n7200.3100.3230.3130.3260.5350.5200.3100.3300.3200.3360.3230.3620.3130.3400.3140.3340.3580.3470.4030.4280.4190.4280.3110.3560.3650.359\\nAvg0.2190.2500.2220.2550.4440.4540.2220.2620.2250.2640.2460.3000.2360.2820.2410.2640.2580.2780.3090.3600.3380.3820.2420.2990.2590.287\\nElectricity\\n96 0.1260.2180.1280.2210.3750.4370.1290.2240.1310.2290.1400.2370.1320.2290.1290.2220.1480.2400.1930.3080.2010.3170.1640.2690.1680.272\\n1920.1440.2350.1500.2430.4420.4730.1400.2200.1510.2460.1530.2490.1470.2430.1470.2400.1620.2530.2010.3150.2220.3340.1770.2850.1840.289\\n3360.1570.2500.1660.2590.4390.4730.1610.2550.1610.2610.1690.2670.1610.2610.1630.2590.1780.2690.2140.3290.2310.3380.1930.3040.1980.300\\n7200.1830.2760.1850.2760.9800.8140.1940.2870.1970.2930.2030.3010.1960.2940.1970.2900.2250.3170.2460.3550.2540.3610.2120.3210.2200.320\\nAvg0.1530.2450.1570.2500.5590.5490.1560.2460.1600.2560.1660.2640.1590.2570.1590.2530.1780.2700.2140.3210.2270.3380.1860.2950.1920.295\\nTraffic\\n96 0.3570.2360.3580.2420.8430.4530.3600.2490.3760.2640.4100.2820.3360.2530.3600.2490.3950.2680.5870.3660.6130.3880.5190.3090.5930.321\\n1920.3770.2410.3780.2530.8470.4530.3750.2500.3970.2770.4230.2870.3460.2570.3790.2560.4170.2760.6040.3730.6160.3820.5370.3150.6170.336\\n3360.3940.2500.3920.2610.8530.4550.3850.2700.4130.2900.4360.2960.3550.2600.3920.2640.4330.2830.6210.3830.6220.3370.5340.3130.6290.336\\n7200.4390.2830.4340.2871.5000.8050.4300.2810.4440.3060.4660.3150.3860.2730.4320.2860.4670.3020.6260.3820.6600.4080.5770.3250.6400.350\\nAvg0.3920.2530.3910.2611.0110.5410.3870.2620.4080.2840.4340.2950.3560.2610.3910.2640.4280.2820.6090.3760.6280.3790.5410.3150.6200.336\\nETTh1\\n96 0.3590.3860.3680.3951.0440.7730.3610.3900.3610.3920.3750.3990.3750.3980.3700.4000.3860.4050.3760.4190.4490.4590.4210.4310.3840.402\\n1920.4020.4170.4010.4161.2170.8320.4090.4140.4040.4180.4050.4160.4120.4220.4130.4290.4410.4360.4200.4480.5000.4820.4740.4870.4360.429\\n3360.4080.4290.4220.4371.2590.8410.4300.4290.4200.4310.4390.4430.4350.4330.4220.4400.4870.4580.4590.4650.5210.4960.5690.5510.4910.469\\n7200.4190.4480.4410.4651.2710.8380.4450.4600.4630.4720.4720.4900.4540.4650.4470.4680.5030.4910.5060.5070.5140.5120.7700.6720.5210.500\\nAvg0.3970.4200.4080.4281.1980.8210.4110.4230.4120.4280.4230.4370.4190.4300.4130.4340.4540.4480.4400.4600.4960.4870.5580.5350.4580.450\\nETTh2\\n96 0.2670.3290.2730.3332.5221.2780.2710.3300.2740.3410.2890.3530.2700.3360.2740.3370.2970.3490.3460.3880.3580.3970.2990.3640.3400.374\\n1920.3380.3750.3400.3783.3121.3840.3170.4020.3390.3850.3830.4180.3320.3800.3140.3820.3800.4000.4290.4390.4560.4520.4410.4540.4020.414\\n3360.3670.4010.3730.4033.2911.3880.3320.3960.3610.4060.4480.4650.3600.4070.3290.3840.4280.4320.4960.4870.4820.4860.6540.5670.4520.452\\n7200.3880.4240.3980.4303.2571.3570.3420.4080.4450.4700.6050.5510.4190.4510.3790.4220.4270.4450.4630.4740.5150.5110.9560.7160.4620.468\\nAvg0.3400.3820.3460.3863.0951.3520.3160.3840.3550.4010.4310.4470.3450.3940.3240.3810.3830.4070.4330.4470.4530.4620.5880.5250.4140.427\\nETTm1\\n96 0.2750.3280.2860.3350.8630.6640.2910.3400.2850.3390.2990.3430.3060.3490.2930.3460.3340.3680.3790.4190.5050.4750.3160.3620.3380.375\\n1920.3190.3540.3290.3611.1130.7760.3270.3650.3270.3650.3350.3650.3350.3660.3330.3700.3770.3910.4260.4410.5530.4960.3630.3900.3740.387\\n3360.3530.3740.3580.3791.2670.8320.3600.3810.3560.3820.3690.3860.3640.3840.3690.3920.4260.4200.4450.4590.6210.5370.4080.4260.4100.411\\n7200.4090.4070.4160.4111.3240.8580.4150.4170.4190.4140.4250.4210.4130.4130.4160.4200.4910.4590.5430.4900.6710.5610.4810.4760.4780.450\\nAvg0.3390.3660.3470.3721.1420.7820.3480.3750.3470.3750.3570.3790.3550.3780.3530.3820.4070.4100.4480.4520.5880.5170.3920.4130.4000.406\\nETTm2\\n96 0.1570.2440.1640.2502.0411.0730.1640.2540.1630.2520.1670.2600.1610.2510.1660.2560.1800.2640.2030.2870.2550.3390.1790.2750.1870.267\\n1920.2130.2850.2180.2882.2491.1120.2230.2950.2160.2900.2240.3030.2150.2890.2230.2960.2500.3090.2690.3280.2810.3400.3070.3760.2490.309\\n3360.2690.3220.2710.3222.5681.2380.2790.3300.2680.3240.2810.3420.2670.3260.2740.3290.3110.3480.3250.3660.3390.3720.3250.3880.3210.351\\n7200.3510.3770.3610.3802.7201.2870.3590.3830.4200.4220.3970.4210.3520.3830.3620.3850.4120.4070.4210.4150.4220.4190.5020.4900.4080.403\\nAvg0.2480.3070.2540.3102.3951.1770.2560.3150.2670.3220.2670.3320.2490.3120.2560.3170.2880.3320.3040.3490.3240.3680.3280.3820.2910.333\\nWins18 23 1 2 0 0 3 5 0 0 0 0 5 1 2 1 0 0 0 0 0 0 0 0 0 0\\naTaken from Wu et al. (2022).\\n16'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 0}, page_content='A Systematic Literature Review of Spatio-Temporal Graph\\nNeural Network Models for Time Series Forecasting and\\nClassification\\nFlavio Corradini 1, Marco Gori 2, Carlo Lucheroni 1, Marco Piangerelli 1, and Martina\\nZannotti1,3\\n1University of Camerino\\n2University of Siena\\n3Syeew s.r.l\\nOctober 2024\\nAbstract\\nIn recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable\\ninterest in the field of time series analysis, due to their ability to capture dependencies among\\nvariables and across time points. The objective of the presented systematic literature review is\\nhence to provide a comprehensive overview of the various modeling approaches and application\\ndomains of GNNs for time series classification and forecasting. A database search was conducted,\\nand over 150 journal papers were selected for a detailed examination of the current state-of-the-\\nart in the field. This examination is intended to offer to the reader a comprehensive collection of\\nproposed models, links to related source code, available datasets, benchmark models, and fitting\\nresults. All this information is hoped to assist researchers in future studies. To the best of our\\nknowledge, this is the first systematic literature review presenting a detailed comparison of the\\nresults of current spatio-temporal GNN models in different domains. In addition, in its final part\\nthis review discusses current limitations and challenges in the application of spatio-temporal GNNs,\\nsuch as comparability, reproducibility, explainability, poor information capacity, and scalability.\\n1 Introduction\\nIn recent years, graph neural networks (GNNs) have emerged as a powerful class of artificial neural\\nnetwork models for processing data that can be represented as graphs. GNNs are in fact particularly\\nwell-suited for a wide range of practical and engineering applications where data naturally lend them-\\nselves to be represented as graph structures, such as applications to transportation networks, image\\nanalysis, and natural language processing. The intuitive concept behind this approach is that nodes\\nin a graph can be put in correspondence to objects or concepts, whereas edges can be assumed to\\nrepresent their mutual relationships. Data for these quantities can then be processed at once by a\\nGNN model. GNNs can be used for three different classes of problems: at graph level, at edge level,\\nand at node level. In a graph-level problem, the goal is to predict a property of a graph based on its\\nentire structure, rather than on individual nodes or edges. For example, a molecule from a sample can\\nbe represented as a graph, and global properties of this molecule can be inferred from data coming\\nfrom its entire structure in relation to the sample. In an edge-level problem, the goal is to predict\\nthe presence or absence of edges between pairs of nodes. As an example, this task can be used in\\nrecommendation systems to predict potential connections between users and items on the basis of past\\ninteractions. In node-level problems, the goal is to predict the identity or role of each node within a\\ngraph, in order to solve either a classification or a regression task. Noticeably, regression tasks can be\\n”autoregressions”, that is, they can include time.\\nThis review examines then the specific use of GNNs on time series related tasks, with a particular\\nfocus on time series classification and forecasting. The idea behind the use of GNNs for time series\\n1\\narXiv:2410.22377v1  [cs.LG]  29 Oct 2024'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 1}, page_content='problems is that GNNs can be made capable of capturing complex relationships, both inter-variable\\n(connections between different variables within a multivariate series) but also inter-temporal (depen-\\ndencies between different points in time) at once. This results in a spatio-temporal GNN approach,\\nwhere the spatial dimension is related to the multivariate framework, and the temporal dimension\\nis related to the temporal nature of the data. Namely, spatio-temporal GNNs are a class of GNN\\nmodels designed to handle data in both spatial and temporal dimensions at once. In this case, in a\\nspatio-temporal graph, data in each node or edge can evolve over time, and the challenge is here to\\nmodel the related dynamic interactions over time.\\nNotwithstanding all this potential interest, to the best of our knowledge there is a lack of systematic\\nliterature reviews (SLRs) on spatio-temporal GNN models for time series applications. Most of the\\nexisting surveys are not SLRs, they are not recent, or only focus on a limited number of application\\ndomains. Individually, recent reviews focus for example on algorithmic features [9], [132], [200], [18],\\nGNN characterization [199], specific fields [80], and distributed training [125], [162]. Only a SLR on\\nspatio-temporal GNNs was indeed recently published by Zeghina et al. [223], but due to the restrictive\\nsearch query, it covers only 52 publications.\\nThe intention of the SLR presented here is to provide a broad and comprehensive overview of the\\napplications of spatio-temporal GNNs for time series classification and forecasting in different areas.\\nThis SLR also aims to assess if the current popularity of spatio-temporal GNN models is indeed due to\\ntheir effectiveness, and to evaluate their efficacy and accuracy in different fields. Additionally, it collects\\nresults and benchmarks to assist researchers in their work by synthesizing the available literature on\\nGNN applications. To the best of our knowledge, this is the first review to present comprehensive\\ntables with all the results from the various models and benchmarks proposed in a highly fragmented\\nliterature. It is thus hoped that this collected and distilled knowledge can become a valuable resource\\nand reference for researchers.\\nIn this review two sets of questions will be posed. The answers to the first set want to provide a\\ngeneral overview of the publications in the field, while the answers to the second set focus on specific\\naspects of the proposed GNN models.\\nThe general overview questions (GQs) are:\\nGQ1) Trend: What is the temporal trend of publications on GNN models? Is there a growing\\ninterest in GNNs?\\nGQ2) Fields: In which fields are GNN models most commonly applied?\\nGQ3) Journals: In which journals are the papers on GNNs published? Are these journals chosen\\nbecause of the specific domain of application or not?\\nGQ4) Research groups: Which are the most active research groups on GNNs?\\nGQ5) Tools: What tools (programming languages, libraries, frameworks) are used to implement\\nthe GNN models?\\nGQ6) Fundings: Did the authors of the papers receive public or private funding for their research?\\nThe specific questions (SQs) are:\\nSQ1) Applications: Which are the most studied applications of spatio-temporal GNNs? Are\\nthere differences in approaches and results across different application areas?\\nSQ2) Graph construction : Is the graph structure predetermined? If not, how do researchers\\ndefine it?\\nSQ3) Taxonomy: Among the various taxonomy classes, which are the most common? Are there\\nsome recurring mechanisms?\\nSQ4) Benchmark models: Which are the most common benchmark models? Are they classical\\nML models or also GNN models?\\nSQ5) Benchmark datasets: Which are the most common benchmark datasets?\\n2'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 2}, page_content='SQ6) Modeling paradigms : What is the most common paradigm? Is it modeling complex\\ninteracting systems, or modeling a system with multiple interacting quantities? Specifically,\\ndo graphs typically condense the relationships among multiple entities, or do they describe\\ndifferent aspects of the same entity? In other words, is the graph structure homogeneous or\\nheterogeneous?\\nSQ7) Metrics: Which are the most common metrics used to assess the accuracy of a given model?\\nThe answers to both the general and the specific questions will be integrated into the following dis-\\ncussion in order to provide a comprehensive overview of the present state-of-the-art of spatio-temporal\\nGNN models. A general overview of such GNN models will be assembled by highlighting similarities\\nand differences between the approaches across different fields. However, in case, for a more detailed\\nanalysis of a specific discipline or other aspects of GNN models, the reader is invited to consult the\\nspecialized reviews mentioned before ([9], [132], [200], [18], [199], [80], [125], [162]).\\nThe paper is organized as follows. After this Introduction, Sec. 2 illustrates the methodology used\\nto collect the papers. Sec. 3 provides a general overview of the selected papers. Sec. 4 introduces some\\nfundamental definitions and notions at the basis of spatio-temporal GNN models, and discusses their\\ntaxonomy and the determination of the graph structure. Sec. 5 is the core of this review. It explores\\nthe diverse domains of application of the spatio-temporal GNN models proposed in the selected papers\\nfor the identified domains. A discussion of the findings and the answers to the research questions\\nmentioned above are provided in Sec. 6. Finally, limits, challenges and future research directions are\\npresented in Sec. 7. Sec. 8 concludes the review. Appendix A provides a list of all journal papers\\nincluded in this SLR, together with the year of publication, group they belong to, case study, and\\nnature of the task (e.g., classification or forecasting).\\n2 Methodology for the systematic literature review\\nTo conduct this SLR, four primary databases were consulted: Scopus, IEEE Xplore, Web of Science,\\nand ACM. These databases were chosen for their comprehensive coverage of academic and scientific\\nliterature, ensuring a broad and inclusive search across multiple disciplines.\\nPrior to commencing the literature search, some exclusion criteria were defined. First, papers\\nfrom the current year 2024 were excluded because it has not yet ended, hence the review only covers\\nup to year 2023. Second, only journal articles were considered, in order to guarantee the inclusion of\\n”established” sources, whereas both conference papers and book chapters were excluded. The exclusion\\nof conference papers is also made in order to keep the number of papers manageable. However, it is\\nimportant to notice that relevant conference papers which provide useful benchmarks are anyway\\nreferenced throughout the text. Third, only articles published in English were included because of\\nfacility of fruition and comparison. Review articles are also not included, although they may be\\nmentioned in the test.\\nAn advanced search query submitted to the databases was designed to capture the breadth of\\nthe topic while maintaining specificity. The query used is ((\"graph neural network\" OR gnn) AND\\n\"time series\") AND (classification OR forecasting) . The first two elements of query aim to\\nrestrict the pool of GNN models to the time series domain, and the second part focuses the search on\\nclassification and forecasting applications.\\nThe selection process of the papers followed the multi-stage approach described below:\\n1. Database search: the search query was submitted to each of the four databases, and all records\\nwere imported into a csv file.\\n2. Duplicate removal: duplicate records were identified and removed.\\n3. Title and abstract screening: the remaining records were screened by reading their titles and\\nabstracts. Studies that at first sight did not meet the criteria were discarded.\\n4. Full-text screening: the full texts of the remaining papers were analyzed one by one, which\\nled to a further assessment of eligibility.\\n5. Data extraction : relevant data were extracted from the included studies, and analyzed in\\norder to better discuss the approach and present the most common datasets and models for each\\nidentified field.\\n3'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 3}, page_content='A total of 473 records were identified, 188 from Scopus, 31 from IEEE, 104 from Web of Science,\\nand 150 from ACM. A scheme of this selection process is sketched in Fig. 1. First, 142 duplicates\\nwere removed, then the titles and abstracts of the remaining 331 records were manually evaluated to\\nselect the pertinent papers. Among these, 170 were discarded because they were considered outside\\nthe scope of this review, and 5 were discarded because they were not in English. Hence, 156 records\\nwere included in the review. Next, Sec. 3 provides an overview of the selected publications.\\nFigure 1: PRISMA flowchart to summarize the identification of the studies and the selection process,\\nadapted from Ref. [145].\\n3 Overview of the publications\\nThis section provides a comprehensive overview of the 156 selected journal articles. After their se-\\nlection, the papers were grouped according to their domain of application, as better explained in the\\nthematic analysis of Sec. 5. In the following, some bibliographic information about the reviewed articles\\nis presented.\\nFig. 2 illustrates the number of publications over time across different groups. A cumulative\\npositive trend can be observed over time, reflecting an increasing interest in spatio-temporal GNN\\nmodels across various fields, since 2020. The three most common groups that have seen the greatest\\nincrease in publications are ”Environment”, ”Generic”, and ”Mobility”. For more details, see also the\\ngroup numerosity reported in Tab. 1.\\nAn inspection of the publication sources indicates that 95 different journals have published articles\\non GNN models for time series forecasting or classification. Tab. 2 lists the 15 journals which contain\\nthe highest number of published papers. The diversity of the journals indicates a vast range of potential\\napplications for spatio-temporal GNN models in different fields. This suggests that researchers often\\nselect journals based on the specific domain of application, allowing them to reach targeted audiences\\nand contribute within their respective fields. Such diversity also underlines the interdisciplinary nature\\nof GNNs research, as it spans multiple domains. In order to examine the degree of collaboration among\\nresearchers, a co-authorship network was generated by using the VOSviewer software [42]. Fig. 4\\n4'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 4}, page_content='0\\n25\\n50\\n75\\n100\\n2020 2021 2022 2023\\nPredictive monitoring\\nOther\\nMobility\\nHealth\\nGeneric\\nFinance\\nEnvironment\\nEnergy\\nNumber of journal publications by group over time\\nFigure 2: Number of journal publications over time across the different groups.\\nTable 1: Pivot table illustrating the number of publications by year and group.\\nField / Year 2020 2021 2022 2023 Total\\nEnergy 1 6 6 13\\nEnvironment 2 4 21 27\\nFinance 3 6 9\\nGeneric 1 4 19 24\\nHealth 1 2 5 8\\nMobility 2 4 15 18 39\\nOther 2 4 14 20\\nPredictive monitoring 2 3 11 16\\nTotal 3 12 41 100 156\\nillustrates authors as nodes and collaborations as connections, revealing some research clusters. Two\\nof these clusters are enlarged and shown in Fig. 4b. In the figure, the color of the nodes is related\\nto the average publication year of an author, so that the evolution of research groups over time can\\nbe evaluated. Two key observations can be made. First, the number of clusters suggests that there\\nare many distinct research groups, indicating a high degree of fragmentation within the community.\\nSecond, the color of the nodes indicates that some groups were active only at the beginning and have\\nsince disappeared, while others have recently formed, as indicated by their yellow color. Some authors\\nhave various collaborations over time, as also shown in Fig. 4b.\\nFig. 3 shows a pie chart of the distribution of corresponding authors among different countries.\\nThis visualization provides an overview of the global distribution of research activities. More than the\\n70% of the authors are affiliated with institutions in China.\\n5'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 5}, page_content=\"Table 2: Top 15 journals with the highest number of published paper.\\nJournal Number of papers\\nIEEE Access 7\\nACM Transactions on Knowledge Discovery from Data 6\\nApplied Intelligence 6\\nIEEE Transactions on Instrumentation and Measurement 6\\nACM Transactions on Intelligent Systems and Technology 5\\nExpert Systems with Applications 5\\nSensors 4\\nApplied Energy 3\\nApplied Soft Computing 3\\nEnergy 3\\nEngineering Applications of Artificial Intelligence 3\\nIEEE Transactions on Intelligent Transportation Systems 3\\nInformation Sciences 3\\nInternational Journal of Data Science and Analytics 3\\nKnowledge-Based Systems 3\\nIndia\\n1,3%\\nAustralia\\n2,6%\\nSwitzerland\\n2,6%\\nSouth Korea\\n3,8%\\nUnited States\\n6,4%\\nChina\\n72,4%\\nCountry of corresponding author's affiliation\\nFigure 3: Pie chart of the distribution of corresponding authors in different countries.\\n6\"),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 6}, page_content='(a) Visualization of the complete collaboration network.\\n(b) Zoom in on two different clusters.\\nFigure 4: Network of collaboration between researchers, created using VOSviewer software.\\n7'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 7}, page_content='4 Graph neural networks\\nThis section introduces some fundamental definitions and notions at the basis of the study of time\\nseries with spatio-temporal GNNs.\\n4.1 Definitions and notations\\nDefinition 1 (Time series). A time series is a sequence of data points indexed in time order (that\\nhere is assumed to be regularly sampled, i.e., equispaced). An equispaced univariate time series of\\nlength T is a sequence of scalar observations collected over time, denoted as ⃗ xt ∈ RT . An equispaced\\nmultivariate time series of length T is a sequence of D-dimensional vector observations collected over\\ntime, denoted as ⃗ xt ∈ RD×T .\\nDefinition 2 (Graph). A graph G is a pair of finite sets G = (V, E), where V = {v1, . . . , vn} is a set\\nof n nodes (also called vertices) and E ⊆ V × V is the set of edges. In an undirected graph each edge\\nis an unordered pair of nodes {vi, vj}, while in a directed graph the edges have an orientation and\\ncorrespond to ordered pairs ( vi, vj). When an edge ( vi, vj) or {vi, vj} exists, the nodes vi and vj are\\ncalled adjacent.\\nDefinition 3 (Spatio-temporal graph). A spatio-temporal graph is a 4-tuple G = (Vt, Et, Xt, T ), where\\nT = {t1, . . . , tT } is a set of T timestamps t, Vt = {v1(t), . . . , vnt(t)} is a set of nt nodes representing\\nspatial entities at time t, Et = {e1(t), . . . , emt(t)} is a set of mt edges representing the relationships\\nbetween nodes at timet, Xt = {xv1(t), . . . , xvnt (t)}∪{re1(t), . . . , remt (t)} is a set of attributes associated\\nwith nodes and edges ( xvi and rei are respectively the attributes of node vi and edge ei, and can be\\neither scalars of vectors) at time t. Notice that the sets of nodes and edges can change over time, as\\nwell as the set of attributes Xt.\\nDefinition 4 (Adjacency matrix). The adjacency matrix A of a graph G with |V| = n nodes is a\\nn × n square matrix, with Aij specifying the number of connections from node vi to node vj for\\ni, j = 1 , . . . , n. Here it is assumed that a graph cannot have more than one edge between any two\\nnodes, so Aij can only be 0 or 1 depending on whether there is a connection or not.\\nDefinition 5 (Degree matrix). The degree matrix D of a graph with |V| = n nodes is a diagonal\\nmatrix whose entries are given by the degree of each node, i.e., the number of edges attached to the\\nnode. In formulas, its diagonal elements are given by\\nDii =\\nX\\nj\\nAij . (1)\\n4.2 Overview of graph neural networks\\nGiven a graph G = (V, E), a GNN model (not necessarily changing in time, i.e., spatio-temporal) aims\\nto generate an embedding (i.e., a vector of real entries) for each node in the graph. An overall graph\\nembedding consists of representing a graph into a possibly different-dimensional space while preserving\\nits structural information.\\nThe notion of GNN was initially proposed by Gori et al. in 2005 in Ref. [51] and by Scarselli\\net al. in 2009 in Ref. [160] as an architecture to implement a function that maps a graph G with n\\nnodes into an m-dimensional Euclidean space. In Ref. [160] the target node’s embedding was learned\\nby first propagating forward at fixed weights neighbor information within the graph in an iterative\\nmanner, until a stable fixed point is reached (whose existence is guaranteed under certain assumptions\\nby Banach’s fixed point theorem), and then propagating errors on targets back across the graph. The\\ncalculation of node embeddings is in general quite complex, and over time many other methods have\\nbeen developed. These methods are based on different architectures and sophisticated mechanisms.\\nBased on these aggregating mechanisms, a taxonomy of GNN models with different classes has been\\ndefined. In this review, we adopt a taxonomy for the discussed spatio-temporal GNNs similar to the\\none proposed by Chen et al. in Ref. [18]. More details will be given in Subsec. 4.3.\\n8'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 8}, page_content='4.3 Spatio-temporal GNNs and their taxonomy\\nThe majority of research on spatio-temporal GNNs is focused on multivariate time series, as they can\\nbe naturally abstracted into spatio-temporal graphs, as depicted in Fig. 5. In this context, nodes cor-\\nrespond to different variables at a given time step, and edges represent relationships between variables.\\nThis spatio-temporal modeling assumes that the feature information of a node depends on both its\\nFigure 5: Representation of a spatio-temporal graph. Here, each node has a dynamic feature, given\\nby the points of the time series.\\nown historical values and on the historical data from its neighboring nodes [223]. Once the the graph\\nstructure is given, information is propagated within it until the output is obtained, as better explained\\nin the following.\\nWhen it comes to the development of a GNN model, there are two primary approaches: one\\nthat handles spatial and temporal substructures in separate blocks, and another that integrates and\\nprocesses these two substructures together [223]. This is implemented by means of modules that are\\npurely spatial, purely temporal, or hybrid combinations of the two. These modules are then organized\\ninto a series of blocks to create the final spatio-temporal GNN model.\\nIn the taxonomy of spatio-temporal GNNs used in this review, the focus is exclusively on the\\ndescription of the spatial module that propagates the information within the nodes at fixed time t\\n(not explicitly included in the following equations for simplicity), which is in the reviewed literature\\na fundamental aspect that differentiates one GNN model from another. Based on the mechanism at\\nthe basis of the information propagation in the graph at a given time step, spatio-temporal GNNs can\\nbe further classified as recurrent GNNs, convolutional GNNs, and attentional GNNs, as explained in\\nthe following. These classes are not mutually exclusive, as some models may fit into hybrid categories.\\nIn addition, the classes are not fully exhaustive, as there may be models that do not align with any\\nof the existing categories. However, for the purpose of this review, the taxonomy discussed above is\\nsufficient.\\nRecurrent GNNs. Recurrent graph neural network modeling mainly includes the results of\\nearly studies on GNNs, where the node embeddings are generated by iteratively propagating neighbor\\ninformation until a stable fixed point is reached. The iterative equation for the calculation of the\\nembedding (or hidden state) of node u has the form\\nh(k)\\nu = f\\n\\x10\\nxu, {xv, r(u,v), h(k−1)\\nv | v ∈ N (u)}\\n\\x11\\n, (2)\\nwhere k is the index related to iteration, f is a parametric trainable function, xu is the attribute\\nassociated to node u, r(u,v) is the weight of the edge ( u, v) between the nodes u and v, N (v) denotes\\nthe neighborhood of node v, and the initial hidden states h(0) are usually randomly initialized. As\\ndetailed in Ref. [160], which introduced recurrent GNNs, to ensure the convergence of Eq. (2), it is\\nnecessary that the iterative function f is a contraction mapping.\\nConvolutional GNNs. Convolutional graph neural networks are closely related to recurrent\\ngraph neural networks. They were at first introduced by Kipf and Welling in 2017 in Ref. [91].\\nUnlike recurrent GNNs, which apply the same iterative contraction mapping f until an equilibrium\\nis reached, convolutional GNNs use different parameters at each updating step, by stacking multiple\\ngraph convolutional layers to extract the node embeddings [200].\\nA basic form for the equation for the kth layer of a convolutional graph neural network with n\\n9'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 9}, page_content='nodes is\\nH (k) = fk\\n\\x10\\n˜AH (k−1)Θk−1\\n\\x11\\n, ˜A = ˆD− 1\\n2 ˆA ˆD− 1\\n2 , ˆA = A + I , ˆDii =\\nX\\nj\\nˆAij , H (0) = X , (3)\\nwhere fk represents the convolutional operation performed at layer k ∈ {1, . . . , K}, A ∈ Rn×n is the\\nadjacency matrix, I ∈ Rn×n is the identity matrix (used to add the self-loops in the adjacency matrix\\nA), ˆD ∈ Rn×n is the diagonal degree matrix associated to ˆA, H (k) ∈ Rn×dk are the dk-dimensional\\nnode embeddings h(k)\\nv of the entire graph produced by the kth layer, X ∈ Rn×d0 is the collection of\\nall node attributes xv of the entire graph, and Θ k ∈ Rdk−1×dk is a trainable weight matrix for the kth\\nlayer [18]. The mechanism of node embedding generation from Eq. (3) is illustrated in Fig. 6b, where\\nit is put in contrast with the mechanism of a recurrent GNNs, shown in Fig. 6a.\\n(a) Recurrent GNNs use the same graph recurrent\\nlayer in the iterative generation of the node em-\\nbeddings. Here Θ represents the weights of the\\nparametric trainable function f.\\n(b) Convolutional GNNs use different graph con-\\nvolutional layers in the generation of the node em-\\nbeddings.\\nFigure 6: Comparison of the node embedding generation mechanisms in a recurrent GNN (left panel)\\nand a convolutional GNN (right panel).\\nAttentional GNNs. In attentional GNNs, the aggregation process uses the attention mechanism\\n[178] to combine node features. The equations to compute the node embedding hu of layer k + 1 from\\nthe embeddings of layer k for each node u are\\nz(k)\\nu = Wkh(k)\\nu , (4a)\\nϵ(k)\\nuv = LeakyReLU\\n\\x10\\n⃗ a(k)⊤\\x10\\nz(k)\\nu || z(k)\\nv\\n\\x11\\x11\\n, (4b)\\nα(k)\\nuv =\\nexp\\n\\x10\\nϵ(k)\\nuv\\n\\x11\\nX\\nw∈N (u)\\nexp\\n\\x10\\nϵ(k)\\nuw\\n\\x11 , (4c)\\nh(k+1)\\nu = f\\n\\uf8eb\\n\\uf8ed X\\nv∈N (u)\\nα(k)\\nuv z(k)\\nv\\n\\uf8f6\\n\\uf8f8 . (4d)\\n10'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 10}, page_content='Eq. (4a) represents a learnable linear transformation of the node embedding h(k)\\nu using the trainable\\nweight matrix Wk of layer k. Eq. (4b) computes a pairwise masked attention score between nodes u\\nand v by concatenating (||) the vectors z(k)\\nu and z(k)\\nv , calculating a dot product with a learnable weight\\nvector ⃗ a(k), and applying the LeakyReLU nonlinearity. The quantity ϵuv indicates the importance of\\nthe features of node v to node u. The term ”masked” refers to the fact that ϵuv are only computed for\\nneighboring nodes. Eq. (4c) normalizes the attention score through the softmax function in order to\\nmake the αuv comparable across different nodes. Finally, Eq. (4d) computes the next layer embedding\\nh(k+1)\\nu by aggregating the information in neighboring nodes, weighted by the attention scores [179].\\nGraph attention networks can also further use a multi-head attention mechanism to stabilize the\\nlearning process of self-attention. In practice, the node updating operation in graph attention networks\\nis a generalization of the traditional averaging or max-pooling of neighboring nodes, which allows each\\nnode to compute a weighted average of its neighbors and identify the most relevant ones [12].\\nAs anticipated, the taxonomy discussed so far focuses only on the propagation mechanisms of the\\nspatial component. However, spatio-temporal GNNs typically consist of a stack of spatial, temporal,\\nor hybrid modules, each one with distinct roles.\\nThe spatial module is responsible for propagating information between nodes, thereby enabling the\\nanalysis of cross-sectional inter-dependencies between different variables. As previously explained in\\nthe description of the mechanisms underlying the taxonomy, the spatial module aggregates information\\nstarting from immediate neighboring nodes and extends it outward, thereby capturing also the influ-\\nence of distant nodes. Noticeably, this spatial operation does not account in principle for temporal\\ninformation. In contrast, the temporal module, often based on architectures such as LSTM, GRU,\\nor self-attention, focuses on the evolution of data over time, independently from cross-sectional node\\ninteractions.\\nWhen these different modules are stacked together, spatial and temporal information end up to\\nbe used simultaneously, although they are processed in separate modules. This occurs because the\\ncomputation of each node state is influenced by both spatial and temporal information. This combined\\napproach allows the spatio-temporal GNN model to effectively capture at the same time both spatial\\nrelationships and temporal dynamics.\\n4.4 Determination of the graph structure\\nA crucial issue when dealing with spatio-temporal GNNs for time series problems is the determination\\nof the graph structure, i.e. the connectivity of the nodes. Namely, some time series datasets come by\\nnature with a pre-defined graph structure (e.g., road network), while some others do not. When a\\nnatural pre-defined graph structure is available, it helps the model to better capture the underlying\\ndynamics of the system. When it is not directly available, it must be in some way defined by the\\nuser (based on domain knowledge or some metrics), determined by some algorithm (such as visibility\\ngraphs [224], [96], [183]), or learned by the model itself.\\nOnce the graph structure has been determined, it is necessary to define the weighted graph adja-\\ncency matrix, which is a generalization of the graph adjacency matrix whose entries are given by the\\nedge weights of the graph. The edge weights can be defined a priori by the user, or again, learned by\\nthe model itself based on a pre-defined architecture. In the first case, the edge weights are assigned\\nbased on some pre-defined metrics or criteria chosen by the user (e.g., spatial distance between the\\nsites or similarity measures). In the latter case, the weights are continuously adjusted throughout\\ntraining as the model learns from the data.\\nEach section of the following thematic analysis examines which approach is prevalent.\\n5 Thematic analysis\\nThis section explores the diverse applications of GNN models in different groups. As said, the selected\\njournal papers are divided in groups according to the application domain. The goal is to provide an\\noverview of the various empirical studies, the most common approaches, and the results obtained for\\ndifferent applications.\\nIn each subsection of the following thematic analysis, a list of the benchmark models that appear\\nin the selected journal papers will be provided. They will be classified as: mathematical and statistical\\nmodels coming from Econometrics, traditional machine learning models, and GNN-based models. As\\n11'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 11}, page_content='for mathematical and statistical models, they generate forecasts by analyzing a limited amount of\\ndata, and they usually struggle with cross-sectional large-scale and temporal long-term dependencies.\\nTraditional machine learning models use more complex architectures, mainly neural networks, but they\\nare not necessarily able to capture intricate spatio-temporal dependencies within the data. GNN-based\\nmodels instead, attempt to represent these complex dependencies using various techniques, resulting\\nin better, though not perfect, accuracy. In addition, information on the most common datasets will\\nbe given, along with the best results reported in the literature. All results presented in the tables of\\nthis SLR are taken directly from the tables of the selected papers.\\nThe groups of papers are presented in alphabetical order, with the exception of the ”Generic”\\napplications group (which cannot be directly attributed to a specific case study) and the ”Other\\ntopics” group (focusing on specific problems of other disciplines). In subsections with a large number\\nof papers and high homogeneity of the datasets, more precise comparisons of the results and in-depth\\nanalyses will be conducted.\\n5.1 Energy\\nThe first group of papers is ”Energy” (with 13 out of 156 papers), which includes all the studies related\\nto energy systems. In particular, it includes research on electricity load, energy consumption, and power\\ngeneration forecasting. This is somehow connected to the ”Environment” subsection (Subsec. 5.2).\\nIndeed, green energy sources are associated with environmental phenomena such as wind speed and\\nsolar radiation inflow. However, this subsection focuses on energy applications, whereas Subsec. 5.2\\nrefers to the study of environmental data that may or may not be related to energy applications. Wind\\nspeed forecasting, for instance, is connected to wind power generation due to the cubic relationship\\nbetween wind speed and wind power [7]. However, wind speed forecasting may also be of interest for\\nother reasons, and thus it is included in the more general Subsec. 5.2.\\n5.1.1 Overview\\nEnergy forecasting is crucial for electricity power grid stability and operational efficiency. Accurate\\nload and generation forecasts ensure that supply meets real-time demand, preventing outages and\\ninstability, and maintaining a reliable supply of power. In addition, effective forecasting allows for\\noptimal scheduling of power plants and energy resources, reducing operational costs and improving\\noverall system efficiency. The majority of econometric and machine learning models tend to focus on\\ntime series from a single site or consider multiple sites without explicitly capturing the relationships\\nbetween them. GNN models may be an innovative and effective approach to addressing energy related\\nproblems.\\nAn overview of the selected papers reveals a noticeable interest in the application of GNNs in the\\nenergy field since 2022. The most frequently studied fields include forecasting of wind ([63], [219],\\n[105]) and solar power ([34], [167], [168], [155]), which are renewable energy sources characterized by\\ntheir intermittency. The two most popular journals among the selected papers are Applied Energy by\\nElsevier, and Frontiers in Energy Research by Frontiers Media SA. The majority of the corresponding\\nauthors are affiliated with institutions in China and in the United States.\\n5.1.2 Datasets\\nThe study of the selected papers in the ”Energy” group suggests that there are no particularly common\\nor widely recognized benchmark datasets. Indeed, researchers often focus on specific datasets, typically\\ncollected within the university campus or in the country where they are located. For the same reason,\\nthe links to these datasets are not always available. In Tab. 3 are listed the links to the public datasets\\nmentioned in the papers.\\nTable 3: List of public datasets in the ”Energy” group and their corresponding links.\\nDataset Used by Link\\nDanish smart heat meters [195] https://doi.org/10.5281/zenodo.6563114\\nDKA Solar Centre [34] https://dkasolarcentre.com.au/\\n12'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 12}, page_content='IEEE bus systems [240] https://doi.org/10.1109/UPEC.2015.7339813\\n(source paper)\\nJERICHO-E-usage [195] https://doi.org/10.6084/m9.figshare.c.524545\\n7.v1\\nNREL [63], [168], [219] https://www.nrel.gov/\\nOPSD [229] https://open-power-system-data.org/\\nPV Switzerland [167], [168] https://doi.org/10.1109/IJCNN48605.2020.\\n9207573 (source paper)\\nUMass Smart* Dataset [191] https://traces.cs.umass.edu/index.php/Smar\\nt/Smart\\nThree of the selected papers ([63], [168], [219]) use data from the National Renewable Energy\\nLaboratory (NREL), a national laboratory of the U.S. Department of Energy (https://www.nrel.gov/).\\nAdditional datasets were used for exogenous weather variables and satellite imagery (such as Himawari-\\n8, available at https://www.data.jma.go.jp/mscweb/en/himawari89/space segment/spsg sample.html\\nin Ref. [34]), but they are of course highly dependent on the location of the study. The data granularity\\nranges from 1 minute to 1 hour, and the forecasting horizon from tens of minutes to days. They depend\\non the dataset, the domain of application, and the purpose of the forecast. Most datasets require pre-\\nprocessing, including missing data interpolation, normalization, and satellite image processing.\\n5.1.3 Proposed models\\nAs for the taxonomy of the proposed GNN models, 8 out of 13 papers use a convolutional GNN,\\nand the other 5 use an attentional GNN. Regarding the graph structure, the papers typically provide\\nonly a brief description. For example, it is rarely highlighted whether the graph structure is static or\\ndynamic, and the way in which it is constructed is often described summarily. In some papers, the\\ngraph structure reflects the geographic location of the elements (e.g., wind or solar power plants, as\\nin Refs. [219] and [155]), whereas in others it is based on the correlation or similarity between time\\nseries (as in Ref. [34]). In some studies, a pre-defined structure is imposed and the weighted adjacency\\nmatrix is learned directly from the model, often using the attention mechanism as in Ref. [168].\\nThe most common loss function used to train the models is the MSE. Many papers mention the\\nprogramming languages Python and Matlab, and few of them mention the Python libraries PyTorch\\n[150] and TensorFlow [1] for the implementation of the GNN model. In this group, only Ref. [191]\\nprovides the link to the source code of the proposed residential load forecasting with multiple correlation\\ntemporal graph neural networks (RLF-MGNN) model (available at https://codeocean.com/capsule/\\n9294192/tree/v1).\\n5.1.4 Benchmark models\\nTab. 4 lists the benchmark models used in the ”Energy” group by at least two papers included in this\\nreview.\\nTable 4: List of benchmark models in the ”Energy” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nAutoregressive integrated moving average\\n(ARIMA)\\n[195], [229], [168], [68]\\nClassical\\nmachine\\nlearning\\nmethods\\nBidirectional long-short term memory net-\\nwork (BiLSTM) [52]\\n[195], [34]\\nConvolutional neural network (CNN) [195], [45], [167], [168]\\n13'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 13}, page_content='Hybrid convolutional neural network and\\nlong-short term memory network (CNN-\\nLSTM)\\n[195], [191]\\nEncoder-decorer architectures (ED) [195], [34], [167], [168]\\nFeed-forward neural network (FNN) [195], [45], [34], [68], [105]\\nGated recurrent unit (GRU) [195], [68]\\nInformer [243] [195], [229]\\nk-nearest neighbors (KNN) [45], [219]\\nLong-short term memory network (LSTM) [195], [229], [34], [63], [240],\\n[219]\\nRecurrent neural network (RNN) [229], [191], [34], [63]\\nSupport vector regression (SVR) [167], [168], [219], [105]\\nTransformer [178] [229], [34]\\nGNN methods Spatio-temporal autoregressive model (ST-\\nAR) [16]\\n[167], [168]\\nThe majority of the papers only use classical machine learning benchmarks. Some papers use simple\\nmathematical and statistical models, such as linear regression (that we incorporated into ARIMA) or\\nhistorical average. Only few papers use other GNN-based models as benchmarks, and there are no\\ncommon benchmarks among them. This is due to the fact that this field has not been extensively\\nexplored in the past, and only a very limited number of GNN models have been specifically developed\\nfor energy forecasting.\\n5.1.5 Results\\nIn all the papers, the authors claim that the proposed GNN model outperforms the benchmarks. The\\nmost common error metrics are the MAE, the RMSE, and the Normalized Root Mean Square Error\\n(NRMSE).\\nIt is not possible to compare the accuracy of the discussed models because there are no common\\nbenchmark datasets. The only two papers that study the same dataset are written by the same authors\\n(Refs. [167] and [168]), and their most recent paper includes a comparison with the models proposed\\nin the first one.\\n5.2 Environment\\nThe group ”Environment” includes a significant number of the selected papers (27 out of 156), which\\nsuggests a considerable research interest on the application of GNNs in this field. However, this group\\nis highly fragmented, as it includes a large number of subdomains and applications, all related to the\\nstudy of environmental data.\\n5.2.1 Overview\\nThe group of environmental studies is broad and includes several disciplines such as physics, clima-\\ntology, oceanography, and atmospheric science. The breadth of the group is reflected in the number\\nof case studies presented in the selected papers, which cover air quality prediction (in terms of PM 2.5\\nconcentration), sea temperature, wind speed, and other applications. Forecasting these quantities can\\nbe useful for several reasons. For instance, predicting wind speed or rainfall can assist in anticipat-\\ning some extreme weather phenomena, and is also related to the prediction of green energy outputs.\\nFor example, in Refs. [7] and [8] the authors estimate the wind power forecasting error obtained by\\nusing wind speed forecasting models. As another example, predicting sea surface temperature can be\\nuseful for weather forecasting, fishing directions, and disaster warnings [209]. Moreover, monitoring\\nand forecasting environmental data are crucial for evaluating the impact of human activities on the\\nenvironment and tracking the progression of climate change.\\n14'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 14}, page_content='The methods for forecasting environmental data are typically categorized as numerical models,\\nstatistical models, and machine learning methods. Numerical methods are based on atmospheric\\nmodels, and are designed to quantify the interaction of different atmospheric variables. The accuracy\\nof these models depends on the availability of data, and the mathematical simulations to get the\\nforecasts may take days or even weeks, thus limiting the ability to make good short-term forecasts.\\nLinear statistical methods leverage long-term dependencies in the recorded data by using regression-\\nlike models. However, they cannot capture non-linear relationships in the dataset, which limits their\\npower. Machine learning models, on the other hand, are often a generalization of linear statistical\\nmethods that can capture more complex and non-linear relationships in the data.\\nAn overview of the selected papers reveals that there has been a rapid increase in interest in the\\nfield among the GNN research community, and more than 20 of the selected papers were published in\\n2023. The most investigated subjects are sea temperature ([209], [87], [147], [47], [187], [173], [164]),\\nwind speed ([7], [157], [8], [49], [46], [40]), and PM 2.5 concentration forecasting ([131], [171], [110],\\n[139], [86], [151]). All the selected papers are published in different journals, with the exception of the\\nEngineering Applications of Artificial Intelligence by Elsevier, which has published two of the papers.\\nThe majority of the corresponding authors are affiliated with institutions in China.\\n5.2.2 Datasets\\nThe analysis of the selected papers shows that in this group of papers there is no reference benchmark\\ndataset. The publicly available datasets cited in the papers are listed in Tab. 5.\\nTable 5: List of public datasets in the ”Environment” group and their corresponding links.\\nDataset Used by Link\\nAgroData-10 [209] https://www.aoml.noaa.gov/phod/argo\\nCCMP wind data [157], [49], [46] http://data.remss.com\\nChina Environmental\\nMonitoring Station PM 2.5\\n[131], [151] https://www.cnemc.cn/\\nChina National Urban Air\\nQuality\\n[22], [39] https://github.com/Friger/GAGNN\\nCopernicus 3D thermoha-\\nline data\\n[147] https://resources.marine.copernicus.eu/pro\\nducts\\nECMWF East China Sea [152] https://github.com/Boltzxuann/GIPMN/tr\\nee/dataset\\nECMWF South and East\\nChina Sea\\n[236] http://apps.ecmwf.int/datasets/data/interim\\n-full-daily\\nKnowAir [139] https://doi.org/10.1145/3397536.3422208\\n(source paper)\\nKorean Peninsula sea data [87] https://www.nifs.go.kr/risa/main.risa\\nNOAA sea datasets [209], [47], [187],\\n[40], [164]\\nhttps://www.ndbc.noaa.gov/\\nNorwegian offshore wind [7], [8] https://frost.met.no/index.html\\nTaiwan air quality dataset [171] https://data.gov.tw/en\\nUrbanAir [139] https://doi.org/10.1016/j.chemosphere.2018\\n.12.128 (source paper)\\nUS EPA PM2.5 [22], [110] https://aqs.epa.gov/aqsweb/airdata/downloa\\nd files.html\\nFive of the selected papers ([209], [47], [187], [40], [164]) use National Oceanic and Atmospheric\\nAdministration (NOAA) sea datasets (https://www.noaa.gov/) from the U.S. Department of Com-\\nmerce (US DOC). The US DOC website permits users to download daily, weekly and monthly mean\\noptimum interpolation sea surface temperature (OISST) data from September 1981 for many geo-\\ngraphical locations. In over half of the papers, exogenous variables are incorporated into the model.\\n15'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 15}, page_content='These include weather data (e.g., humidity, rainfall, pressure, temperature, wind) and, in the case of\\nPM2.5 concentration forecasting, other pollutants (e.g., CO, NO 2, O3, PM10, SO2).\\nThe data granularity ranges from 1 minute to 1 month, depending on the case study, and the\\nforecasting horizon from 10 minutes to 240 days. Most datasets require some pre-processing, including\\ninterpolation of missing data, removal of outliers and normalization.\\n5.2.3 Proposed models\\nRegarding the taxonomy of the proposed GNN models, the majority of papers present convolutional\\nGNN (17 out of 27 papers), followed by 6 attentional GNNs and 3 hybrid architectures. The description\\nin one paper (Ref. [39]), however, does not specify the classification of its model within the proposed\\ntaxonomy. As for the graph structure, not all of the papers describe it with sufficient precision, and\\nsome of them are vague about the graph construction and its static or dynamic nature. In almost half\\nof the papers, the edge weights are learned directly from the model, often using the graph attention\\nmechanism. When the graph structure is defined manually, the most popular criterion is the spatial\\ndistance between the sites ( d), even in its exponential ( e−d) or inverse proportion (1 /d) formulation.\\nFinally, there are a few papers where more than one graph structure is fused together in order to\\ncapture more complex dynamics, as in Ref. [131].\\nNot all papers specify the loss function used to train the models. Among those that do, MAE and\\nMSE are used in roughly equal proportions. Many papers mention the programming language Python\\nand the library PyTorch for the implementation of the models. Few papers also provide a link to the\\nsource code for the proposed model, as shown in Tab. 6.\\nTable 6: List of source codes of the ”Environment” models in the review.\\nModel Link\\nGroup-aware graph neural network (GAGNN) [22] https://github.com/Friger/GAGNN\\nGridded information propagation and mixing net-\\nwork (GIPMN) [152]\\nhttps://github.com/Boltzxuann/GIPM\\nN/tree/dataset\\nHierarchical graph recurrent network (HiGRN) [209] https://github.com/Neoyanghc/HiGRN\\nSpatio-temporal FFTransformer (ST-\\nFFTransformer) [7]\\nhttps://github.com/LarsBentsen/FFTra\\nnsformer\\n5.2.4 Benchmark models\\nTab. 7 presents the benchmark models employed in the ”Environment” group by at least two articles\\nexamined in this review.\\nTable 7: List of benchmark models in the ”Environment” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nAutoregressive integrated moving average\\n(ARIMA)\\n[209], [87], [8], [126]\\nHistorical average (HA) [171], [139], [209], [126]\\nNaive [7], [8]\\nVector autoregression (VAR) [209], [8], [126]\\nClassical\\nmachine\\nlearning\\nmethods\\nBidirectional long-short term memory net-\\nwork (BiLSTM) [52]\\n[87], [40]\\nCombined FC-LSTM and convolution neural\\nnetwork (CFCC-LSTM) [213]\\n[147], [187]\\nHybrid convolutional neural network and\\ngated recurrent unit (CGRU) [206]\\n[157], [49], [46]\\nConvolutional neural network (CNN) [101], [40]\\n16'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 16}, page_content='Hybrid convolutional neural network and\\nlong-short term memory network (CNN-\\nLSTM)\\n[110], [173]\\nConvolutional long-short term memory net-\\nwork (ConvLSTM) [165]\\n[209], [173]\\nFeed-forward neural network (FNN) [131], [86], [126]\\nGated recurrent unit encoder–decoder (GED)\\n[204]\\n[47], [187], [164]\\nGated recurrent unit (GRU) [131], [139], [86], [151], [126],\\n[6], [40]\\nInformer [243] [101], [164]\\nLong-short term memory network (LSTM) [22], [131], [171], [110], [39],\\n[139], [86], [151], [209], [101],\\n[236], [87], [147], [47], [126],\\n[187], [6], [40], [164]\\nPredictive deep convolutional neural network\\n(PDCNN) [245]\\n[157], [49]\\nSupport vector regression (SVR) [171], [139], [147], [47], [126]\\nTemporal convolutional network (TCN) [5] [139], [101]\\nTemporal pattern attention long-short term\\nmemory network (TPA-LSTM) [166]\\n[139], [157], [236], [147], [49],\\n[46]\\nTransformer [178] [101], [87]\\nHybrid wavelet random forest and deep belief\\nnetwork (Wavelet-DBN-RF)\\n[157], [49], [46]\\nExtreme gradient boosting (XGBoost) [22], [131], [39], [40]\\nGNN methods Deeper graph convolutional network (Deep-\\nerGCN) [104]\\n[22], [39]\\nGraph convolutional neural network and long-\\nshort term memory (GC-LSTM) [154]\\n[22], [131], [39], [139], [86]\\nGraph convolutional network (GCN) [126], [173]\\nGraph WaveNet (GWN) [202] [22], [139], [46], [164]\\nHierarchical graph convolution network\\n(HGCN) [56]\\n[22], [209]\\nMultivariate time series forecasting with\\ngraph neural network (MTGNN) [201]\\n[139], [236], [46]\\nMulti long-short term memory network\\n(Multi-LSTM) [50]\\n[157], [49], [46]\\nSuperposition graph neural network (SGNN)\\n[219]\\n[157], [49]\\nSpatio-temporal graph convolutional network\\n(STGCN) [216]\\n[171], [139], [209], [147], [164]\\nSpatio-temporal correlation graph neural net-\\nwork (STGN) [49]\\n[157], [46]\\nSpatio-Temporal U-Network (ST-UNet) [217] [22], [39]\\nTemporal graph convolutional network (T-\\nGCN) [239]\\n[139], [101], [147]\\nThe majority of the benchmarks used fall within the category of classical machine learning, although\\nthere are several GNN benchmarks as well. The two most common benchmark models are LSTM and\\nGRU. These models are widely used by environmental data forecasters due to their ability to capture\\ntemporal dependencies, and they are considered a reliable benchmark in the field. As for GNN models,\\n17'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 17}, page_content='the most common benckmarks are the graph convolutional neural network and long-short term memory\\n(GC-LSTM) [154] (which appears only in quality air forecasting problems) and spatio-temporal graph\\nconvolutional network (STGCN) [216]. The GC-LSTM is a model that integrates graph convolutional\\nnetworks and long-short-term memory networks, proposed by Qi et al. in 2019 to forecast the spatio-\\ntemporal variation of PM 2.5 concentration. The STGCN model instead was proposed in the traffic\\ndomain by Yu et al. in 2018. This model is composed of two spatio-temporal convolutional blocks\\nand a fully-connected output layer. Each convolutional block contains two temporal gated convolution\\nlayers and one spatial graph convolution layer in the middle. The code for the STGCN model is\\navailable at https://github.com/VeritasYin/STGCN IJCAI-18. Finally, it is important to point out\\nthat the multivariate time series forecasting with graph neural network (MTGNN) model proposed by\\nWu et al. in 2020 in Ref. [201] is often the second-best performing model in the papers using it, thus\\nit deserves more attention and further investigation in future research.\\n5.2.5 Results\\nIn each paper, the proposed model significantly improves on the benchmarks. The accuracy of the\\nmodels is measured in terms of MAE, RMSE, and MAPE. Among the papers, there is a high diversity\\nin the choice of the proposed datasets. However, there are two sets of papers using the same dataset,\\nwhose results can be compared.\\nThe first dataset is the CCMP wind speed dataset (available at http://data.remss.com), which\\ncontains 3,942 geographical grid points from the east and southeast sea areas of China. The three\\npapers using it ([157], [49], [46]) select 120 points from the area and use data records from January\\n2010 to April 2019 at 6 hours intervals, with a total of 13,624 samples at each node, for 10 different\\nforecasting horizons. All papers present results for the complete set of 120 nodes and a subset of nodes.\\nHowever, since their results are similar, only the average results for all the 120 nodes are reported in\\nTab. 8 for the sake of simplicity.\\nTable 8: Comparison of the average accuracy of the different models on the CCMP wind speed dataset\\nfor time horizons from 6 hours to 7 days ahead, expressed in terms of MAE and RMSE. The smallest\\nerrors are underlined. Numbers from the papers.\\nModel Metrics 6 h 12 h 18 h 1 d 2 d 3 d 4 d 5 d 6 d 7 d\\nCGRU MAE 1.658 1.795 1.937 2.038 2.290 2.401 2.478 2.525 2.556 2.594\\nRMSE 2.100 2.277 2.461 2.594 2.911 3.042 3.141 3.198 3.245 3.285\\nDAGLN [157] MAE 1.068 1.219 1.346 1.443 1.716 1.852 1.935 1.984 2.019 2.067\\nRMSE 1.423 1.625 1.793 1.917 2.255 2.428 2.512 2.570 2.607 2.656\\nDASTGN [46] MAE 1.039 1.184 1.297 1.387 1.662 1.806 1.900 1.945 1.995 2.028\\nRMSE 1.384 1.572 1.721 1.829 2.170 2.338 2.449 2.498 2.554 2.600\\nDLinear MAE 1.145 1.306 1.432 1.528 1.798 1.937 2.022 2.080 2.124 2.155\\nRMSE 1.528 1.745 1.913 2.037 2.366 2.512 2.604 2.666 2.713 2.748\\nEMD-SVRCKH MAE 1.111 1.311 1.432 1.526 1.788 1.919 1.998 2.051 2.090 2.119\\nRMSE 1.489 1.755 1.909 2.027 2.344 2.495 2.587 2.650 2.695 2.730\\nGraph WaveNet MAE 1.101 1.265 1.386 1.487 1.762 1.901 1.951 1.992 2.089 2.124\\nRMSE 1.467 1.685 1.852 1.982 2.304 2.466 2.554 2.582 2.684 2.717\\nHistorical Inertia MAE 1.139 1.301 1.405 1.511 1.773 1.912 2.007 2.081 2.109 2.114\\nRMSE 1.494 1.704 1.843 1.977 2.304 2.467 2.571 2.637 2.678 2.687\\nMTGNN MAE 1.091 1.237 1.351 1.454 1.724 1.864 1.935 1.990 2.036 2.059\\nRMSE 1.454 1.650 1.801 1.936 2.267 2.436 2.515 2.579 2.632 2.657\\nMulti LSTMs MAE 1.312 1.645 1.914 2.129 2.610 2.748 2.793 2.818 2.842 2.867\\nRMSE 1.663 2.079 2.415 2.679 3.257 3.420 3.477 3.510 3.540 3.573\\nPDCNN MAE 2.025 2.026 2.385 2.551 2.959 3.011 3.012 2.971 2.918 2.877\\nRMSE 2.561 2.562 3.033 3.243 3.750 3.824 3.833 3.789 3.727 3.675\\n18'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 18}, page_content='SGNN MAE 2.542 2.562 2.573 2.578 2.583 2.618 2.666 2.695 2.712 2.716\\nRMSE 3.222 3.246 3.264 3.270 3.275 3.328 3.392 3.431 3.453 3.458\\nSTGN [49] MAE 1.084 1.281 1.407 1.523 1.809 1.942 2.046 2.073 2.137 2.189\\nRMSE 1.430 1.697 1.863 2.013 2.358 2.514 2.631 2.649 2.717 2.771\\nTPA-LSTM MAE 1.203 1.552 1.803 1.984 2.432 2.667 2.836 2.959 3.027 3.045\\nRMSE 1.562 1.997 2.319 2.544 3.082 3.383 3.598 3.745 3.825 3.842\\nWavelet-DBN-RF MAE 1.451 1.597 1.749 1.836 2.087 2.162 2.200 2.217 2.232 2.243\\nRMSE 1.855 2.038 2.226 2.328 2.617 2.703 2.744 2.766 2.788 2.798\\nFor all the time horizons, the most recent dynamic adaptive spatio-temporal graph neural network\\n(DASTGN) model [46] has the highest accuracy. Other two well performing models are the data-based\\nadaptive graph learning network (DAGLN) [157] and the multivariate time series forecasting with\\ngraph neural network (MTGNN) [201].\\nThe second dataset shared by two papers ([22], [39]) is the one of China National Urban Air Quality\\n(available at https://github.com/Friger/GAGNN). It contains air quality index data (AQI) of 209\\ncities, collected from January 2017 to April 2019 at 1 hour granularity, for a total of 20,370 samples.\\nBoth the papers also integrate exogenous weather variables (humidity, wind direction, wind speed,\\nrainfall, air pressure, temperature, and visibility) collected from Envicloud (http://www.envicloud.cn\\n/). The average accuracy of the models used in the two papers is compared in Tab. 9.\\nTable 9: Comparison of the average accuracy of the different models on the Chinese AQI dataset for\\ntime horizons from 1 to 6 hours ahead, expressed in terms of MAE and RMSE. The smallest errors\\nare underlined. Numbers from the papers.\\nModel Metrics 1h 2h 3h 4h 5h 6h\\nAirFormer MAE 5.95 9.31 11.87 13.87 15.52 16.95\\nRMSE 11.49 17.23 21.32 24.31 26.72 28.71\\nDeeperGCN MAE 6.54 9.74 11.77 13.40 15.29 16.41\\nRMSE 13.67 18.93 21.14 23.83 26.25 28.02\\nFGA MAE 5.87 9.14 11.71 13.75 15.42 16.80\\nRMSE 11.36 17.01 21.05 24.09 26.55 28.52\\nGAGNN [22] MAE 5.56 8.59 10.80 12.52 13.91 15.10\\nRMSE 10.81 16.17 19.84 22.51 24.63 26.37\\nGC-LSTM MAE 5.95 9.16 11.58 13.46 15.00 16.31\\nRMSE 11.91 16.98 20.82 23.69 25.97 27.82\\nGWNet MAE 5.76 9.64 12.79 15.30 17.28 18.81\\nRMSE 11.27 17.57 22.31 25.75 28.52 30.48\\nHGCN MAE 5.70 9.09 11.73 13.84 15.55 16.95\\nRMSE 11.18 17.09 21.33 24.52 26.99 28.94\\nHighAir MAE 5.50 8.52 10.81 12.50 14.00 15.09\\nRMSE 10.80 16.10 19.85 22.70 24.91 26.40\\nINNGNN [39] MAE 5.48 8.49 10.67 12.34 13.72 14.91\\nRMSE 10.70 16.03 19.66 22.29 24.37 26.11\\nLSTM MAE 6.50 10.26 13.18 15.52 17.40 18.91\\nRMSE 13.85 19.26 23.52 26.83 29.46 31.55\\nMegaCRN MAE 5.38 8.76 10.80 12.73 14.46 16.03\\nRMSE 10.64 16.46 19.92 22.82 25.45 27.60\\nSHARE MAE 5.84 9.07 11.49 13.35 14.74 15.79\\nRMSE 11.27 16.84 20.77 23.60 25.80 27.38\\n19'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 19}, page_content='ST-UNet MAE 5.95 9.30 11.58 13.38 14.82 16.02\\nRMSE 11.74 18.01 21.34 23.90 25.94 27.64\\nXGBoost MAE 6.85 10.89 13.99 16.27 18.14 19.56\\nRMSE 14.25 19.80 24.72 28.14 30.63 33.44\\nThe hybrid interpretable neural network and graph neural network model (INNGNN) [39] is the\\nmost accurate model for all time horizons except 1 hour, where it is beaten by the meta-graph convo-\\nlutional recurrent network model (MegaCRN) [74]. The authors attribute this result to the fact that\\nin the short term the AQI is strongly influenced by nearby cities, and the MegaCRN model is more\\nfocused on local features.\\n5.3 Finance\\n”Finance” is another emerging field of study, though not yet widely explored, with 9 out of 156\\nselected papers. The difficulty of predicting financial data is a well-established fact. This is due\\nto the inherently complex nature of markets, the influence of geopolitical events on them, and the\\nunpredictable behavior of humans which is often irrational and difficult to predict. This complexity\\nmakes financial time series highly volatile, and accurate forecasts and data modeling are essential to\\ndevelop effective trading strategies. GNNs are taken into consideration to model the inter-dependencies\\nbetween variables together with series dynamics.\\n5.3.1 Overview\\nThe ”Finance” group is relatively narrow, in part due to the limited number of published papers. The\\nmajority of studies are concentrated on stock price prediction ([69], [149], [176], [186], [174]), with\\nthe aim of forecasting the future trend of the price of a company. Sometimes the stock prediction\\nproblem is seen as a classification task. Indeed, due to the stochastic nature and volatility of financial\\nmarkets, predicting price movements (”up”, ”neutral”, ”down”) can be more feasible than accurately\\nforecasting specific prices. For example, assuming a daily granularity, a rise (”up”) can be considered\\nto occur when the next trading day’s closing price ( pt) is at least 0.55% higher than the previous\\nday’s closing price ( pt−1), whereas if pt is more than 0.50% lower than pt−1, the price movement can\\nbe classified as a ”down”. Otherwise, it is classified as ”neutral”. All the selected papers addressing\\nstock prices approach the problem from a classification perspective. Another distinctive aspect in this\\ngroup of papers is that researchers usually also evaluate how their predictions affect the stability of\\ninvestment returns over a specified period. This is commonly done through trading simulations in the\\ntest set, evaluated with metrics such as the Sharpe ratio and overall returns. These simulations require\\nmodeling rather than forecasting.\\nAn examination of the selected papers indicates an interest in the application of GNNs in Finance\\nfrom 2022. Two of the papers were published in IEEE Access by IEEE, while the remaining ones\\nappeared in various other journals. Most of the corresponding authors are affiliated with academic\\ninstitutions in China.\\n5.3.2 Datasets\\nThe number of datasets in this group is limited. The links to the public datasets referenced in the\\nselected papers are listed in Tab. 10.\\nTable 10: List of public datasets in the ”Finance” group and their corresponding links.\\nDataset Used by Link\\nChina A-Shares [186], [174], [28] https://finance.sina.com.cn/stock\\nCSI 100 index [176] https://tushare.pro/\\nCSI 300 index [69], [176] https://global.csmar.com/\\nS&P 100 index [176] https://finance.yahoo.com/\\n20'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 20}, page_content='S&P 500 index [69] https://finance.yahoo.com/\\nRussell 1K index [176] https://finance.yahoo.com/\\nRussell 3K index [176] https://finance.yahoo.com/\\nAll the listed datasets represent collections of stock indices. The most common datasets include the\\nChina Securities Index (CSI), the 100 and 500 Standard and Poor’s (S&P) indices, and the 100 and\\n300 China A-Shares indices. Many studies also include exogenous data, such as other market data,\\nevents, and news. As for the news, they can be processed by some language model like BERT (as in\\nRefs. [69] and [28]). Most of the data have a granularity of 1 day (which is typical of financial data),\\nand the forecasting horizon is also generally 1 day. Only one of the selected papers (Ref. [176]) analyses\\nalso longer time horizons for stock forecasting. As for pre-processing, half of the selected papers use\\nnormalization techniques, including min-max normalization, Z-score normalization, and logarithmic\\nnormalization.\\n5.3.3 Proposed models\\nIn terms of the taxonomy of the proposed GNN models, 5 out of 9 papers utilize attentional GNNs,\\nwhile the other 4 papers employ convolutional GNNs. As for the graph structure, some papers use\\nmodels which learn the graph structure themselves ([176], [111]), while some others defines it a priori\\nbased on the relationships between companies and stocks ([186], [72], [214]).\\nSince the majority of papers deal with a stock movement classification problem, the most common\\nloss function is the cross entropy. The majority of the papers explicitly indicate that Python is the\\nprogramming language utilized, with nearly half employing PyTorch and a few utilizing TensorFlow.\\nOnly Ref. [186] provides a link to the source code of the proposed knowledge graph and graph convo-\\nlution neural network (KG-GCN) model, available at https://github.com/Gjl12321/KG GCN Stock\\nPrice Trend Prediction System.\\n5.3.4 Benchmark models\\nTab. 11 displays the benchmark models utilized in the ”Finance” group by a minimum of two of the\\nselected papers.\\nTable 11: List of benchmark models in the ”Finance” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nMean reversion (MR) [186], [174]\\nClassical\\nmachine\\nlearning\\nmethods\\nConvolutional neural network (CNN) [69], [111]\\nDual-stage attention-based recurrent neural\\nnetwork (DARNN)\\n[186], [174]\\nFeed-forward neural network (FNN) [69], [214]\\nGated recurrent unit (GRU) [149], [214]\\nLong-short term memory network (LSTM) [69], [176], [186], [111], [174],\\n[72], [214]\\nSupport vector machine (SVM) [111], [214]\\nGNN methods Graph convolutional network (GCN) [69], [149], [176], [111], [214]\\nFinancial Graph Attention Network (Fin-\\nGAT) [66]\\n[149], [176]\\nHierarchical graph attention network (HATS)\\n[89]\\n[176], [186], [174]\\n21'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 21}, page_content='Spatiotemporal hypergraph convolution net-\\nwork (STHGCN) [159]\\n[186], [174]\\nIt is evident that there is no consensus regarding the most appropriate benchmark models, especially\\nin the context of GNNs. The most common benchmark is the LSTM, which is considered an effective\\napproach for processing long term information. The most commonly used GNN model is a vanilla\\ngraph convolutional network (GCN), followed by a more complex hierarchical graph attention network\\n(HATS) proposed by Raehyun et al. in 2019 [89]. The HATS model has been proposed for predicting\\nstock movements using relational data. It aggregates information from different types of relationships\\namong data and incorporates this information into each stock representation. The source code for the\\nHATS model is available at https://github.com/dmis-lab/hats.\\n5.3.5 Results\\nThe accuracy of the proposed classification models is primarily evaluated in terms of accuracy, precision\\nand F1 score. The only two papers with comparable results are Refs. [186] and [174], which study 758\\nfrequently traded stocks collected from the A-share market in China from January 2013 to December\\n2019 (available at https://finance.sina.com.cn/stock). These two papers compare the results obtained\\nby using 5, 10 and 20 trading days as records. For sake of simplicity, a comparison of the accuracy of\\nthe models in the two papers with 20 days only is provided in Tab. 12.\\nTable 12: Comparison of the average accuracy of the different models on the A-share market in China\\nwith 20 training days as records, expressed in terms of accuracy, precision, recall, and F1 score. The\\nbest results are underlined.\\nModel Accuracy Precision Recall F1 score\\nDARNN 38.41% 37.99% 39.24% 38.60%\\nGCN+LSTM 37.30% 39.28% 34.16% 36.54%\\nHATS 38.85% 38.70% 35.06% 36.78%\\nHGTAN 40.02% 41.77% 39.03% 40.32%\\nKG-GCN [186] 51.38% 65.72% 31.45% 39.27%\\nLSTM 35.03% 36.43% 34.23% 35.20%\\nMOM 35.73% 35.19% 32.82% 33.96%\\nMONEY [174] 39.90% 43.92% 40.61% 42.20%\\nMR 35.32% 38.03% 33.60% 35.68%\\nSFM 34.54% 26.93% 33.32% 29.49%\\nSTHGCN 38.45% 37.22% 32.82% 34.87%\\nTGC 37.81% 36.96% 34.49% 35.67%\\nThe two best models are the knowledge graph and graph convolution neural network (KG-GCN)\\n[186] and the stock price movement prediction via convolutional network with adversarial hypergraph\\nmodel (MONEY) [174], depending on the metrics considered.\\n5.4 Health\\nThe fourth group of papers is ”Health”, which includes 8 out of 156 publications. These papers focus on\\ncritical aspects of health monitoring, disease modeling, and diagnostic tools, with particular emphasis\\non the spread and diagnosis of disease. All the papers working with functional magnetic resonance\\nimaging (fMRI) data were excluded from the analysis, as the dataset was generated as image sequences\\nrather than time series. This key difference in data structure makes them less relevant to the spatio-\\ntemporal focus of this review.\\n22'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 22}, page_content='5.4.1 Overview\\nApplications of GNNs in the ”Health” group have been studied starting from 2021. The reason of this\\nspread may be attributed to two key factors. The first reason is the growing presence of sensors in\\nhospitals, which enable the collection of multivariate time series signals which can be used for the timely\\ndiagnosis of chronic diseases. The second reason is the spread of the COVID-19 pandemic, which has\\nmotivated researchers to investigate and forecast the spread of the virus. Epidemic forecasting is the\\nmost common topic among the selected papers. The prediction of epidemics presents two significant\\nchallenges. First, their seasonality can vary greatly over time, and they can manifest with different\\nintensities and durations. Second, occasional pandemics can disrupt established patterns even for\\nyears. These complexities make the forecasting task more difficult, and machine learning and GNN\\nmodels have been studied to address this issue. A common reference model for epidemic prediction is\\nthe SIR model [84], an acronym referring to individuals which stands for susceptible (S), infectious (I),\\nand recovered (R). The SIR model divides the population into individuals who are susceptible to the\\ndisease, those who are infected and can spread it, and those who have recovered and are now immune,\\nand uses differential equations to describe how the disease spreads. In general, some papers use it as\\nbenchmark (e.g., Ref. [220]).\\nAll the selected papers are published in different journals, but primarily in the field of computer\\nscience. The majority of the corresponding authors are affiliated with academic institutions based in\\nChina.\\n5.4.2 Datasets\\nTab. 13 provides a list of the public datasets utilized in the selected papers, along with links for\\naccessing them.\\nTable 13: List of public datasets in the ”Health” group and their corresponding links.\\nDataset Used by Link\\nBonn dataset [183] http://dx.doi.org/10.1103/PhysRevE.64.061\\n907 (source paper)\\nBrazilian COVID-19 data [142] https://github.com/lcaldeira/GrafoBrasilCo\\nvid\\nCOVID-19 data [220] https://www.2019ncov.chinacdc.cn/\\nCOVID-19 vaccinations [118] https://ourworldindata.org/us-states-vaccina\\ntions\\neICU [207] https://doi.org/10.1038/sdata.2018.178\\n(source paper)\\nMIMIC-IV [207] https://physionet.org/content/mimiciv/2.0/\\nNytimes COVID-19 data [118] https://github.com/nytimes/covid-19-data\\nParkinson’s Progres-\\nsion Markers Initiative\\n(PPMI)\\n[234] http://www.ppmi-info.org/\\nParkinson Speech data set\\n(PS)\\n[234] https://doi.org/10.1109/jbhi.2013.2245674\\n(source paper)\\nSpanish COVID-19 data [137] https://cnecovid.isciii.es/\\nSpikes and slow waves\\n(SSW) dataset\\n[183] http://dx.doi.org/10.1109/ICDM50108.2020.\\n00067 (source paper)\\nUS influenza [227] https://github.com/aI-area/DVGSN/tree/m\\nain/dataset\\nNone of the papers in this section use the same dataset, so it is not possible to compare the\\nresults of the proposed models. Most of the datasets are related to the COVID-19 effects and have a\\ndaily granularity. In addition, some papers ([220], [142], [234], [207]) include exogenous variables that\\ncan be either context-related (e.g., geography, population, health conditions, vaccinations, economic\\n23'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 23}, page_content='factors), or specific to a single patient (including personal information such as age, gender, and medical\\nconditions).\\n5.4.3 Proposed models\\nAs for the taxonomy of the proposed models, 5 out of 8 papers use a convolutional GNN architec-\\nture. Two papers use a pure attentional model, while another paper presents a hybrid convolutional-\\nattentional architecture. Different approaches are used to define the graph structure. Some papers\\nbase their graphs on the geographical distribution of the data (as in Refs. [220], [142] and [234]), and\\nothers use models that learn the graph structure directly, as in Refs. [227] and [118]. There is also a\\npaper that uses a variant of the visibility algorithm [183]. In almost all the cases, the graph is static.\\nIn terms of programming languages, this community uses only Python with PyTorch. As for loss\\nfunctions, MSE is typically used for prediction tasks, and cross entropy is used for classification tasks.\\nTab. 14 shows the links to the related source codes.\\nTable 14: List of source codes of the ”Health” models in the review.\\nModel Link\\nDynamic virtual graph significance networks\\n(DVGSN) [227]\\nhttps://github.com/aI-area/DVGSN\\nDeep learning of contagion dynamics on complex net-\\nworks [137]\\nhttps://doi.org/10.5281/zenodo.4974521\\nSparse spectra graph convolutional network (SSGC-\\nNet) [183]\\nhttps://github.com/anonymous2020-sou\\nrce-code/WNFG-SSGCNet-ADMM\\nSpatial-Temporal Graph Convolutional Network\\n(STGCN) [142]\\nhttps://github.com/lcaldeira/CovidPa\\nndemicForecasting\\nTime-aware Context-Gated Graph Attention Net-\\nwork (T-ContextGGAN) [207]\\nhttps://github.com/OwlCitizen/TConte\\nxt-GGAN\\n5.4.4 Benchmark models\\nTab. 15 lists the benchmark models in the ”Health” group that are discussed in at least two papers.\\nTable 15: List of benchmark models in the ”Health” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nAutoregressive integrated moving average\\n(ARIMA)\\n[227], [220], [142]\\nClassical\\nmachine\\nlearning\\nmethods\\nFeed-forward neural network (FNN) [183], [227]\\nLong-short term memory network (LSTM) [220], [118]\\nThere are no particularly relevant benchmark models, and none fall within the category of GNNs.\\nThis is because GNNs have not been widely studied in the context of health.\\n5.4.5 Results\\nIn each paper, the authors claim that the proposed GNN-based model outperforms the benchmarks.\\nThe most common error metrics are the MAE and the MSE for forecasting problems and accuracy,\\nsensitivity and specificity for classification problems. A comparison of the accuracy of the different\\nmodels is not possible due to the lack of common benchmark datasets.\\n24'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 24}, page_content='5.5 Mobility\\nThe topic that includes the largest number of papers identified by the query (39 out of 156) is ”Mobil-\\nity”, which regards the movement of people. It includes urban traffic, air travel, and bicycle demand,\\namong other applications. Given the large number of articles in this group, a more detailed comparison\\nof benchmarks, models, and results can be provided.\\n5.5.1 Overview\\nEarly traffic prediction is essential for improving the efficiency of transportation systems, helping\\ndrivers plan their trips more effectively, and preventing urban congestion. The advent of smart cities\\ninfrastructures and transportation systems has contributed to the collection of rich data from road\\nsensors that can be used for this purpose. However, making accurate traffic forecasts is challenging\\ndue to constantly changing traffic patterns over time and space, as well as the influence of external\\nfactors such as weather conditions and special events. Unlike classical statistical and machine learning\\nmodels, GNNs can be particularly effective for traffic forecasting due to their ability to model complex\\nrelationships in spatial and temporal domain. The increasing number of papers published each year in\\nthis group reflects both the growing research interest in this field and the growing confidence in GNN\\nmodels. The most commonly investigated areas regard urban traffic ([133], [185], [33], [108], [107],\\n[103], [128], [205], [127], [79], [106], [95], [94], [210], [59], [54], [231], [85], [146], [134], [241], [20], [2],\\n[13], [55], [36], [15], [77]), especially urban traffic flow and urban traffic speed. Among the journals\\nwhere the selected papers were published, Applied Intelligence by Springer is the most common, with\\n5 out of 39 papers. This is followed by ACM Transactions on Intelligent Systems and Technology\\nand ACM Transactions on Knowledge Discovery from Data , both published by the Association for\\nComputing Machinery, each with 4 out of 39 papers. Most of the corresponding authors are affiliated\\nwith Chinese institutions.\\n5.5.2 Datasets\\nTab. 16 provides a list of the public datasets mentioned in the selected papers, along with the links to\\naccess them.\\nTable 16: List of public datasets in the ”Mobility” group and their corresponding links.\\nDataset Used by Link\\nCAAC flights [14] http://www.caac.gov.cn/index.html\\nCD-HW [20] https://doi.org/10.1109/ACCESS.2020.3027\\n375 (source paper)\\nCity Bike NYC [203], [122] https://citibikenyc.com/system-data\\nDiDi Beijing [193] https://github.com/didi/TrafficIndex\\nDiDi Chengdu [133], [108] https://gaia.didichuxing.com/\\nHuai’an Bus Transaction [65] https://www.pandabus.cn/panda/panda-min\\nibus.html\\nLos-loop [128] https://github.com/lehaifeng/T-GCN/tree/\\nmaster/data\\nMETR-LA [133], [107], [103],\\n[106], [95], [94],\\n[59], [231], [85],\\n[20], [13]\\nhttps://paperswithcode.com/dataset/metr-la\\nMilan network [146] https://github.com/arunasubbiah/milan-tel\\necom-data-modeling\\nNE-BJ [103] https://github.com/tsinghua-fib-lab/Traffic\\n-Benchmark\\nNYC-Taxi [102] https://www1.nyc.gov/site/tlc/about/tlc-tri\\np-record-data.page\\n25'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 25}, page_content='PeMS03 [79], [94], [210],\\n[54], [241], [55]\\nhttps://www.kaggle.com/datasets/elmahy/p\\nems-dataset\\nPeMS04 (PeMSD4) [185], [33], [107],\\n[230], [79], [94],\\n[210], [54], [85],\\n[241], [2], [55], [36],\\n[15]\\nhttps://paperswithcode.com/dataset/pems04\\nPeMS07 (PeMSD7) [79], [95], [94], [54],\\n[241], [2], [55]\\nhttps://paperswithcode.com/dataset/pems07\\nPeMS08 (PeMSD8) [185], [33], [107],\\n[79], [210], [54],\\n[85], [241], [55],\\n[36], [15]\\nhttps://paperswithcode.com/dataset/pems08\\nPEMS-BAY [103], [106], [95],\\n[59], [231], [85]\\nhttps://paperswithcode.com/dataset/pems-b\\nay\\nQ-Traffic [134] https://github.com/JingqingZ/BaiduTraffic\\n#Dataset\\nRCOTP [185] https://developer.ibm.com/exchanges/data\\n/all/airline/\\nUVDS [13] https://doi.org/10.1007/978-3-030-73280-6 6\\n(source paper)\\nTaxiBJ [20] https://www.microsoft.com/en-us/research\\n/publication/forecasting-citywide-crowd-flo\\nws-based-big-data/\\nTaxi Shenzhen [128], [77] https://github.com/lehaifeng/T-GCN/tree/\\nmaster/data\\nTLC Taxi [124] https://www.nyc.gov/site/tlc/about/tlc-tri\\np-record-data.page\\nThe most popular datasets are PeMS ones (http://pems.dot.ca.gov/), provided by the California\\nTransportation Agency Performance Measurement System, which include various versions differing in\\ntime range, spatial coverage, and the number of sensors, and METR-LA, collected from loop detectors\\nin Los Angeles County highway. The number of sensors is in the hundreds, and so is the number of\\nnodes in the graph. For a more detailed overview of the benchmark datasets in the traffic domain, see\\nRef. [75]. Other common public traffic datasets can be found at https://paperswithcode.com/task/\\ntraffic-prediction. In terms of external data, the most commonly used are weather conditions, time\\ninformation (time of day and day of week) and events (e.g., holidays), as well as information about\\nthe road network (e.g., points of interest).\\nThe majority of the papers utilize data with a 5-minute granularity, and provide forecasts for\\nhorizons ranging from 5 minutes to 1 hour ahead. In addition to imputation of missing values, in\\nalmost half of the cases the pre-processing of the data includes data normalization (min-max or Z-\\nscore normalization) and the selection of time windows regarding recent, daily and weekly data (as in\\n8 out of 39 papers).\\n5.5.3 Proposed models\\nAs for the proposed models, 18 out of 39 belong to the pure convolutional GNN paradigm, followed by\\n11 attentional GNNs, 9 hybrid convolutional-attention architectures, and 1 recurrent GNN. Regarding\\nthe definition of the graph structure, most of the papers discuss it in detail. Half of the papers\\nare based on a pre-defined adjacency matrix, and among the remaining papers that propose models\\nthat learn the adjacency matrix on their own, a further half is based on a pre-initialization of the\\nmatrix, as for example in Ref. [79]. Here, more than in any other group, there are multiple ways\\nto define the adjacency matrix, taking into account either the structure of the road network, the\\nactual flow of people, or a combination of different information. More in detail, some models use the\\n26'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 26}, page_content='road connectivity ([230], [231], [146], [77]), others their spatial distance ([133], [102], [107], [95], [13],\\n[15]), and some others incorporate extra information about the so-called points of interest (such as\\nRefs. [203], [102]). Other models instead consider some similarity measures between the time series\\n([33], [54], [85], [146]), such as the correlation coefficient or cosine similarity. Still others directly use\\nthe number of people traveling on the road at a given time step ([203], [193], [14], [38]).\\nThe two most common loss functions used to train the models are the MSE and the MAE. Few\\npapers consider a combination of loss functions, especially when dealing with a model that has to learn\\nthe graph structure by itself (Refs. [185], [20], [55]). Not all papers specify the language or library\\nused for the code. Among those that do, Python is the most common language, with the majority\\nusing PyTorch, while only a few researchers use TensorFlow. In addition, a few papers provide a link\\nto the source code for the proposed model. The papers that provide such links are listed in Tab. 17.\\nTable 17: List of source codes of the ”Mobility” models in the review.\\nModel Link\\n3-Dimensional Graph Convolution Network\\n(3DGCN) [203]\\nhttps://github.com/FIBLAB/3D-DGC\\nN\\nAdaptive generalized PageRank graph neural net-\\nwork (AGP-GNN) [59]\\nhttps://github.com/guoxiaoyuatbjtu/a\\ngp-gnn\\nAutomated dilated spatio-temporal synchronous\\ngraph network (Auto-DSTSGN) [79]\\nhttps://github.com/jinguangyi-uto-DST\\nSGN\\nDynamic multi-view graph neural network for city-\\nwide traffic inference (CTVI+) [33]\\nhttps://github.com/dsj96/TKDD\\nDynamic Graph Convolutional Recurrent Network\\n(DGCRN) [103]\\nhttps://github.com/tsinghua-fib-lab/T\\nraffic-Benchmark\\nSpatio-temporal Causal Graph Attention Network\\n(STCGAT) [241]\\nhttps://github.com/zsqZZU/STCGAT\\n/tree/v1.0.0\\nSpatial-Temporal Graph Convolutional Neural Net-\\nwork with LSTM layers (STGCN-LSTM) [2]\\nhttps://github.com/ant-agafonov/stgc\\nn-lstm\\nSpatio-Temporal Graph Mixformer (STGM) [95] https://github.com/mouradost/stgm\\n5.5.4 Benchmark models\\nTab. 18 lists the benchmark models used in the papers in the ”Mobility” group. The benchmarks are\\ncategorized into mathematical and statistical methods, classical machine learning models, and GNN\\nmodels, and only those appearing in more than two papers are included.\\nTable 18: List of benchmark models in the ”Mobility” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nAutoregressive integrated moving average\\n(ARIMA)\\n[133], [203], [185], [102], [108],\\n[103], [65], [128], [127], [184],\\n[106], [59], [146], [20], [14],\\n[15], [77]\\nHistorical average (HA) [133], [203], [102], [108], [103],\\n[65], [193], [128], [106], [95],\\n[231], [146], [241], [36], [15],\\n[77]\\nVector autoregression (VAR) [107], [103], [20], [36], [15]\\nClassical\\nmachine\\nlearning\\nmethods\\nFeed-forward neural network (FNN) [133], [33], [103], [205], [122],\\n[231], [238], [134], [20]\\nGated recurrent unit (GRU) [108], [107], [65], [127], [238],\\n[146], [134], [2], [36], [15], [77]\\n27'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 27}, page_content='Long-short term memory network (LSTM) [133], [102], [108], [107], [103],\\n[127], [184], [79], [106], [94],\\n[210], [59], [231], [238], [85],\\n[241], [20], [2], [55], [36], [15]\\nLong-short term time series network (LST-\\nNet) [97]\\n[193], [95]\\nSeq2Seq [175] [205], [134]\\nSpatial and temporal normalization for multi-\\nvariate time series forecasting (ST-Norm) [35]\\n[59], [54]\\nSupport vector regression (SVR) [107], [103], [128], [184], [106],\\n[124], [146], [14], [55], [77]\\nExtreme gradient boosting (XGBoost) [23] [33], [122]\\nGNN methods Adaptive graph convolutional recurrent net-\\nwork (AGCRN) [4]\\n[185], [108], [103], [65], [95],\\n[54], [134], [241], [55], [36]\\nAttention-based spatial-temporal graph con-\\nvolutional network (ASTGCN) [57]\\n[133], [185], [108], [107], [103],\\n[184], [79], [95], [94], [210],\\n[54], [238], [241], [20], [2], [55],\\n[36], [15]\\nDual-stage attention-based recurrent neural\\nnetwork (DA-RNN) [156]\\n[102], [205]\\nDiffusion convolutional recurrent neural net-\\nwork (DCRNN) [119]\\n[133], [203], [185], [102], [103],\\n[65], [128], [184], [79], [106],\\n[95], [94], [210], [59], [54],\\n[231], [85], [134], [241], [20],\\n[13], [14], [55], [36], [15]\\nDynamic spatial-temporal aware graph neural\\nnetwork (DSTAGNN) [98]\\n[185], [241]\\nGeoMAN [123] [102], [15]\\nGraph convolutional network (GCN) [230], [77]\\nGraph multi-attention network (GMAN) [242] [103], [106], [95], [59], [85],\\n[13]\\nGraph WaveNet (GWN) [202] [133], [185], [103], [65], [79],\\n[106], [95], [94], [210], [59],\\n[54], [85], [134], [13]\\nMulti-range attentive bicomponent graph con-\\nvolutional network (MRA-BGCN) [24]\\n[106], [59]\\nMultivariate time series forecasting with\\ngraph neural network (MTGNN) [201]\\n[103], [106], [95], [94], [59],\\n[54], [85], [134], [13]\\nMulti-view graph convolutional network\\n(MVGCN) [172]\\n[203], [102]\\nSpatial-temporal fusion graph neural network\\n(STFGNN) [114]\\n[108], [65], [79], [210], [54],\\n[241], [55]\\nSpatio-temporal graph convolutional network\\n(STGCN) [216]\\n[133], [203], [185], [102], [107],\\n[103], [65], [127], [184], [79],\\n[106], [95], [94], [210], [59],\\n[54], [231], [238], [85], [134],\\n[20], [2], [13], [14], [55], [36],\\n[15]\\nSpatial temporal graph neural network\\n(STGNN) [188]\\n[203], [128]\\nSpatial-temporal graph ODE networks\\n(STGODE) [43]\\n[107], [79], [54], [241]\\n28'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 28}, page_content='Deep-meta-learning based model (ST-\\nMetaNet) [148]\\n[102], [103], [106], [59]\\nSpatio-temporal synchronous graph convolu-\\ntional network (STSGCN) [169]\\n[108], [107], [103], [79], [210],\\n[54], [241], [55], [36]\\nTemporal graph convolutional network (T-\\nGCN) [239]\\n[128], [184], [94], [238], [146]\\nAs for the basic mathematical and statistical methods, two widely used benchmarks are the ARIMA\\nmodel and historical average model (HA), which uses the average of past observations as the forecast\\nfor future values. Despite their apparent simplicity, these models are sometimes shown to be very\\npowerful benchmarks, with an accuracy comparable to that of more complex models. As for the\\ntraditional machine learning models, LSTMs are widely recognized as a type of recurrent neural network\\ncapable of handling and long-term dependencies in traffic data. As for GNN-based models instead,\\nthe three most used models are, in order, the spatio-temporal graph convolutional network (STGCN)\\n[216], the diffusion convolutional recurrent neural network (DCRNN) [119], and the attention-based\\nspatial-temporal graph convolutional network (ASTGCN) [57], all developed specifically for the traffic\\nforecasting problem.\\nThe STGCN model was proposed by Yu et al. in 2018, and it integrates graph convolution and gated\\ntemporal convolution through spatio-temporal convolutional blocks. The source code is available at\\nhttps://github.com/VeritasYin/STGCN IJCAI-18. The DCRNN model was proposed by Li et al. in\\n2017, and it aims to capture the spatial dependency using bidirectional random walks on the graph, and\\nthe temporal dependency using the encoder-decoder architecture with scheduled sampling. The code\\ncan be accessed at https://github.com/liyaguang/DCRNN. Finally, the ASTGCN model was proposed\\nby Guo et al. in 2019, and it models in parallel recent, daily-periodic and weekly-periodic dependencies\\nas three independent components with a spatio-temporal attention mechanism to capture the dynamic\\nspatio-temporal correlations, and a spatio-temporal convolution which simultaneously employs graph\\nconvolutions to capture the spatial patterns. The code is available at https://github.com/Davidham3\\n/ASTGCN-2019-mxnet. For a more comprehensive list of source codes for the most dated benchmark\\nGNN models in the traffic domain see Ref. [76].\\n5.5.5 Results\\nIn all papers, the authors claim that the proposed GNN-based models are able to outperform the other\\nbenchmarks. However, there are a limited number of situations in which some simple statistical and\\nmathematical models demonstrate comparable accuracy, as it can be observed in Tabs. 19 and 20.\\nFor the and comparison and evaluation of the models, the most commonly used error metrics are\\nMAE, RMSE, and MAPE. Few papers also use different metrics, such as the coefficient of determi-\\nnation R2. However, despite the use of the same metrics and the availability of widely used datasets,\\ncomparing models is not trivial because different papers often focus on different time windows of a\\ndataset.\\nIn Tab. 19 it is shown the accuracy of the different models on the METR-LA dataset with a 5-\\nminute granularity from 1 st March 2012 to 30 th June 2012 for 15, 30 and 60 minute time horizons,\\nexpressed in terms of MAE, MAPE and RMSE.\\nTable 19: Comparison of the average accuracy of the different models on the METR-LA dataset for\\n15, 30 and 60 minute time horizons, expressed in terms of MAE, MAPE and RMSE. The results of\\nthe models marked with ⋆ are taken from papers in the ”Generic” group. The smallest errors are\\nunderlined. Numbers from the papers.\\nTime horizon 15 min 30 min 60 min\\nMetrics MAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSE\\nAGCRN 2.87 7.70 5.58 3.23 9.00 6.58 3.62 10.38 7.51\\nAGP-GNN [59] 2.71 7.21 5.25 3.04 8.15 6.20 3.41 9.70 7.20\\nARIMA 3.99 9.60 8.12 5.15 12.70 10.45 6.90 17.40 13.23\\n29'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 29}, page_content='ASTGCN 4.86 9.21 9.27 5.43 10.13 10.61 6.51 11.64 12.52\\nDCRNN 2.77 7.30 5.38 3.15 8.80 6.45 3.60 10.50 7.60\\nDGCRN [103] 2.62 6.63 5.01 2.99 8.02 6.05 3.44 9.73 7.19\\nDST-GCNN [20] 2.68 7.20 5.35 3.01 8.50 6.23 3.41 10.30 7.47\\nDyGCN-LSTM [94] 2.64 6.35 4.64 3.24 8.04 5.24 3.53 9.30 6.53\\nFC-GAGA 2.75 7.25 5.34 3.10 8.57 6.30 3.51 10.14 7.31\\nFNN 3.99 9.90 7.94 4.23 12.90 8.17 4.49 14.00 8.69\\nGA-LSTM [231] 2.30 7.01 4.97 2.98 8.50 6.00 3.55 9.97 7.23\\nGGRU 2.71 6.99 5.24 3.12 8.56 6.36 3.64 10.62 7.65\\nGMAN 2.77 7.25 5.48 3.07 8.35 6.34 3.40 9.72 7.22\\nGWN 2.69 6.90 5.15 3.07 8.37 6.22 3.53 10.01 7.37\\nHA 4.16 13.00 7.80 4.16 13.00 7.80 4.16 13.00 7.80\\nLSTM 3.44 9.60 6.30 3.77 10.90 7.23 4.37 13.20 8.69\\nMCFGNN⋆ [27] 2.31 5.86 5.29 2.66 6.89 6.27 3.05 7.65 7.20\\nMRA-BGCN 2.67 6.80 5.12 3.06 8.30 6.17 3.49 10.00 7.30\\nMS-GAT 2.65 6.43 2.99 4.83 8.31 5.91 6.48 9.48 8.11\\nMTGNN 2.69 6.86 5.18 3.05 8.19 6.17 3.49 9.87 7.23\\nMTGODE⋆ [82] 2.66 6.87 5.10 3.00 8.19 6.05 3.39 9.80 7.05\\nMVST-GNN [106] 2.70 5.19 6.97 3.06 8.12 6.08 3.40 9.60 6.98\\nSDGCN 2.76 7.16 5.38 3.15 8.53 6.46 3.61 9.87 7.43\\nSTAG-GCN [133] 2.67 7.00 5.23 3.07 8.26 6.15 3.50 9.93 7.24\\nSTFGNN⋆ 2.57 6.51 4.73 2.83 7.46 5.46 3.18 8.81 6.40\\nSTGCN 2.88 7.62 5.74 3.47 9.57 7.24 4.59 12.70 9.40\\nSTGODE⋆ 3.47 8.76 6.76 4.36 11.14 8.47 5.50 14.32 10.33\\nSTG-NCDE⋆ 3.77 8.54 9.47 4.84 10.63 12.04 6.35 13.49 14.94\\nSTGRAT 2.60 6.61 5.07 3.01 8.15 6.21 3.49 10.01 7.42\\nST-MetaNet 2.69 6.91 5.17 3.10 8.57 6.28 3.59 10.63 7.52\\nSTSGCN 3.31 8.06 7.62 4.13 10.29 9.77 5.06 12.91 11.66\\nSVR 3.99 9.30 8.45 5.05 12.10 10.87 6.72 16.70 13.76\\nTF-GAN [85] 2.63 6.55 4.94 3.06 8.36 6.20 3.32 9.48 7.11\\nT-GCN 3.03 7.81 5.26 3.52 9.45 6.12 4.30 11.80 7.31\\nVAR 4.42 10.20 7.89 5.41 12.70 9.13 6.52 15.80 10.11\\nWaveNet 2.99 8.04 5.89 3.59 10.25 7.28 4.45 13.62 8.93\\nThe results on the METR-LA dataset are not uniform, and the best performing models vary\\ndepending on the metric and the time horizon. However, especially for longer time horizons, the\\nDynamic Graph Convolution LSTM Network (DyGCN-LSTM) [94] appears to be the most accurate\\none (excluding the ”Generic” ⋆ models, among which the best model is the multichannel fusion graph\\nneural network (MCFGNN) [27]).\\nTab. 20 displays the accuracy of various models on the PEMS-BAY dataset with a 5-minute gran-\\nularity and data spanning from 1 st January 2017 to 30 th June 2017 for 15, 30 and 60 minute time\\nhorizons, with results expressed in terms of MAE, MAPE and RMSE.\\nTable 20: Comparison of the average accuracy of the different models on the PEMS-BAY dataset for\\n15, 30 and 60 minute time horizons, expressed in terms of MAE, MAPE and RMSE. The smallest\\nerrors are underlined. Numbers from the papers.\\nTime horizon 15 min 30 min 60 min\\nMetrics MAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSE\\n30'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 30}, page_content='AGCRN 1.37 2.94 2.87 1.69 3.87 3.85 1.96 4.64 4.54\\nAGP-GNN [59] 1.33 2.79 2.86 1.61 3.66 3.67 1.86 4.30 4.34\\nARIMA 1.62 3.50 3.30 2.33 5.40 4.76 3.38 8.30 6.50\\nASTGCN 1.52 3.22 3.13 2.01 4.48 4.27 2.61 6.00 5.42\\nDCRNN 1.38 2.90 2.95 1.74 3.9 3.97 2.07 4.90 4.74\\nDGCRN [103] 1.28 2.66 2.69 1.59 3.55 3.63 1.89 4.43 4.42\\nFC-GAGA 1.36 2.87 2.86 1.68 3.80 3.80 1.97 4.67 4.52\\nFNN 2.20 5.19 4.42 2.30 5.43 4.63 2.46 5.89 4.98\\nGMAN 1.34 2.81 2.82 1.62 3.62 3.72 1.86 4.31 4.32\\nGWN 1.30 2.73 2.74 1.63 3.67 3.70 1.95 4.63 4.52\\nHA 2.88 6.80 5.59 2.88 6.80 5.59 2.88 6.80 5.59\\nLSTM 2.05 4.80 4.19 2.20 5.20 4.55 2.37 5.70 4.96\\nMRA-BGCN 1.29 2.90 2.72 1.61 3.80 3.67 1.91 4.60 4.46\\nMTGNN 1.32 2.77 2.79 1.65 3.69 3.74 1.94 4.53 4.49\\nMTGODE⋆ [82] 1.29 2.72 2.73 1.61 3.61 3.66 1.88 4.39 4.31\\nMVST-GNN [106] 1.30 2.73 2.74 1.57 3.56 3.54 1.80 4.19 4.26\\nSDGCN 1.35 2.89 2.88 1.69 3.87 3.91 1.99 4.69 4.59\\nSTGCN 1.36 2.90 2.96 1.81 4.17 4.27 2.49 5.79 5.69\\nSTGODE⋆ 1.43 2.99 2.88 1.84 3.84 3.90 2.30 4.61 4.89\\nSTG-NCDE⋆ 1.38 2.91 2.93 1.71 3.91 3.84 2.03 4.82 4.58\\nSTGRAT 1.29 2.67 2.71 1.61 3.63 3.69 1.95 4.64 4.54\\nSTID 1.30 2.73 2.81 1.62 3.68 3.72 1.89 4.47 4.4\\nST-MetaNet 1.36 2.82 2.90 1.76 4.00 4.02 2.20 5.45 5.06\\nST-Norm 1.34 2.82 2.88 1.67 3.75 3.83 1.96 4.62 4.52\\nSTSGCN 1.44 3.04 3.01 1.83 4.17 4.18 2.26 5.40 5.21\\nSVR 1.85 3.80 3.59 2.48 5.50 5.18 3.28 8.00 7.08\\nVAR 1.74 3.60 3.16 2.32 5.00 4.25 2.93 6.50 5.44\\nWaveNet 1.39 2.91 3.01 1.83 4.16 4.21 2.35 5.87 5.43\\nThe results on the the PEMS-BAY dataset indicate that the dynamic graph convolutional recurrent\\nnetwork (DGCRN) [103] is the most accurate model for the 15-minute forecasting horizon, while the\\nmulti-view spatial-temporal graph neural network (MVST-GNN) [106] is the best model for the longer\\nhorizons.\\nAs for the other PeMS datasets (PeMS03, PeMS04, PeMS07 and PeMS08), the results are largely\\nconsistent with an indication that the automated dilated spatio-temporal synchronous graph network\\n(Auto-DSTSGN) [79] is the most accurate model (excluding the ”Generic” ⋆ models), as shown in\\nTabs. 21, 22, 23, and 24. Tab. 21 reports the accuracy of the different models applied to the PeMS03\\ndataset, which includes data from 1st September 2018 to 30th November 2018 at a 5-minute granularity\\nfor a 60-minute time horizon, with results given in terms of MAE, MAPE and RMSE.\\nTable 21: Comparison of the average accuracy of the different models on the PeMS03 dataset for a\\n60-minute time horizon, expressed in terms of MAE, MAPE and RMSE. The results of the models\\nmarked with ⋆ are taken from papers in the ”Generic” group. The smallest errors are underlined.\\nNumbers from the papers.\\nTime horizon 60 min\\nMetrics MAE MAPE RMSE\\nAGCRN 17.69 19.40 29.66\\nASTGCN 17.69 19.40 29.66\\n31'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 31}, page_content='Auto-DSTSGN [79] 14.59 14.22 25.17\\nAutoSTG 16.27 16.10 27.63\\nDCGCN [54] 15.29 - 25.98\\nDCRNN 18.18 18.91 30.31\\nGraph WaveNet 19.85 19.31 32.94\\nLSGCN 17.94 - 29.85\\nLSTM 21.33 23.33 35.11\\nSTAGCN [55] 15.40 14.48 26.23\\nSTCGNN [210] 15.81 14.59 27.23\\nSTFGCN 16.77 16.30 28.34\\nSTGCN 17.49 17.15 30.12\\nSTGODE 16.53 16.68 27.79\\nSTSGCN 17.48 16.78 29.21\\nSVR 21.97 21.51 35.29\\nIn Tab. 22 is shown the accuracy of the different models on the PeMS04 dataset with a 5-minute\\ngranularity and data from 1st January 2018 to 28th February 2018 a 60-minute time horizon, expressed\\nin terms of MAE, MAPE and RMSE.\\nTable 22: Comparison of the average accuracy of the different models on the PeMS04 dataset for a\\n60-minute time horizon, expressed in terms of MAE, MAPE and RMSE. The results of the models\\nmarked with ⋆ are taken from papers in the ”Generic” group. The smallest errors are underlined.\\nNumbers from the papers.\\nTime horizon 60 min\\nMetrics MAE MAPE RMSE\\nAGCRN 19.83 12.97 32.26\\nARIMA⋆ 33.73 24.18 48.80\\nASTGCN 22.93 16.56 35.22\\nAuto-DSTSGN [79] 18.85 13.21 30.48\\nAutoformer⋆ 23.76 18.01 36.59\\nAutoSTG 20.38 14.12 32.51\\nCrossformer⋆ 20.40 14.62 32.79\\nDCGCN [54] 20.28 - 31.65\\nDCRNN⋆ 19.71 13.54 31.43\\nDGCRN⋆ 19.04 12.80 30.82\\nDMGF-Net [107] 20.59 13.63 32.43\\nDSA-NET⋆ 22.79 16.03 35.77\\nDSTGN⋆ [120] 18.61 12.31 30.79\\nDSTIGNN⋆ [48] 18.41 12.45 29.97\\nFEDFormer⋆ 22.86 16.04 35.07\\nGMAN⋆ 20.93 14.06 33.34\\nGODERN-FS⋆ [225] 19.17 12.74 30.96\\nGRU 23.68 16.44 39.27\\nGWN⋆ 19.36 13.31 31.72\\nHA⋆ 38.03 27.88 59.24\\nLSGCN 21.53 13.18 33.86\\nLSTM⋆ 23.81 18.12 36.62\\n32'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 32}, page_content='MTGNN⋆ 24.89 17.29 39.66\\nMTGODE⋆ 19.55 13.08 32.99\\nSCINet⋆ 19.29 11.89 31.28\\nSDGL⋆ [121] 18.65 12.38 31.30\\nSTAGCN [55] 19.02 12.46 30.75\\nSTCGNN [210] 19.39 12.71 31.17\\nSTFGCN 19.83 13.02 31.88\\nSTG2SEQ 25.20 18.77 38.48\\nSTGODE⋆ 20.84 13.77 32.82\\nSTIDGCN⋆ 18.47 12.42 29.90\\nSTGCN⋆ 21.16 13.83 34.89\\nSTSGCN⋆ 21.08 13.88 33.83\\nSTGODE 20.84 13.76 32.84\\nSTSGCN 21.19 13.90 33.65\\nSVR 28.70 19.20 44.56\\nTCN⋆ 23.22 15.59 37.26\\nVAR⋆ 23.51 17.85 36.39\\nThe MAE, MAPE and RMSE of the different models on the PeMS07 dataset, with data from 1 st\\nMay 2017 to 31 st August 2017, a 5-minute granularity and a 60-minute time horizon, are presented in\\nTab. 23.\\nTable 23: Comparison of the average accuracy of the different models on the PeMS07 dataset for\\na 60-minute time horizon, expressed in terms of MAE, MAPE and RMSE. The smallest errors are\\nunderlined. Numbers from the papers.\\nTime horizon 60 min\\nMetrics MAE MAPE RMSE\\nAGCRN 22.37 9.12 36.55\\nASTGCN 28.05 13.92 42.57\\nAuto-DSTSGN [79] 20.08 8.57 33.02\\nAutoSTG 23.22 9.95 36.47\\nDCGCN [54] 22.06 - 34.66\\nDCRNN 25.30 11.66 38.58\\nGWN 26.85 12.12 42.78\\nLSGCN 27.31 - 41.46\\nLSTM 29.98 13.20 45.94\\nSTAGCN [55] 21.10 8.92 34.10\\nSTFGCN 22.07 9.21 35.80\\nSTGCN 25.38 11.08 38.78\\nSTGODE 22.59 - 37.54\\nSTSGCN 24.26 10.21 39.03\\nSVR 32.49 14.26 50.22\\nFinally, Tab. 24 displays the accuracy of the various models on the PeMS08 dataset with a 5-minute\\ngranularity and data spanning from 1 st July 2016 to 31 st August 2016 for a 60-minute time horizon,\\nexpressed in terms of MAE, MAPE and RMSE.\\n33'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 33}, page_content='Table 24: Comparison of the average accuracy of the different models on the PeMS08 dataset for a\\n60-minute time horizon, expressed in terms of MAE, MAPE and RMSE. The results of the models\\nmarked with ⋆ are taken from papers in the ”Generic” group. The smallest errors are underlined.\\nNumbers from the papers.\\nTime horizon 60 min\\nMetrics MAE MAPE RMSE\\nAGCRN 15.95 10.09 25.22\\nARIMA⋆ 31.09 22.73 44.32\\nASTGCN 18.61 13.08 28.16\\nAuto-DSTSGN [79] 14.74 9.45 23.76\\nAutoformer⋆ 21.04 15.98 31.33\\nAutoSTG 16.37 10.36 25.46\\nCrossformer⋆ 16.25 11.14 26.14\\nDCGCN [54] 15.68 - 24.39\\nDCRNN⋆ 15.26 9.96 24.28\\nDMGF-Net [107] 16.48 10.56 25.73\\nDSA-NET⋆ 17.14 11.32 26.96\\nDSTIGNN⋆ [48] 13.88 9.04 23.64\\nFEDformer⋆ 19.55 13.58 29.30\\nGMAN⋆ 16.97 11.32 26.70\\nGODERN-FS⋆ [225] 15.59 9.95 24.79\\nGRU 22.00 13.33 36.24\\nGWN⋆ 15.07 9.51 23.85\\nHA⋆ 34.86 24.07 52.04\\nLSGCN 17.73 11.20 26.76\\nLSTM 22.20 14.20 34.06\\nMTGNN⋆ 18.28 12.15 30.05\\nMTGODE⋆ 15.61 10.15 25.96\\nSCINet⋆ 15.78 9.97 24.60\\nSDGL⋆ [121] 14.93 9.61 24.13\\nSTAGCN [55] 15.36 9.80 24.32\\nSTCGNN [210] 15.55 10.02 24.61\\nSTFGCN 16.64 10.60 26.22\\nSTIDGCN⋆ 14.10 9.15 23.72\\nSTG2SEQ 20.17 17.32 30.71\\nSTGCN⋆ 17.50 11.29 27.09\\nSTGODE 16.79 10.58 26.01\\nSTSGCN 17.13 10.96 26.80\\nSVR 23.25 14.64 36.16\\nTCN⋆ 22.72 14.03 35.79\\nVAR⋆ 22.07 14.04 31.02\\n5.6 Predictive monitoring\\nIn modern industry, sensor-based monitoring of processes has become essential. This pairs with the\\nrapid growth of the Internet of Things (IoT), which aims at a smarter management across various ap-\\nplications [109]. In this framework, multi-sensor systems can be used to detect anomalies in complex\\nscenarios and to monitor the overall status of a system. Predictive monitoring is a task that involves\\nthe continuous observation and analysis of a system, in order to forecast its future states and verify\\n34'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 34}, page_content='whether the predicted outcomes meet certain standards. This predictive monitoring task is important\\nbecause it significantly improves efficiency, reliability, and cost of industrial operations. Related ap-\\nproaches include several techniques, such as anomaly detection, fault diagnosis and remaining useful\\nlife estimation.\\nDespite the specificity of the field, 16 out of 156 papers address the topic of predictive monitoring.\\nThis includes anomaly detection ([109], [112]), fault diagnosis ([177], [221], [100], [226], [141], [233], [99])\\nand RUL estimation ([192], [25], [182], [138], [212], [93], [196]). The use of GNNs in these analyses is\\njustified by the necessity of using a model capable of capturing the spatio-temporal correlations among\\nthe many sensors present in the system.\\n5.6.1 Overview\\nPredictive monitoring includes several techniques, such as anomaly detection, fault diagnosis and\\nremaining useful life estimation. Anomaly detection is a technique that aims to identify abnormal data\\nthat are not due to random deviations, and are rather generated by a different underlying mechanism\\n[62]. It is a prerequisite part of fault diagnosis [78]. Fault diagnosis is the process of determining\\nwhether a fault has occurred in a system, including the identification of the time, location, type, and\\nseverity of the fault. It is usually considered to be a classification problem. As for remaining useful life\\n(RUL) estimation, it aims to determine how long a machine will operate before it needs to be repaired\\nor replaced, and it is useful for scheduling maintenance interventions.\\nThe selected papers implement these three techniques, with anomaly detection being less prevalent\\nthan the other two. The main applications studied are industrial (e.g., bearings fault diagnosis [221],\\n[100], [141], [138], [212]) and mechanical (e.g, engines RUL estimation [192], [25], [182], [93]). There are\\nalso two papers dealing with energy related tasks, specifically fault diagnosis of photovoltaic systems\\n[177] and energy networks [226]. They are included in this section due to their specific focus on fault\\nclassification rather than on energy-related time series forecasting. It shoud also be underlined that the\\nmajority of the papers aim at solving a classification task, which is typical of anomaly detection and\\nfault diagnosis. As for RUL estimation, the quantity to be predicted is the RUL itself. For this reason,\\nwe do not refer to forecasting time horizons as in the other sections, since the goal is to estimate the\\nRUL rather than to forecast data over future time intervals.\\nThe majority of the collected papers were published in 2023 by authors affiliated with Chinese\\nuniversities. The most popular journal for these topics is IEEE Transactions on Instrumentation and\\nMeasurement by the Institute of Electrical and Electronics Engineers by IEEE, with 5 out of 15 papers.\\nAll other journals contain only one of the selected articles. It can be observed that the majority of the\\narticles are published in IEEE journals.\\n5.6.2 Datasets\\nTab. 25 contains a list of the public datasets used in the selected papers, with the links to access them.\\nTable 25: List of public datasets in the ”Predictive monitoring” group and their corresponding links.\\nDataset Used by Link\\nAir Quality China [109] https://www.aqistudy.cn/historydata/\\nAir Quality - TPS [109] https://www.kaggle.com/amritpal333/tps-jul\\ny-2021-original-dataset-clean\\nC-MAPSS [192], [25], [182],\\n[93]\\nhttps://ntrs.nasa.gov/api/citations/20070034\\n949/downloads/20070034949.pdf\\nCWRU bearing dataset [100], [99] http://csegroups.case.edu/bearingdatacenter\\n/home\\nJNU bearing dataset [100] http://mad-net.org:8765/explore.html?id=9\\n&t=0.9355271549540183\\nNASA battery dataset [196] http://ti.arc.nasa.gov/project/prognostic-dat\\na-repository\\n35'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 35}, page_content='NASA prognostics data [233] https://ti.arc.nasa.gov/tech/dash/groups/p\\ncoe/prognostic-data-repository/#bearing\\nN-CMAPSS [192], [182] https://ti.arc.nasa.gov/tech/dash/groups/p\\ncoe/prognostic-data-repository/\\nMFPT [99] https://api.semanticscholar.org/CorpusID:\\n26870683 (source paper)\\nNREL PV [177] https://www.nrel.gov/\\nPRONOSTIA [138], [212] https://hal.science/hal-00719503/document\\n(source paper)\\nSEU bearing dataset [100] https://github.com/cathysiyu/Mechanical-d\\natasets\\nXJTUGearbox and XJ-\\nTUSpurgear\\n[99] http://dx.doi.org/10.1016/j.ymssp.2021.1086\\n53 (source paper)\\nThe two most popular datasets are C-MAPSS and N-CMAPSS, two well-known datasets for RUL\\nprediction provided by NASA. C-MAPSS describes the deterioration of aircraft engines by recording\\ntemperature, pressure, and fan speed during the cruise phase (https://ntrs.nasa.gov/api/citations/\\n20070034949/downloads/20070034949.pdf). N-CMAPSS is an extended version of C-MAPSS, whose\\nrecords cover climbing, cruising, and descending flight conditions (https://ti.arc.nasa.gov/tech/dash/\\ngroups/pcoe/prognostic-data-repository/). Practically no papers use exogenous variables, with the\\nexception of Ref. [177], which evaluates the impact of meteorological variables on fault diagnosis of\\nphotovoltaic systems.\\nThe granularity of the studied time series is highly variable, ranging from 1 hour to fractions of\\nseconds. Many papers report the frequency of the sensors, with values ranging from 1 Hz to 50 kHz.\\nMost datasets are normalized, by either using min-max normalization or Z-score normalization. In\\nsome papers, a data augmentation procedure is employed to mitigate the natural imbalance between\\nthe number of normal and anomalous conditions.\\n5.6.3 Proposed models\\nHalf of the proposed GNN models (9 out of 16 papers) fall under the category of pure convolutional\\nmodels, followed by 4 attentional GNNs, 2 hybrid models, and one whose description does not clarify\\nwhich class of the taxonomy it belongs to (Ref. [138]). Almost all the models have a graph learning\\nmodule, so that the graph structure is rarely defined a priori by the researchers. Three papers ([100],\\n[141], [182]) use the visibility graph algorithm proposed by Lacasa et al. in Ref. [96] for converting the\\ntime series into a graph.\\nThe most common loss functions in this group are the cross entropy function for classification\\nproblems, and MSE or RMSE for RUL forecasting problems. Most of the papers mention the Python\\nlanguage and the PyTorch library, but only two papers provide a link to the source code of their\\nmodel, and in both cases, it is for RUL prediction. The source code for the convolution-graph attention\\nnetwork (ConvGAT) [25] is available at https://github.com/CUG-FDGroup, while the one for the\\ngraph network with causal connectivity and temporal convolutional neural network feature extractor\\nproposed in Ref. [138] is available at https://github.com/mylonasc/gnn-tcnn.\\n5.6.4 Benchmark models\\nTab. 26 presents the benchmark models employed in the ”Predictive monitoring” group by at least\\ntwo articles examined in this review.\\nTable 26: List of benchmark models in the ”Predictive monitoring” group divided per category.\\nCategory Model Used by\\nClassical\\nmachine\\nlearning\\nmethods\\nAttention-based long-short term memory net-\\nwork\\n[25], [93]\\n36'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 36}, page_content='Feature-attention based bidirectional GRU\\nand CNN model (AGCNN) [246]\\n[25], [182], [93]\\nBidirectional long-short term memory net-\\nwork (BiLSTM) [52]\\n[192], [25], [182], [93]\\nConvolutional neural network (CNN) [109], [226], [192], [25], [182],\\n[212], [93]\\nDecision tree (DT) [109], [226]\\nFeed-forward neural network (FNN) [100], [138]\\nGradient boosting (GB) [226], [182]\\nGated recurrent unit (GRU) [177], [226], [212]\\nLogistic regression (LR) [109], [226]\\nLong-short term memory network (LSTM) [100], [226], [182], [93], [196]\\nRandom forest (RF) [226], [182]\\nSupport vector machine (SVM) [109], [226]\\nTransformer [178] [192], [182]\\nGNN methods Graph attention network (GAT) [179] [221], [226], [99]\\nGraph convolutional network (GCN) [221], [100], [212], [99]\\nHierarchical attention graph convolutional\\nnetwork (HAGCN) [115]\\n[192], [25]\\nSpatio-temporal fusion attention (STFA) [93] [25], [182]\\nSpatio-temporal graph convolutional network\\n(STGCN) [216]\\n[233], [192]\\nIn this group of papers there is a high variety of benchmarks used, and only few papers share a\\nbenchmark model, especially in the GNN category. The most popular benchmarks are two classic\\nwell-known machine learning models, the CNN and the LSTM network. Not surprisingly, there are\\nno classical statistical or mathematical benchmark models, since they are more frequently used in\\nforecasting problems compared to classification problems.\\nAmong the GNN benchmarks, there are two models that have been specifically developed for re-\\nmaining useful life prediction, namely the hierarchical attention graph convolutional network (HAGCN)\\n[115] and the spatio-temporal fusion attention (STFA) [93]. In the HAGCN model, proposed by Li\\net al. in 2021, the spatial dependencies are modeled by a hierarchical graph representation layer, and\\na bidirectional long-short term memory network is used for modeling temporal dependencies of sen-\\nsor measurements. As for the STFA model, proposed by Kong et al. in 2022, it integrates a priori\\nknowledge about the equipment’s structure with a spatio-temporal deep learning architecture which\\nemploys LSTM cells and an attention mechanism. Neither of the two original papers includes a link\\nto the source code.\\n5.6.5 Results\\nThe results demonstrate that the proposed GNN models outperform the benchmarks. The accuracy of\\nthe models is evaluated using different metrics, depending on whether the context is classification or\\nforecasting. For classification tasks, several studies present confusion matrices and calculate accuracy\\npercentages. For RUL forecasting problems instead, the most commonly used metrics are the RMSE\\nand the score function.\\nThe only results that can be compared across papers are those for the C-MAPSS dataset. Tab. 27\\nshows the accuracy of the different models, expressed in terms of RMSE and score function.\\n37'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 37}, page_content='Table 27: Comparison of the accuracy of the different models on the C-MAPSS dataset on sub-datasets\\nFD001–FD004, expressed in terms of RMSE and score function. The smallest errors are underlined.\\nNumbers from the papers.\\nSub-dataset FD001 FD002 FD003 FD004\\nMetrics RMSE Score RMSE Score RMSE Score RMSE Score\\n1D CNN 12.61 273.70 22.36 10412.00 12.64 284.10 23.31 12466.00\\n2D CNN 18.45 1286.70 30.29 13570.00 19.82 1596.20 29.16 7886.40\\nAGCNN 12.42 225.51 19.43 1492.76 13.39 227.09 21.50 3392.60\\nAttention-based LSTM 14.53 322.44 - - - - 27.08 5649.14\\nATT-LSTM 13.95 320.00 17.65 2102.00 12.72 223.00 20.21 3100.00\\nBiGRU-AS 13.68 284.00 20.81 2454.00 15.53 428.00 27.31 4708.00\\nBi-LSTM 13.65 295.00 23.18 4130.00 13.74 317.00 24.86 5430.00\\nBiLSTM-ED 14.74 273.00 22.07 3099.00 17.48 574.00 23.49 3202.00\\nBLCNN 13.18 302.28 19.09 1557.56 13.75 381.37 20.97 3858.78\\nCDSG [182] 11.26 188.00 18.13 1740.00 12.03 218.00 19.73 2332.00\\nChebyNet-EdgePool 15.21 450.22 16.28 1229.22 14.67 421.02 16.26 1159.74\\nCNN-LSTM 14.40 290.00 27.23 9869.00 14.32 316.00 26.69 6594.00\\nConvGAT [25] 11.34 197.43 14.12 771.61 10.97 235.26 15.51 1231.17\\nC-Transformer 13.79 475.46 16.11 2214.59 17.10 939.10 19.77 3237.37\\nDBN 15.21 417.59 27.12 9031.64 14.71 442.43 29.88 7954.51\\nDCNN 12.61 273.70 22.36 10412.00 12.64 284.1 23.31 12466.00\\nEarlier CNN 18.45 1287.00 30.29 13570.00 19.82 1596.00 29.16 7886.00\\nELM 17.27 523.00 37.28 498149.97 18.90 573.78 38.43 121414.00\\nGA+RBM+LSTM 12.56 231.00 22.73 3366.00 12.10 251.00 22.66 2840.00\\nGAT-EdgePool 13.53 309.59 15.07 1380.41 14.66 470.74 17.60 1726.53\\nGAT-TopkPool 13.21 303.18 17.25 5338.80 15.36 507.52 21.44 2971.93\\nGB 15.67 474.01 29.09 87280.06 16.84 576.72 29.01 17817.92\\nGGCN 11.82 186.70 17.24 1493.70 12.21 245.19 17.36 1371.50\\nHAGCN 11.93 222.30 15.05 1144.10 11.53 240.30 15.74 1218.60\\nHDNN 13.02 245.00 15.24 1282.42 12.22 287.72 18.16 1527.42\\nIMDSSN 12.14 206.11 17.40 1775.15 12.35 229.54 19.78 2852.81\\nLSTM 16.14 338.00 24.49 4450.00 16.18 852.00 28.17 5550.00\\nLSTMBS 14.89 481.00 26.86 7982.00 15.11 493.00 27.11 5200.00\\nMODBN 15.04 334.23 25.05 5585.00 12.51 421.91 28.66 6557.00\\nMS-CNN 11.44 196.22 19.35 3747.00 11.67 241.89 22.22 4844.00\\nRF 17.91 479.75 29.59 70456.86 20.27 711.13 31.12 46567.63\\nSBI 13.58 228.00 19.59 2650.00 19.16 1727.00 22.15 2901.00\\nSMDN 13.40 272.00 - - - - 23.40 4302.00\\nSTFA [93] 11.35 194.44 19.17 2493.09 11.64 224.53 21.41 2760.13\\nTransformer 13.52 287.07 19.32 1436.74 13.44 263.64 20.38 2784.62\\nIn general, the best results on the different sub-datasets are achieved by the GNN-based compre-\\nhensive dynamic structure GNN (CDSG) [182] and convolution-graph attention network (ConvGAT)\\n[25] models.\\n5.7 Generic\\nThis subsection includes all the papers that do not explicitly address a specific problem. They still\\npresent the results on some benchmark case studies, but the related datasets may cover different\\n38'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 38}, page_content='groups. In particular, the focus is on papers that do not discuss or mention any specific case study\\nprior to the experimental section. The reason for this choice is that they are considered to explore\\nbroader methodologies or frameworks that are applied to diverse datasets without a declared focus on\\na particular case.\\n5.7.1 Overview\\nA significant number of papers (24 out of 156) have been categorized under the ”Generic” group.\\nThe majority of these works commit to address a multivariate time series forecasting problem which\\nis not necessarily limited to a specific field. This ”Generic” subset includes one paper from 2021,\\n4 from 2022, and 19 from 2023, highlighting the recent interest in applying GNNs in more general\\ncontexts. These papers are distributed across various journals, with the exception of Expert Systems\\nwith Applications by Elsevier, IEEE Transactions on Knowledge and Data Engineering by IEEE, and\\nInformation Sciences by Elsevier, each publishing 2 of the selected papers. The majority of authors\\nare affiliated with institutions in China.\\n5.7.2 Datasets\\nThe selected papers make use of the datasets listed in Tab. 28 in their experiments.\\nTable 28: List of public datasets in the ”Generic” papers and their corresponding links.\\nDataset Used by Link\\nBeijing Traffic [26] https://github.com/BuaaPercy/Traffic-Dat\\naSet-of-Beijing-Road\\nCCMP wind data [120] http://data.remss.com\\nCI earthquakes [10] https://doi.org/10.5194/adgeo-43-31-2016\\n(source paper)\\nChickenpox cases [10] https://doi.org/10.48550/arXiv.2102.08100\\n(source paper)\\nCW earthquakes [30] https://doi.org/10.5194/adgeo-43-31-2016\\n(source paper)\\nElectricity consumption [120], [48], [82],\\n[121], [58], [21],\\n[135], [170], [225],\\n[64]\\nhttps://github.com/laiguokun/multivariate-t\\nime-series-data\\nElectricity Consuming\\nLoad (ECL)\\n[189], [88] https://archive.ics.uci.edu/ml/datasets/Elec\\ntricityLoadDiagrams20112014\\nElectricity transformer\\nTemperature (ETT)\\n[237], [189], [88] https://github.com/zhouhaoyi/ETDataset\\nEnergy [41] http://dx.doi.org/10.1016/j.enbuild.2017.01.0\\n83 (source paper)\\nEnron [70] https://doi.org/10.1073/pnas.1800683115\\n(source paper)\\nEu-Core [70] https://doi.org/10.1145/ (source paper)\\nExchange-rate [120], [48], [237],\\n[41], [121], [58],\\n[21], [135], [88],\\n[190], [30], [64]\\nhttps://github.com/laiguokun/multivariate-t\\nime-series-data\\nFacebook [70] http://networkrepository.com/socfb\\nGEFCOM 2012 [30] https://doi.org/10.1016/j.ijforecast.2013.07.0\\n01 (source paper)\\nHypertext [70] http://networkrepository.com/ia-infect-hyp\\ner (source paper)\\n39'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 39}, page_content='ILI [237], [88], [30] https://gis.cdc.gov/grasp/fluview/fluportal\\ndashboard.html\\nJena Weather [237] https://www.bgc-jena.mpg.de/wetter/\\nLoan [70] https://ai.ppdai.com/mirror/showCompetit\\nionRisk\\nMETR-LA [32], [82], [58], [21],\\n[27], [10]\\nhttps://paperswithcode.com/dataset/metr-la\\nMIMIC-III Heart Failure [61] https://doi.org/10.1038/sdata.2016.35\\n(source paper)\\nMIMIC-III Infection [61] https://doi.org/10.1038/sdata.2016.35\\n(source paper)\\nMOOC network [116] http://snap.stanford.edu/jodie/mooc.csv\\nNasdaq [41], [21] https://doi.org/10.48550/arXiv.1704.02971\\n(source paper)\\nNOAA Weather [189] https://www.ncei.noaa.gov/data/local-clima\\ntological-data/\\nPeMS03 [48] https://www.kaggle.com/datasets/elmahy/p\\nems-dataset\\nPeMS04 (PeMSD4) [120], [48], [121],\\n[58], [225]\\nhttps://paperswithcode.com/dataset/pems04\\nPeMS08 (PeMSD8) [120], [48], [121],\\n[58], [225]\\nhttps://paperswithcode.com/dataset/pems08\\nPEMS-BAY [32], [82], [10] https://paperswithcode.com/dataset/pems-b\\nay\\nPhysioNet [61] https://archive.physionet.org/challenge/2012\\n/papers/\\nReddit network [116] http://snap.stanford.edu/jodie/reddit.csv\\nSolar-Energy [120], [48], [82],\\n[121], [58], [21],\\n[135], [170], [190],\\n[225], [64]\\nhttps://github.com/laiguokun/multivariate-t\\nime-series-data\\nTAIEX [26] https://finance.yahoo.com/\\nTaoBao [70] https://tianchi.aliyun.com/competition/ent\\nrance/\\nTraffic [82], [121], [58],\\n[21], [135], [170],\\n[88], [190], [64]\\nhttps://github.com/laiguokun/multivariate-t\\nime-series-data\\nUS stock market price [30] https://doi.org/10.48550/arXiv.1810.09936\\n(source paper)\\nWikipedia network [116] http://snap.stanford.edu/jodie/wikipedia.csv\\nWind-Speed [237] https://www.kaggle.com/datasets/fedesorian\\no/wind-speed-prediction-dataset\\nIt should be noted that there are multiple recurring datasets that can be considered as benchmarks\\nfor the community. Some of these have already been discussed in the previous sections. If the results\\nfor these datasets have already been shown in previous sections, they will not be repeated again here.\\nInstead, they will be just marked with a star ⋆ together with the benchmark models in the tables of\\nthe previous sections.\\nThe four most common datasets are Electricity consumption, Exchange-rate, Solar-Energy and\\nTraffic, all available at https://github.com/laiguokun/multivariate-time-series-data. The Electricity\\nconsumption dataset contains the hourly electricity consumption in kWh for 321 customers recorded\\nfrom 2012 to 2014. The Traffic dataset describes the road occupancy rates measured by different\\n40'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 40}, page_content='sensors on the San Francisco Bay area road network from 2015 to 2016, with an hourly granularity.\\nIn Solar energy it is recorded the solar power production from 137 photovoltaic plants in Alabama\\nState in 2006, with a 10 minutes granularity. Finally, Exchange rate contains daily exchange rates\\nof 8 countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand and\\nSingapore, from 1990 to 2016.\\nIn general, the data granularity varies significantly, ranging from fractions of seconds to one week,\\nindicating that there is no particular focus on a specific granularity. Individual papers often cover\\nmultiple granularities, and many papers take into account datasets with granularity from 5 minutes\\nto 1 day. As for the forecasting horizon, almost all papers focus on multi-step forecasting, usually\\nconsidering 12 different forecasting horizons at the same time. Only a few papers mention data pre-\\nprocessing, and usually utilize min-max or Z-score normalization.\\n5.7.3 Proposed models\\nAmong the 24 ”Generic” papers, 19 use a purely convolutional GNN approach, and two employ a\\npurely attentional model. One paper lacks sufficient detail to determine its classification within the\\ntaxonomy [88], and another declares it is using an aggregating approach that is simpler than either\\nconvolutional or attentional approaches [135]. One more paper proposes a multivariate time series\\nwith dynamic graph neural ordinary differential equations (MTGODE) model, where the continuous\\ndynamics of simplified graph propagation is described by an ordinary differential equation (ODE) [82].\\nAs for the definition of the graph structure, the majority of these models use self-learning techniques.\\nThis is a straightforward choice, since all the papers in this section want to ensure the applicability of\\nthe model in different contexts, and the use of pre-defined rules for the definition of the graph limits\\nthe model’s universal applicability. Half of the papers employ a dynamic graph structure, which adapts\\nto the varying temporal patterns observed in the data.\\nThe two most commonly utilized loss functions are the MSE and the MAE. Since the graph structure\\nis typically learned by the models themselves, many papers include a regularization term in the loss\\nfunction for the optimization of the graph structure (e.g., Refs. [120], [121], [58], [27]).\\nThe majority of the papers specify the language and libraries used for the code, namely Python\\nwith PyTorch, PyTorch Geometric [44], and Torch Spatiotemporal [31]. However, only slightly less\\nthan half of the papers provide a link to the source code. The links to the code repositories are listed\\nin Tab. 29.\\nTable 29: List of source codes of the ”Generic” models in the review.\\nModel Link\\nAdaptive dependency learning neural network\\n(ADLNN) [170]\\nhttps://github.com/AbishekSriramulu/\\nADLGNN.git\\nGraph neural network with neural Granger causality\\n(CauGNN) [41]\\nhttps://github.com/RRRussell/CauGN\\nN\\nGformer [189] https://github.com/wxh453751461/Gf\\normer\\nMulti-scale adaptive graph neural network\\n(MAGNN) [21]\\nhttps://github.com/shangzongjiang/MA\\nGNN\\nMultivariate time series deep spatiotemporal fore-\\ncasting model with a graph neural network (MDST-\\nGNN) [64]\\nhttps://github.com/yiminghzc/MDS\\nT-GNN\\nMultivariate time series with dynamic graph neural\\nordinary differential equations (MTGODE) [82]\\nhttps://github.com/GRAND-Lab/MTG\\nODE\\nStatic and dynamic graph learning network (SDGL)\\n[121]\\nhttps://github.com/ZhuoLinLi-shu/S\\nDGL\\nSparse graph learning from spatiotemporal time se-\\nries [32]\\nhttps://github.com/andreacini/sparse-g\\nraph-learning\\n41'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 41}, page_content='Temporal decomposition enhanced graph neural\\nnetwork for multivariate time series forecasting\\n(TDG4MSF) [135]\\nhttps://github.com/TYUT-Theta/MHZ\\nN.git\\nTemporal graph convolution and attention (T-GAN)\\n[70]\\nhttps://github.com/malei666666/T G\\nAN\\nTwo GNN models with different graph structures [10] https://github.com/StefanBloemheuvel\\n/graph comparison\\n5.7.4 Benchmark models\\nTab. 30 displays the benchmark models utilized by at least two of the selected ”Generic” papers.\\nTable 30: List of benchmark models in the ”Generic” group divided per category.\\nCategory Model Used by\\nMathematical\\nand statistical\\nmethods\\nAutoregressive integrated moving average\\n(ARIMA)\\n[120], [48], [121], [58], [21],\\n[135], [170], [26], [225], [30]\\nGaussian process (GP) [120], [48], [121], [21], [135],\\n[170]\\nHistorical average (HA) [121], [225]\\nVector autoregression (VAR) [120], [48], [41], [121], [58],\\n[135], [26], [225]\\nClassical\\nmachine\\nlearning\\nmethods\\nAutoformer [198] [48], [237], [88]\\nDual self-attention network for multivariate\\ntime series forecasting (DSANet) [71]\\n[120], [121]\\nFeed-forward neural network (FNN) [26], [30]\\nGated recurrent unit (GRU) [120], [121], [189], [225], [64]\\nHybrid framework based on fully Dilated CNN\\n(HyDCNN) [117]\\n[82], [170]\\nInformer [243] [48], [237], [189], [88]\\nLogTrans [140] [189], [88]\\nLong-short term memory network (LSTM) [61], [48], [58], [189], [26], [88],\\n[27], [225], [30]\\nLong-short term time series network (LST-\\nNet) [97]\\n[120], [48], [82], [41], [121],\\n[21], [135], [189], [170], [88],\\n[190], [30], [64]\\nMulti-Level Construal Neural Network (ML-\\nCNN) [29]\\n[41], [190]\\nNeural basis expansion analysis for inter-\\npretable time series forecasting (N-BEATS)\\n[143]\\n[19], [88]\\nRecurrent neural network with fully connected\\ngated recurrent units (RNN-GRU)\\n[120], [41], [121], [58], [21],\\n[135], [170], [190]\\nReformer [92] [237], [189], [88]\\nSCINet [129] [48], [64]\\nTemporal pattern attention long-short term\\nmemory network (TPA-LSTM) [166]\\n[120], [48], [82], [121], [58],\\n[21], [135], [170], [64]\\nTransformer [178] [61], [41]\\nHybrid of the multilayer perception and au-\\ntoregressive model (VAR-MLP) [228]\\n[120], [48], [121], [58], [21],\\n[135], [170], [190], [64]\\n42'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 42}, page_content='GNN methods Adaptive graph convolutional recurrent net-\\nwork (AGCRN) [4]\\n[120], [48], [121], [58], [21],\\n[225]\\nAttention-based spatial-temporal graph con-\\nvolutional network (ASTGCN) [57]\\n[121], [58]\\nDiffusion convolutional recurrent neural net-\\nwork (DCRNN) [119]\\n[120], [48], [82], [121], [58],\\n[26], [27], [190], [225]\\nGraph auto encoder (GAE) [90] [116], [70]\\nGraph multi-attention network (GMAN) [242] [120], [82], [27], [225]\\nGraph for time series (GTS) [161] [32], [121]\\nGraph WaveNet (GWN) [202] [48], [82], [58], [21], [27], [190]\\nMultivariate time series forecasting with\\ngraph neural network (MTGNN) [201]\\n[32], [120], [48], [82], [121],\\n[58], [21], [135], [170], [27],\\n[190], [225], [64]\\nNode2vec [53] [116], [70]\\nSpatial-temporal fusion graph neural network\\n(STFGNN) [114]\\n[120], [27]\\nSpatio-temporal graph convolutional network\\n(STGCN) [216]\\n[120], [82], [121], [58], [26],\\n[27], [225]\\nSpatial-temporal graph ODE networks\\n(STGODE) [43]\\n[82], [225]\\nSpatio-temporal synchronous graph convolu-\\ntional network (STSGCN) [169]\\n[120], [48], [121]\\nThere is a wide variety of benchmark models in this ”Generic” group. As for mathematical and\\nstatistical models, autoregressions and ARIMA models are very popular for their simplicity and in-\\nterpretability. Among classical machine learning models, the most widely used models are LSTMs,\\ntemporal pattern attention long-short term memory network (TPA-LSTM) [166], and long-short term\\ntime series network (LSTNet) [97]. The TPA-LSTM model, introduced by Shih et al. in 2019, em-\\nploys a set of filters to capture time-invariant temporal patterns, and an attention mechanism to\\nidentify relevant time series for multivariate forecasting. The source code for this benchmark model\\nis available at https://github.com/gantheory/TPA-LSTM. The LSTNet model, proposed by\\nLai et al. in 2018, combines a convolution neural network and a recurrent neural network to cap-\\nture short-term local dependencies among variables and identify long-term patterns. Additionally, it\\nincorporates an autoregressive model that enhances the robustness of the deep learning approach\\nto time series with significant scale fluctuations. The source code for the model is accessible at\\nhttps://github.com/laiguokun/LSTNet. Among GNN models, the most common ones are DCRNN\\n[119] and MTGNN [201], already discussed in the previous sections.\\n5.7.5 Results\\nThe most common error metrics used in the evaluation of the models in the ”Generic” group are, in or-\\nder, the mean absolute error (MAE), correlation coefficient (CORR), root mean square error (RMSE),\\nroot relative squared error (RRSE), mean absolute percentage error (MAPE), and mean squared error\\n(MSE). Differently from other metrics, where lower values indicate higher accuracy, the correlation\\ncoefficient (CORR) quantifies the strength of the the linear relationship between predicted and actual\\nvalues, with higher values indicating a better performance. This Results subsection compares the\\nresults obtained for the Electricity consumption, Exchange rate, Solar energy and Traffic datasets.\\nThe results of the proposed models and benchmarks on METR-LA and PEMS datasets were already\\nreported with a ⋆ in Subsec. 5.5.5.\\nTab. 31 shows the accuracy of the various models applied to the Electricity consumption dataset,\\nwhich includes data from 1 st January 2012 to 31 st December 2014 at hourly granularity, for 3, 6, 12\\nand 24 steps ahead horizons. The accuracy is expressed in terms of RRSE and CORR.\\n43'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 43}, page_content='Table 31: Comparison of the average accuracy of the different models on the Electricity consumption\\ndataset for 3, 6, 12, and 24 steps time horizons, expressed in terms of RRSE and CORR. The numbers\\ncorresponding to the highest accuracy are underlined.\\nTime horizon 3 steps 6 steps 12 steps 24 steps\\nMetrics RRSE CORR RRSE CORR RRSE CORR RRSE CORR\\nADLGNN [170] 0.0719 0.9506 0.0809 0.9386 0.0887 0.9312 0.0930 0.9294\\nAGCRN 0.0766 0.9408 0.0894 0.9309 0.0921 0.9222 0.0967 0.9183\\nAGLG-GRU [58] 0.0738 0.9434 0.0864 0.9302 0.0912 0.9283 0.0947 0.9274\\nAR 0.0995 0.8845 0.1035 0.8632 0.105 0.8591 0.1054 0.8595\\nARIMA 0.0917 0.8902 0.1002 0.8834 0.1028 0.8604 0.1042 0.8487\\nAutoformer 0.1258 0.9147 0.1344 0.9001 0.1357 0.8921 0.1554 0.8704\\nCrossformer 0.0742 0.9452 0.0855 0.9351 0.0901 0.9280 0.0967 0.9205\\nDSTGN [120] 0.0713 0.9518 0.0821 0.9424 0.0887 0.9357 - -\\nDSTIGNN [48] 0.0733 0.9515 0.082 0.9412 0.0899 0.9338 0.0954 0.9289\\nESG 0.0718 0.9494 0.0844 0.9372 0.0898 0.9321 0.0962 0.9279\\nFEDformer 0.0889 0.9321 0.1006 0.9191 0.1154 0.908 0.1202 0.9012\\nGP 0.1500 0.8670 0.1907 0.8334 0.1621 0.8394 0.1273 0.8818\\nGTS 0.0790 0.9291 0.0884 0.9187 0.0957 0.9135 0.0951 0.9098\\nGWN 0.0746 0.9459 0.0922 0.9310 0.0909 0.9267 0.0962 0.9226\\nHyDCNN 0.0832 0.9354 0.0898 0.9329 0.0921 0.9285 - -\\nInformer 0.1337 0.8903 0.1532 0.8705 0.1635 0.8527 0.1834 0.8399\\nLSTNet 0.0864 0.9283 0.0931 0.9135 0.1007 0.9077 0.1007 0.9119\\nMAGNN [21] 0.0745 0.9476 0.0876 0.9323 0.0908 0.9282 0.0963 0.9217\\nMDST-GNN [64] 0.0738 0.9454 0.0833 0.9346 0.0884 0.9264 0.0922 0.9222\\nMTGNN 0.0745 0.9474 0.0878 0.9316 0.0916 0.9278 0.0953 0.9234\\nMTGODE [82] 0.0736 0.9430 0.0809 0.9340 0.0891 0.9279 - -\\nMTHetGNN 0.0749 0.9456 0.0892 0.9307 0.0959 0.8783 0.0969 0.8782\\nMTNet 0.0840 0.9319 0.0901 0.9226 0.0934 0.9165 0.0969 0.9147\\nRNN-GRU 0.1102 0.8597 0.1144 0.8623 0.1183 0.8472 0.1295 0.8651\\nSARIMA 0.0906 0.9055 0.0999 0.8829 0.1026 0.8674 0.1036 0.8621\\nSCINet 0.0740 0.9494 0.0845 0.9387 0.0929 0.9305 0.0967 0.927\\nSDGL [121] 0.0698 0.9534 0.0805 0.9445 0.0889 0.9351 0.0935 0.9301\\nSDLGNN 0.0726 0.9502 0.082 0.9384 0.0896 0.9304 0.0947 0.9257\\nSDLGNN-Corr 0.0737 0.9475 0.0841 0.9346 0.0923 0.9263 0.0971 0.9227\\nSTG-NCDE 0.6152 0.8739 0.6584 0.8663 0.7302 0.8728 - -\\nTDG4-MSF [135] 0.0731 0.9499 0.0828 0.9371 0.0894 0.9306 0.0969 0.9246\\nTheta 0.0975 0.8906 0.1029 0.8723 0.1040 0.8576 0.1041 0.8595\\nTPA-LSTM 0.0823 0.9439 0.0916 0.9337 0.0964 0.9250 0.1006 0.9133\\nTRMF 0.1802 0.8538 0.2039 0.8424 0.2186 0.8304 0.3656 0.7471\\nVAR-MLP 0.1393 0.8708 0.1620 0.8389 0.1557 0.8192 0.1274 0.8679\\nThe accuracy of the various models varies depending on the studied forecasting horizon but, overall,\\none can notice that the GNN models perform better than the others, with the static and dynamic graph\\nlearning network (SDGL) [121] being the most accurate.\\nTab. 32 presents the accuracy of different models applied to the Exchange rate dataset, with daily\\ndata from 1990 to 2016. The evaluation covers horizons of 3, 6, 12, and 24 steps ahead, with accuracy\\nmeasured by RRSE and CORR.\\n44'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 44}, page_content='Table 32: Comparison of the average accuracy of the different models on the Exchange rate dataset\\nfor 3, 6, 12, and 24 steps time horizons, expressed in terms of RRSE and CORR. The numbers\\ncorresponding to the highest accuracy are underlined.\\nTime horizon 3 steps 6 steps 12 steps 24 steps\\nMetrics RRSE CORR RRSE CORR RRSE CORR RRSE CORR\\nAGCRN 0.0269 0.9717 0.0331 0.9615 0.0374 0.9531 0.0476 0.9334\\nAGLG-GRU [58] 0.0191 0.9792 0.0233 0.9701 0.0328 0.9548 0.0449 0.9372\\nAR 0.0228 0.9734 0.0279 0.9656 0.0353 0.9526 0.0445 0.9357\\nARIMA 0.0198 0.9754 0.0261 0.9721 0.0344 0.9548 0.0445 0.9301\\nAutoformer 0.0291 0.9631 0.0379 0.9397 0.0441 0.9201 0.0501 0.9012\\nCrossformer 0.0226 0.9759 0.0269 0.9656 0.0358 0.9436 0.0489 0.9291\\nDSTGN [120] 0.0179 0.9782 0.0250 0.9715 0.0346 0.9562 - -\\nDSTIGNN [48] 0.0173 0.9818 0.0244 0.9723 0.0333 0.9571 0.0430 0.9407\\nESG 0.0181 0.9792 0.0246 0.9717 0.0345 0.9564 0.0468 0.9392\\nFEDformer 0.0256 0.9701 0.0287 0.9555 0.0379 0.9385 0.0487 0.9190\\nGP 0.0239 0.8713 0.0272 0.8193 0.0394 0.8484 0.0580 0.8278\\nGTS 0.0180 0.9898 0.0260 0.9824 0.0333 0.9701 0.0442 0.9518\\nGWN 0.0251 0.9740 0.0300 0.9640 0.0381 0.951 0.0486 0.9294\\nInformer 0.0882 0.9563 0.1081 0.9321 0.1301 0.8911 0.1521 0.8021\\nLSTNet 0.0226 0.9735 0.0280 0.9658 0.0356 0.9511 0.0449 0.9354\\nMAGNN [21] 0.0183 0.9778 0.0246 0.9712 0.0343 0.9557 0.0474 0.9339\\nMDST-GNN [64] 0.0172 0.9811 0.0245 0.9727 0.0337 0.9578 0.0431 0.9392\\nMTGNN 0.0194 0.9786 0.0259 0.9708 0.0349 0.9551 0.0456 0.9372\\nMTHetGNN 0.0198 0.9769 0.0259 0.9701 0.0345 0.9539 0.0451 0.9360\\nMTNet 0.0212 0.9767 0.0258 0.9703 0.0347 0.9561 0.0442 0.9388\\nRNN-GRU 0.0192 0.9786 0.0264 0.9712 0.0408 0.9531 0.0626 0.9223\\nSARIMA 0.0197 0.9748 0.0253 0.9643 0.0338 0.9495 0.0444 0.9370\\nSCINet 0.0171 0.9787 0.0240 0.9704 0.0331 0.9553 0.0436 0.9396\\nSDGL [121] 0.0180 0.9808 0.0249 0.9730 0.0342 0.9583 0.0455 0.9402\\nTDG4-MSF [135] 0.0172 0.9825 0.0244 0.9718 0.0330 0.9569 0.0433 0.9386\\nTheta 0.0497 0.9738 0.0257 0.9656 0.0342 0.9510 0.0441 0.9323\\nTPA-LSTM 0.0174 0.9790 0.0241 0.9709 0.0341 0.9564 0.0444 0.9381\\nTRMF 0.0351 0.9142 0.0875 0.8123 0.0494 0.8993 0.0563 0.8678\\nVAR-MLP 0.0265 0.8609 0.0394 0.8725 0.0407 0.8280 0.0578 0.7675\\nAlso in this case, GNN models are the best overall. The discrete graph structure learning for\\ntime series model (GTS), proposed in 2021 by Shang et al. in Ref. [161] and used as a benchmark in\\nRef. [121], demonstrates itself to have a superior performance. It is therefore a suitable benchmark for\\ncomparison.\\nTab. 33 summarizes the accuracy of the various models on the Solar energy dataset, covering the\\nperiod from January 1, 2016, to December 31, 2016, at 10-minutes intervals. The analysis considers\\nforecasting horizons of 3, 6, 12, and 24 steps ahead, and the results are expressed in terms of RRSE\\nand CORR.\\nTable 33: Comparison of the average accuracy of the different models on the Solar energy dataset for 3,\\n6, 12, and 24 steps time horizons, expressed in terms of RRSE and CORR. The numbers corresponding\\nto the highest accuracy are underlined.\\nTime horizon 3 steps 6 steps 12 steps 24 steps\\n45'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 45}, page_content='Metrics RRSE CORR RRSE CORR RRSE CORR RRSE CORR\\nADLGNN [170] 0.1708 0.9866 0.2188 0.9768 0.2897 0.9551 0.4128 0.906\\nAGCRN 0.1840 0.9841 0.2432 0.9708 0.3185 0.9487 0.4141 0.9087\\nAGLG-GRU [58] 0.1762 0.9842 0.2302 0.9682 0.3021 0.9532 0.4130 0.9084\\nAR 0.2435 0.9710 0.3790 0.9263 0.5911 0.8107 0.8699 0.5314\\nARIMA 0.2328 0.9739 0.3413 0.9402 0.4531 0.8886 0.5810 0.8133\\nAutoformer 0.1935 0.9784 0.2604 0.9651 0.3959 0.9111 0.6064 0.818\\nCrossformer 0.1758 0.9801 0.2304 0.9679 0.3101 0.9437 0.4115 0.9001\\nDSTGN [120] 0.1787 0.9867 0.2358 0.9721 0.3079 0.9513 - -\\nDSTIGNN [48] 0.1684 0.9869 0.2165 0.9773 0.2863 0.9584 0.4064 0.9118\\nESG 0.1708 0.9865 0.2278 0.9743 0.3073 0.9519 0.4101 0.9100\\nFEDformer 0.1901 0.9827 0.2444 0.9606 0.3502 0.9275 0.4851 0.8876\\nGP 0.2259 0.9751 0.3286 0.9448 0.5200 0.8518 0.7973 0.5971\\nGTS 0.1842 0.9842 0.2691 0.9645 0.3259 0.9481 0.4796 0.8678\\nGWN 0.1773 0.9846 0.2279 0.9743 0.3068 0.9527 0.4206 0.9055\\nHyDCNN 0.1806 0.9865 0.2335 0.9747 0.3094 0.9515 - -\\nInformer 0.2134 0.9715 0.2701 0.9549 0.4331 0.8985 0.7017 0.7921\\nLSTNet 0.1843 0.9843 0.2559 0.969 0.3254 0.9467 0.4643 0.887\\nMAGNN [21] 0.1771 0.9853 0.2361 0.9724 0.3015 0.9539 0.4108 0.9097\\nMDST-GNN [64] 0.1764 0.9855 0.2321 0.9735 0.3082 0.9519 0.4119 0.9103\\nMTGNN 0.1778 0.9852 0.2348 0.9726 0.3109 0.9509 0.4270 0.9031\\nMTGODE [82] 0.1693 0.9868 0.2171 0.9771 0.2901 0.9577 - -\\nMTHetGNN 0.1838 0.9845 0.2600 0.9681 0.3169 0.9486 0.4231 0.9031\\nMTNet 0.1847 0.9840 0.2398 0.9723 0.3251 0.9462 0.4285 0.9013\\nRNN-GRU 0.1932 0.9823 0.2628 0.9675 0.4163 0.9150 0.4852 0.8823\\nSARIMA 0.2227 0.9760 0.3228 0.9461 0.4512 0.8900 0.5714 0.8184\\nSCINet 0.1775 0.9853 0.2301 0.9739 0.2997 0.9550 0.4081 0.9112\\nSDGL [121] 0.1699 0.9866 0.2222 0.9762 0.2924 0.9565 0.4047 0.9119\\nSDLGNN 0.1720 0.9864 0.2249 0.9757 0.3024 0.9547 0.4184 0.9051\\nSDLGNN-Corr 0.1806 0.9848 0.2378 0.9722 0.3042 0.9534 0.4173 0.9067\\nSTG-NCDE 0.2346 0.9748 0.2908 0.9605 0.5149 0.8639 - -\\nTDG4-MSF [135] 0.1746 0.9858 0.2348 0.9727 0.3082 0.9527 0.4031 0.9143\\nTheta 0.2442 0.9685 0.3327 0.9445 0.4488 0.8979 0.6092 0.8139\\nTPA-LSTM 0.1803 0.9850 0.2347 0.9742 0.3234 0.9487 0.4389 0.9081\\nTRMF 0.2473 0.9703 0.3470 0.9418 0.5597 0.8475 0.9005 0.5598\\nVAR-MLP 0.1922 0.9829 0.2679 0.9655 0.4244 0.9058 0.6841 0.7149\\nThe results indicate that the dynamic spatiotemporal interactive graph neural network (DSTIGNN)\\nmodel [48] and the static and dynamic graph learning network (SDGL) model [121] are the most\\naccurate on the solar energy dataset, for shorter and longer time horizons, respectively.\\nFinally, Tab. 34 illustrates the performance of different models on the Traffic dataset, with hourly\\ndata between 2015 and 2016. The accuracy of each model is assessed across 3, 6, 12, and 24 steps\\nahead horizons, with RRSE and CORR as the evaluation metrics.\\nTable 34: Comparison of the average accuracy of the different models on the Traffic dataset for 3, 6,\\n12, and 24 steps time horizons, expressed in terms of RRSE and CORR. The numbers corresponding\\nto the highest accuracy are underlined.\\nTime horizon 3 steps 6 steps 12 steps 24 steps\\n46'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 46}, page_content='Metrics RRSE CORR RRSE CORR RRSE CORR RRSE CORR\\nADLGNN [170] 0.4047 0.9028 0.4201 0.8928 0.4299 0.8876 0.4416 0.8818\\nAGCRN 0.4379 0.8850 0.4635 0.8670 0.4694 0.8679 0.4707 0.8664\\nAGLG-GRU [58] 0.4173 0.8958 0.4722 0.8541 0.4427 0.8755 0.4526 0.8842\\nAR 0.5991 0.7752 0.6218 0.7568 0.6252 0.7544 0.6300 0.7519\\nARIMA 0.5841 0.7959 0.6194 0.7655 0.6197 0.7652 0.6248 0.7610\\nGP 0.6082 0.7831 0.6772 0.7406 0.6406 0.7671 0.5995 0.7909\\nGTS 0.4665 0.8695 0.4779 0.8582 0.4792 0.8589 0.4766 0.8573\\nGWN 0.4484 0.8801 0.4689 0.8674 0.4725 0.8646 0.4741 0.8646\\nHyDCNN 0.4198 0.8915 0.4290 0.8855 0.4352 0.8858 0.4423 0.8819\\nLSNet 0.4777 0.8721 0.4893 0.8690 0.4950 0.8614 0.4973 0.8588\\nMAGNN [21] 0.4097 0.8992 0.4555 0.8753 0.4423 0.8815 0.4434 0.8813\\nMDST-GNN [64] 0.4162 0.8958 0.4461 0.8803 0.4377 0.8841 0.4452 0.8792\\nMTGNN 0.4162 0.8963 0.4754 0.8667 0.4461 0.8794 0.4535 0.8810\\nMTGODE [82] 0.4127 0.900 0.4259 0.8945 0.4329 0.8899 - -\\nMTHetGNN 0.4826 0.8643 0.5198 0.8452 0.5147 0.8744 0.5250 0.8418\\nMTNet 0.4764 0.8728 0.4855 0.8681 0.4877 0.8644 0.5023 0.8570\\nRNN-GRU 0.5358 0.8511 0.5522 0.8405 0.5562 0.8345 0.5633 0.8300\\nSARIMA 0.5823 0.7967 0.5974 0.7837 0.6002 0.7811 0.6151 0.7697\\nSCINet 0.4216 0.8920 0.4414 0.8809 0.4495 0.8772 0.4453 0.8825\\nSDGL [121] 0.4142 0.9010 0.4475 0.8825 0.4584 0.8760 0.4571 0.8766\\nSDLGNN 0.4053 0.9017 0.4209 0.8925 0.4313 0.8868 0.4444 0.8801\\nSDLGNN-Corr 0.4227 0.8937 0.4378 0.8846 0.4576 0.8746 0.4579 0.8784\\nTDG4-MSF [135] 0.4029 0.9014 0.4196 0.8925 0.4294 0.8864 0.4366 0.8834\\nTheta 0.6071 0.7768 0.6241 0.7606 0.6271 0.7591 0.6012 0.783\\nTPA-LSTM 0.4487 0.8812 0.4658 0.8717 0.4641 0.8717 0.4765 0.8629\\nTRMF 0.6708 0.6964 0.6261 0.7430 0.5956 0.7748 0.6442 0.7278\\nVAR-MLP 0.5582 0.8245 0.6570 0.7695 0.6023 0.7929 0.6146 0.7891\\nThe accuracy of the models varies by horizon and metric. However, the best one in terms of RRSE\\nover all horizons is the temporal decomposition enhanced graph neural network for multivariate time\\nseries forecasting (TDG4MSF) [135].\\n5.8 Other topics\\nThis final subsection includes 20 papers that fall outside the previously discussed groups. The case\\nstudies include cellular traffic prediction, recommendation systems for user preferences, and other\\nforecasting or classification problems. Given the heterogeneity of the papers, only a general overview\\nis provided here, and no comparisons of the approaches or discussions of benchmarks are presented.\\n5.8.1 Overview\\nSince GNNs have recently gained considerable popularity, many recent papers focus on applications\\nthat do not relate to the categories discussed so far. A frequently discussed topic in this ”Other topics”\\ncategory is cellular traffic prediction ([194], [17], [218]), in terms of SMS messages, calls, internet\\nconnections, phone’s traffic statistics or locations, which are essential for the cellular network resource\\nmanagement system. Another topic is modulation classification ([181], [208], [3]), which is the process\\nof determining the modulation used at the transmitter based on observations of the received signal\\n[60]. A further area of investigation is the study of user preferences ([235], [83]) to enable personalized\\nexperiences and recommendations.\\n47'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 47}, page_content='Other applications discussed in the papers of this section are encrypted traffic classification [37],\\nurban spatio-temporal event prediction [81], prediction of the quality of operational processes [232],\\nseismic intensity forecasting [11] (with source code available at https://github.com/StefanBloemheuv\\nel/GCNTimeseriesRegression), prediction of spatio-temporal dynamics with complex structures [197],\\ntailing dam monitoring [158], forecasting of educational video engagement [144], subsurface production\\nforecasting [130], time series prediction for data centers maintenance [163], hydraulic runoff [211],\\nwater demand forecasting [222], and human activity recognition [136] (with source code available at\\nhttps://github.com/riktimmondal/HAR-Sensor).\\n5.8.2 Datasets\\nAmong the papers selected here, there are only three shared datasets, two for modulation classification\\nand one for cellular traffic prediction. However, the datasets are handled in different ways, either in\\nterms of pre-processing techniques, selected time windows, or train/test split percentages. For this\\nreason, the results of these papers are not directly comparable among each other.\\nThe two datasets for modulation classification used by Refs. [181], [208] and [3] are RML2016.10a\\n(https://www.kaggle.com/datasets/raindrops12/rml201610a, source paper: http://dx.doi.org/10.1007\\n/978-3-319-44188-7 16) and RML2016.10b (https://www.kaggle.com/datasets/marwanabudeeb/rm\\nl201610b, source paper: https://pubs.gnuradio.org/index.php/grcon/article/view/11). They include\\n220,000 and 1.2 million samples respectively, in many modulation types. Each modulation type has\\n20 levels of signal-to-noise ratios at 2 dB intervals from -20 dB to 18 dB.\\nAs for the cellular traffic prediction dataset used in Refs. [194] and [17], it was proposed by Telecom\\nItalia and MIT Media Lab during the ”Telecom Italia Big Data Challenge”, and it is available at\\nhttps://dataverse.harvard.edu/dataverse/bigdatachallenge. The investigated geographical area is\\ndivided in a grid, and each cell records the number of SMS messages, calls, and wireless network traffic\\ndata in a 10 minutes interval.\\n6 Discussion\\nThe aim of this SLR is to provide a comprehensive and detailed overview of the use of spatio-temporal\\nGNN models for time series classification and forecasting in various fields, and to assess their per-\\nformance in different application domains. Existing literature has been collected and synthesized in\\norder to present an overview of datasets, models, and tables of results to support researchers in their\\nfuture work. In the Introduction two sets of research questions were defined: a generic set and a\\nspecific set. Now, the following discussion will answer those research questions. Actually, most of the\\ngeneral research questions were already addressed in the overview presented in Sec. 3. As for the tools,\\nthe GNN community mostly uses Python and PyTorch, although some researchers use TensorFlow.\\nUnfortunately, only few papers provide a link to the source code of the proposed model. Almost all\\npapers are funded by public or private entities.\\nAs for the specific questions, the answers are the following.\\nSQ1 (Applications). The three most investigated fields are, in order, ”Mobility”, ”Environment”,\\nand ”Generic”. Regarding the first two groups, their datasets can be naturally translated into graphs,\\nwhich explains why they have been studied more extensively over time. In contrast, the ”Generic”\\ngroup has recently gained a significant interest for the applicability to broader contexts. As for the\\ndifferences between the various fields, the main ones are related to the definition of the graph, which\\nis not always explicit, and the benchmarks used, often related to the mindset of specific communities.\\nIt is not easy to compare results across different applications and to determine the most promising\\nfields. The studies in the ”Generic” group appear promising across a wide range of domains.\\nSQ2 (Graph construction). Most of the selected papers focus on pre-defined graph structures\\n(when available), with the objective of extracting the maximum amount of information and enhancing\\nthe interpretability of the model. However, there is a recent growing interest in models that learn the\\ngraph structure and the edge weights themselves. This trend is expected to become more prevalent in\\nthe future.\\nSQ3 (Taxonomy). As observed in Ref. [223], there are two main approaches to the design of\\nspatio-temporal GNNs: one that treats the spatial and temporal components in separate modules,\\nand one that integrates and processes them together. The analysis of the collected papers reveals that\\n48'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 48}, page_content='the most common approach in literature is that of separate modules. Specifically, researchers often\\naddress the spatial and temporal aspects of the problem independently from each other, and focus on\\neach module separately. The proposed taxonomy of GNN models refers only to the spatial component,\\nand the review indicates that the convolutional and the attentional approaches are the prevalent ones.\\nApproximately 62% of the models are purely convolutional, 25% are purely attentional, and 8% are\\nhybrid convolutional-attentional. With regard to the temporal component, the recurrent structures of\\nGRUs and the attention mechanism are widely used.\\nSQ4 (Benchmark models). As for the benchmark models, there are many options available,\\nand their choice depends on the specific application. In fields such as energy, Finance, health, and\\npredictive monitoring, the focus tends to be on simpler statistical and classical machine learning\\nbenchmarks, with only a limited use of GNN benchmarks. In the ”Generic” group, many recent\\nmachine learning benchmark models based on the Transformer architecture have emerged. Notably,\\nin ”Mobility”, ”Environment”, and ”Generic” groups (which are also the most investigated fields),\\nthere are many reference GNN benchmark models. The most prevalent benchmarks are the graph\\nconvolutional neural network with long-short term memory (GC-LSTM) [154], spatio-temporal graph\\nconvolutional network (STGCN) [216], attention-based spatial-temporal graph convolutional network\\n(ASTGCN) [57], diffusion convolutional recurrent neural network (DCRNN) [119], spatio-temporal\\ngraph convolutional network (STGCN) [216], and multivariate time series forecasting with graph neural\\nnetwork (MTGNN) [201].\\nSQ5 (Benchmark datasets). The datasets mentioned in the selected papers are closely related\\nto the specific case of study. Even though some benchmark datasets are listed in the ”Generic” group,\\nthere is no common standard dataset for the entire GNN research community. It would be beneficial\\nto agreeing on, and start adopting some of the most commonly used datasets such as the traffic ones,\\nwith the goal of developing a shared benchmark dataset for the whole research community. Hopefully,\\nthis review could serve this purpose by providing comprehensive tables of the results of the models on\\ndifferent datasets to facilitate comparison between spatio-temporal GNN models.\\nSQ6 (Modeling paradigms). As for the modeling paradigm, most of the selected papers work\\nwith a homogeneous graph, which models the relationships among multiple entities of the same nature.\\nThis is due to the fact that many basic GNN algorithms were originally developed for homogeneous\\ngraphs, and because it is easier to identify relationships between quantities of the same type. Also, in\\nmany cases, the focus is on multivariate series where the variables of interest and the target quantities\\nare inherently of the same nature.\\nSQ7 (Metrics). Regarding the error metrics, their choice is highly dependent on the specific case\\nstudy. However, the most common ones are mean absolute error (MAE), mean squared error (MSE),\\nand mean absolute percentage error (MAPE) for forecasting problems, and accuracy for classification\\nproblems.\\n7 Limits, challenges, and future research directions\\nIn this section the limitations and challenges of spatio-temporal GNN modeling will be discussed,\\ntogether with directions for further research.\\nComparability. As highlighted in Ref. [132], the evaluation of GNN models has improved thanks\\nto the introduction of the Open Graph Benchmark (OGB) [67], which provides a standardized evalua-\\ntion framework and a variety of benchmark graph datasets. However, there is currently no standardized\\nbenchmark for spatio-temporal GNNs. As a result, each model is evaluated on its own selection of\\ndatasets, and this fragmentation makes it difficult to compare the results of different studies. To\\nalleviate this problem, this review presented all the information gathered from the selected papers,\\nincluding datasets, benchmarks, codes, and tables of results. It is intended to serve as a detailed\\noverview and a foundation for further exploration, in the hope that researchers will begin to examine\\nthe data collected in the presented tables and, over time, identify relevant datasets and benchmark\\nmodels.\\nReproducibility. The limited availability of links to repositories, source code, and datasets makes\\nit difficult to evaluate the progress of knowledge in spatio-temporal GNNs. Moreover, in many papers\\nthe authors do not give detailed information about the model and the graph structure, like definition\\nof the graph, number of nodes, calculation of edge weights. This issues complicate the verification of\\nresults and the assessment of the reproducibility of experiments, which are essential to drive further\\n49'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 49}, page_content='research in this field. Without access to these fundamental resources, it is extremely difficult to assess\\nthe accuracy and significance of the proposed models.\\nExplainability. A crucial aspect of GNN models is explainability, intended as the ability to\\ninterpret and understand their decision-making processes. Despite their attempt to explicitly model\\nspatial relationships between series, GNNs are often considered as ”black box” models. None of\\nthe selected papers adequately tackle the concept of explainability. However, recent literature is\\nbeginning to investigate this important topic. For instance, few papers studied perturbation-based\\nexplanation methods, which however have proven to be somewhat ineffective as reported in Ref. [180].\\nOther approaches, such as GNNExplainer [215], aim to identify a small subset of node features that\\nhave a crucial role in GNN’s predictions. Recently, some researchers have started to explore graph\\ncounterfactuals as a means of generating explanations, as reviewed in Ref. [153].\\nPoor information capacity. Another limitation of GNNs lies in the poor information behind\\nthe design of the graph structure and the model definition. A poorly constructed graph with unrep-\\nresentative or overly sparse connections, as well as a lack of physical constraints in the equations of\\nthe model, can significantly affect the performance. Although efforts have been made to address these\\nissues, such as integrating physical constraints through differential equations, these approaches are not\\nalways effective. Therefore, it is essential to develop new techniques to overcome these limitations.\\nHeterogeneity. The majority of current GNN models are designed to deal with homogeneous\\ngraphs, where nodes and edges are all of the same type. As a result, it is challenging to use these GNNs\\nwith heterogeneous graphs, which have different types of nodes and edges or different inputs. Therefore,\\nfurther research is necessary to develop GNN models that can effectively capture the interactions\\nbetween various types of nodes and edges.\\nScalability. Spatio-temporal GNNs are widely used for modeling and analyzing large and complex\\ntime series-based network structures. However, GNN models often require a significant amount of\\nmemory to compute the adjacency matrix and node embeddings, especially in the case of dynamic\\ngraph structures. This scalability challenge results in high computational costs, and in many cases the\\nnecessity to use significant GPU resources. Therefore, the development of more scalable GNN models\\nis crucial to facilitate time series analysis even in environments with limited computing power.\\n8 Conclusions\\nThis paper presented the results of a SLR on the application of spatio-temporal GNN models to time\\nseries classification and forecasting problems in different fields. Lately, GNNs have gained significant\\npopularity due to their ability to process graph-structured data. This has led in more recent years to\\nthe development of spatio-temporal GNNs in the field of time series analysis, due to their ability to\\nmodel dependencies between variables and across time points.\\nThis SLR has brought forward on two set of questions: generic questions which can be answered\\nat the level of bibliographic overview, and more specific questions regarding particular aspects of\\nthe proposed spatio-temporal GNN models. By answering these questions, it has emerged that the\\nmajority of the models in the selected papers is characterized by a convolutional spatial aggregation.\\nHowever, several graph attentional models are also emerging. In addition, while the majority of the\\nselected papers focus on models with a pre-defined graph structure, an increasing number of studies,\\nparticularly in the ”Generic” group, are beginning to develop models that learn the graph structure\\nautonomously.\\nA first key point which results from the presented overview is that the current literature on spatio-\\ntemporal GNNs is very fragmented, as it was pictorially reported in Fig. 4. This can be attributed to\\nthe fact that the involved researchers come from different communities with focus on specific application\\ndomains. As a result, there seems to be a lack of standardised datasets or benchmarks. The objective\\nof this review was also to collect information on datasets, proposed models, links to source codes,\\nbenchmarks, and results, in order to provide a foundation for future studies. A second major key\\npoint resulting from the proposed SLR seems hence to be that there is a need for the GNN research\\ncommunity to work on common datasets and develop comparable and reproducible models. This would\\nenhance transparency and make it easier to evaluate advancements in the field and to share them more\\neasily.\\n50'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 50}, page_content='References\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G.S. Corrado, A. Davis,\\nJ. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´ e, R. Monga, S. Moore, D. Mur-\\nray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Van-\\nhoucke, V. Vasudevan, F. Vi´ egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,\\nand X. Zheng: Tensorflow: Large-scale machine learning on heterogeneous systems , 2015.\\nhttps://www.tensorflow.org/, Software available from tensorflow.org.\\n[2] A. Agafonov: Short-term traffic data forecasting: A deep learning approach . Optical Memory\\nand Neural Networks (Information Optics), 30(1), 2021. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85104539673&doi=10.3103%2fS1060992X21010021&partnerID=40&md5=44\\nb73a773260f9da8cbbf782470d9ea5.\\n[3] H. Bai, J. Yang, M. Huang, and W. Li: A symmetric adaptive visibility graph classification\\nmethod of orthogonal signals for automatic modulation classification . IET Communications,\\n17(10):1208–1219, Apr. 2023, ISSN 1751-8636. http://dx.doi.org/10.1049/cmu2.12608.\\n[4] L. Bai, L. Yao, C. Li, X. Wang, and C. Wang: Adaptive graph convolutional recurrent net-\\nwork for traffic forecasting . In Proceedings of the 34th International Conference on Neural In-\\nformation Processing Systems , NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc.,\\nISBN 9781713829546.\\n[5] S. Bai, J.Z. Kolter, and V. Koltun: An empirical evaluation of generic convolutional and recurrent\\nnetworks for sequence modeling. ArXiv, abs/1803.01271, 2018. https://api.semanticscholar.org/\\nCorpusID:4747877.\\n[6] T. Bai and P. Tahmasebi: Graph neural network for groundwater level forecasting . Journal of\\nHydrology, 616:128792, Nov. 2022.\\n[7] L.Ø. Bentsen, N.D. Warakagoda, R. Stenbro, and P. Engelstad: Spatio-temporal wind speed\\nforecasting using graph networks and novel transformer architectures . Applied Energy, 333,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144917861&doi=10.1016%2fj.a\\npenergy.2022.120565&partnerID=40&md5=ffc8c8a1fb27fe716f687927e4d3d73a.\\n[8] L.Ø. Bentsen, N.D. Warakagoda, R. Stenbro, and P. Engelstad: A unified graph formulation for\\nspatio-temporal wind forecasting. Energies, 16(20), 2023. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85175042962&doi=10.3390%2fen16207179&partnerID=40&md5=ac64d6e4a\\n268cf9dad20c2af1c4bb39e.\\n[9] R. Bing, G. Yuan, M. Zhu, F. Meng, H. Ma, and S. Qiao: Heterogeneous graph neural net-\\nworks analysis: a survey of techniques, evaluations and applications . Artif. Intell. Rev.,\\n56(8):8003–8042, dec 2022, ISSN 0269-2821. https://doi.org/10.1007/s10462-022-10375-2.\\n[10] S. Bloemheuvel, J. van den Hoogen, and M. Atzmueller: Graph construction on complex spa-\\ntiotemporal data for enhancing graph neural network-based approaches . International Journal of\\nData Science and Analytics, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851\\n72328971&doi=10.1007%2fs41060-023-00452-2&partnerID=40&md5=76a1c44d50e1df063ff8f6\\ne71dc83f77.\\n[11] S. Bloemheuvel, J. van den Hoogen, D. Jozinovi´ c, A. Michelini, and M. Atzmueller:Graph neural\\nnetworks for multivariate time series regression with application to seismic data . International\\nJournal of Data Science and Analytics, 16(3):317 – 332, 2023. https://www.scopus.com/inwar\\nd/record.uri?eid=2-s2.0-85137122128&doi=10.1007%2fs41060-022-00349-6&partnerID=40&md\\n5=263390c81b83a2a1be9799a0587ca192.\\n[12] S. Brody, U. Alon, and E. Yahav: How attentive are graph attention networks? ArXiv,\\nabs/2105.14491, 2021. https://api.semanticscholar.org/CorpusID:235254358.\\n51'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 51}, page_content='[13] K.H.N. Bui, J. Cho, and H. Yi: Spatial-temporal graph neural network for traffic forecasting:\\nAn overview and open research issues . Applied Intelligence, 52(3):2763 – 2774, 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85124804317&doi=10.1007%2fs10489-021-0\\n2587-w&partnerID=40&md5=3344390f0ecfc943b8e053fbc7009b08.\\n[14] K. Cai, Y. Li, Y.P. Fang, and Y. Zhu: A deep learning approach for flight delay prediction through\\ntime-evolving graphs. IEEE Transactions on Intelligent Transportation Systems, 23(8):11397 –\\n11407, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136088717&doi=10.11\\n09%2fTITS.2021.3103502&partnerID=40&md5=4ee70282a27e20ea83814c8d9bc1f405.\\n[15] Y. Cao, D. Liu, Q. Yin, F. Xue, and H. Tang: Msasgcn: Multi-head self-attention spatiotemporal\\ngraph convolutional network for traffic flow forecasting . Journal of Advanced Transportation,\\n2022, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133003671&doi=10.1155\\n%2f2022%2f2811961&partnerID=40&md5=4df5ab09ac6da0385b1f406983ee2be9.\\n[16] R.E. Carrillo, M. Leblanc, B. Schubnel, R. Langou, C. Topfel, and P.J. Alet: High-resolution pv\\nforecasting from imperfect data: A graph-based solution. Energies, 13(21), 2020, ISSN 1996-1073.\\nhttps://www.mdpi.com/1996-1073/13/21/5763.\\n[17] G. Chen, Y. Guo, Q. Zeng, and Y. Zhang: A novel cellular network traffic prediction algorithm\\nbased on graph convolution neural networks and long short-term memory through extraction of\\nspatial-temporal characteristics. Processes, 11(8), 2023. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85169154539&doi=10.3390%2fpr11082257&partnerID=40&md5=f719b6c1b\\nb7c7936c4f153c52656f227.\\n[18] H. Chen and H. Eldardiry: Graph time-series modeling in deep learning: A survey . ACM Trans.\\nKnowl. Discov. Data, 18(5), feb 2024, ISSN 1556-4681. https://doi.org/10.1145/3638534.\\n[19] H. Chen, R.A. Rossi, K. Mahadik, S. Kim, and H. Eldardiry: Graph deep factors for probabilistic\\ntime-series forecasting. ACM Trans. Knowl. Discov. Data, 17(2), feb 2023, ISSN 1556-4681.\\nhttps://doi.org/10.1145/3543511.\\n[20] K. Chen, F. Chen, B. Lai, Z. Jin, Y. Liu, K. Li, L. Wei, P. Wang, Y. Tang, J. Huang, and X.S.\\nHua: Dynamic spatio-temporal graph-based cnns for traffic flow prediction. IEEE Access, 8:185136\\n– 185145, 2020. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098607394&doi=10.\\n1109%2fACCESS.2020.3027375&partnerID=40&md5=c5d36eecef2555da4e292f99ef1cb600.\\n[21] L. Chen, D. Chen, Z. Shang, B. Wu, C. Zheng, B. Wen, and W. Zhang: Multi-scale adaptive\\ngraph neural network for multivariate time series forecasting . IEEE Transactions on Knowledge\\nand Data Engineering, 35(10):10748–10761, 2023.\\n[22] L. Chen, J. Xu, B. Wu, and J. Huang: Group-aware graph neural network for nationwide city\\nair quality forecasting . ACM Trans. Knowl. Discov. Data, 18(3), dec 2023, ISSN 1556-4681.\\nhttps://doi.org/10.1145/3631713.\\n[23] T. Chen and C. Guestrin: Xgboost: A scalable tree boosting system . In Proceedings of the\\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ,\\nKDD ’16, p. 785–794, New York, NY, USA, 2016. Association for Computing Machinery,\\nISBN 9781450342322. https://doi.org/10.1145/2939672.2939785.\\n[24] W. Chen, L. Chen, Y. Xie, W. Cao, Y. Gao, and X. Feng: Multi-range attentive bicomponent\\ngraph convolutional network for traffic forecasting . Proceedings of the AAAI Conference on\\nArtificial Intelligence, 34(04):3529–3536, Apr. 2020. https://ojs.aaai.org/index.php/AAAI/arti\\ncle/view/5758.\\n[25] X. Chen and M. Zeng: Convolution-graph attention network with sensor embeddings for remaining\\nuseful life prediction of turbofan engines . IEEE Sensors Journal, 23(14):15786 – 15794, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85162688641&doi=10.1109%2fJSEN.20\\n23.3279365&partnerID=40&md5=cc06b102724af71e03e41bdebaaaddda.\\n52'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 52}, page_content='[26] Y. Chen, F. Ding, and L. Zhai: Multi-scale temporal features extraction based graph convolutional\\nnetwork with attention for multivariate time series prediction. Expert Systems with Applications,\\n200, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127194643&doi=10.1016\\n%2fj.eswa.2022.117011&partnerID=40&md5=dac99df11794f7278ec0ff4a6b1704f1.\\n[27] Y. Chen and Z. Xie: Multi-channel fusion graph neural network for multivariate time series\\nforecasting. Journal of Computational Science, 64, 2022. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85139025049&doi=10.1016%2fj.jocs.2022.101862&partnerID=40&md5=423\\n91def56db7fa952dfe1bb329da4ab.\\n[28] D. Cheng, F. Yang, S. Xiang, and J. Liu: Financial time series forecasting with multi-modality\\ngraph neural network. Pattern Recognition, 121, 2022. https://www.scopus.com/inward/record\\n.uri?eid=2-s2.0-85112022477&doi=10.1016%2fj.patcog.2021.108218&partnerID=40&md5=1ec\\ne9d9d7796d076f08dfd3910ae76bc.\\n[29] J. Cheng, K. Huang, and Z. Zheng: Towards better forecasting by fusing near and distant future\\nvisions. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):3593–3600, Apr.\\n2020, ISSN 2159-5399. http://dx.doi.org/10.1609/aaai.v34i04.5766.\\n[30] W. Chung, J. Moon, D. Kim, and E. Hwang: Graph construction method for gnn-based multi-\\nvariate time-series forecasting . Computers, Materials and Continua, 75(3):5817 – 5836, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85165540972&doi=10.32604%2fcmc.20\\n23.036830&partnerID=40&md5=b5867582cb82c9f892b79183cc41e585.\\n[31] A. Cini and I. Marisca: Torch Spatiotemporal, Mar. 2022. https://github.com/TorchSpatiotemp\\noral/tsl, License: MIT.\\n[32] A. Cini, D. Zambon, and C. Alippi: Sparse graph learning from spatiotemporal time series .\\nJournal of Machine Learning Research, 24, 2023. https://www.scopus.com/inward/record.uri?e\\nid=2-s2.0-85175601404&partnerID=40&md5=e8cf0bc88a664da0ef171127c7a14f0d.\\n[33] S. Dai, J. Wang, C. Huang, Y. Yu, and J. Dong: Dynamic multi-view graph neural networks for\\ncitywide traffic inference. ACM Trans. Knowl. Discov. Data, 17(4), feb 2023, ISSN 1556-4681.\\nhttps://doi.org/10.1145/3564754.\\n[34] Y. Dai, J. Lai, X. Xu, J. Xiahou, J. Lian, and Z. Zhang: Multi-view short-term photovoltaic\\npower prediction combining satellite images feature learning and graph mutual information feature\\nrepresentation. IET Computer Vision, 2023. https://www.scopus.com/inward/record.uri?eid=2\\n-s2.0-85147499586&doi=10.1049%2fcvi2.12174&partnerID=40&md5=a25051ff9ae58912e6c68bc\\n3895250fd.\\n[35] J. Deng, X. Chen, R. Jiang, X. Song, and I.W. Tsang: St-norm: Spatial and temporal nor-\\nmalization for multi-variate time series forecasting . In Proceedings of the 27th ACM SIGKDD\\nConference on Knowledge Discovery & Data Mining, KDD ’21, p. 269–278, New York, NY, USA,\\n2021. Association for Computing Machinery, ISBN 9781450383325. https://doi.org/10.1145/34\\n47548.3467330.\\n[36] C. Diao, D. Zhang, W. Liang, K.C. Li, Y. Hong, and J.L. Gaudiot:A novel spatial-temporal multi-\\nscale alignment graph neural network security model for vehicles prediction . IEEE Transactions\\non Intelligent Transportation Systems, 24(1):904 – 914, 2023. https://www.scopus.com/inwar\\nd/record.uri?eid=2-s2.0-85123379822&doi=10.1109%2fTITS.2022.3140229&partnerID=40&md\\n5=4e19dd4617a6a5bff882d03f1e00dc31.\\n[37] Z. Diao, G. Xie, X. Wang, R. Ren, X. Meng, G. Zhang, K. Xie, and M. Qiao:Ec-gcn: A encrypted\\ntraffic classification framework based on multi-scale graph convolution networks . Computer Net-\\nworks, 224, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148540984&doi=1\\n0.1016%2fj.comnet.2023.109614&partnerID=40&md5=544b1d43ba5bbf943c358da881a99fdf.\\n[38] H. Ding, Y. Lu, N. Sze, and H. Li: Effect of dockless bike-sharing scheme on the demand for lon-\\ndon cycle hire at the disaggregate level using a deep learning approach . Transportation Research\\nPart A: Policy and Practice, 166:150 – 163, 2022. https://www.scopus.com/inward/record.uri\\n53'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 53}, page_content='?eid=2-s2.0-85143767347&doi=10.1016%2fj.tra.2022.10.013&partnerID=40&md5=de898bf6a7\\ndd160fc4de8c56504639d2.\\n[39] H. Ding and G. Noh: A hybrid model for spatiotemporal air quality prediction based on in-\\nterpretable neural networks and a graph neural network . Atmosphere, 14(12), 2023. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85180550137&doi=10.3390%2fatmos141218\\n07&partnerID=40&md5=a5ad7d78eb9fd57308e7e9f6301ee0d8.\\n[40] D. Dong, S. Wang, Q. Guo, X. Li, W. Zou, and Z. You: Ocean wind speed prediction based on the\\nfusion of spatial clustering and an improved residual graph attention network . Journal of Marine\\nScience and Engineering, 11:2350, Dec. 2023.\\n[41] Z. Duan, H. Xu, Y. Huang, J. Feng, and Y. Wang: Multivariate time series forecasting with\\ntransfer entropy graph. Tsinghua Science and Technology, 28(1):141 – 149, 2023. https://www.\\nscopus.com/inward/record.uri?eid=2-s2.0-85135310646&doi=10.26599%2fTST.2021.9010081\\n&partnerID=40&md5=c3c088c1b79b24b218bf757061f0df5b.\\n[42] N.J. van Eck and L. Waltman: Software survey: VOSviewer, a computer program for bibliometric\\nmapping. Scientometrics, 84(2):523–538, Aug. 2010.\\n[43] Z. Fang, Q. Long, G. Song, and K. Xie: Spatial-temporal graph ode networks for traffic flow\\nforecasting. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &\\nData Mining , KDD ’21, p. 364–373, New York, NY, USA, 2021. Association for Computing\\nMachinery, ISBN 9781450383325. https://doi.org/10.1145/3447548.3467430.\\n[44] M. Fey and J.E. Lenssen: Fast graph representation learning with PyTorch Geometric . In ICLR\\nWorkshop on Representation Learning on Graphs and Manifolds , 2019.\\n[45] M. Ganjouri, M. Moattari, A. Forouzantabar, and M. Azadi: Spatial-temporal learning structure\\nfor short-term load forecasting. IET Generation, Transmission and Distribution, 17(2):427 – 437,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143894086&doi=10.1049%2fg\\ntd2.12684&partnerID=40&md5=17423066d04adbbce9774c9afd3a8466.\\n[46] Z. Gao, Z. Li, L. Xu, and J. Yu: Dynamic adaptive spatio-temporal graph neural network for\\nmulti-node offshore wind speed forecasting. Applied Soft Computing, 141:110294, Apr. 2023.\\n[47] Z. Gao, Z. Li, J. Yu, and L. Xu: Global spatiotemporal graph attention network for sea surface\\ntemperature prediction. IEEE Geoscience and Remote Sensing Letters, 20, 2023. https://www.\\nscopus.com/inward/record.uri?eid=2-s2.0-85149383269&doi=10.1109%2fLGRS.2023.3250237\\n&partnerID=40&md5=95e16ad0cf0a26e824391312a2fb45d5.\\n[48] Z. Gao, Z. Li, H. Zhang, J. Yu, and L. Xu: Dynamic spatiotemporal interactive graph neural\\nnetwork for multivariate time series forecasting . Knowledge-Based Systems, 280, 2023. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85172002920&doi=10.1016%2fj.knosys.2023\\n.110995&partnerID=40&md5=2be2a784e3e84d3eec5db6790012fa47.\\n[49] X. Geng, L. Xu, X. He, and J. Yu: Graph optimization neural network with spatio-temporal\\ncorrelation learning for multi-node offshore wind speed forecasting. Renewable Energy, 180:1014\\n– 1025, 2021. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114689825&doi=10.10\\n16%2fj.renene.2021.08.066&partnerID=40&md5=ae0660246216bdf8c2ab1ee34bd108af.\\n[50] A. Ghaderi, B.M. Sanandaji, and F. Ghaderi: Deep forecast: Deep learning-based spatio-temporal\\nforecasting. ArXiv, abs/1707.08110, 2017. https://api.semanticscholar.org/CorpusID:10435935.\\n[51] M. Gori, G. Monfardini, and F. Scarselli: A new model for learning in graph domains . In\\nProceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. , vol. 2, pp.\\n729–734 vol. 2, 2005.\\n[52] A. Graves and J. Schmidhuber: Framewise phoneme classification with bidirectional lstm net-\\nworks. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005. ,\\nvol. 4, pp. 2047–2052 vol. 4, 2005.\\n54'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 54}, page_content='[53] A. Grover and J. Leskovec: node2vec: Scalable feature learning for networks . In Proceedings\\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Min-\\ning, KDD ’16, p. 855–864, New York, NY, USA, 2016. Association for Computing Machinery,\\nISBN 9781450342322. https://doi.org/10.1145/2939672.2939754.\\n[54] J. Gu, Z. Jia, T. Cai, X. Song, and A. Mahmood: Dynamic correlation adjacency-matrix-based\\ngraph neural networks for traffic flow prediction . Sensors, 23(6), 2023. https://www.scopus.com\\n/inward/record.uri?eid=2-s2.0-85151047271&doi=10.3390%2fs23062897&partnerID=40&md5=\\nb338af2918a8766408479a1bbdf89d16.\\n[55] Y. Gu and L. Deng: Stagcn: Spatial–temporal attention graph convolution network for traffic\\nforecasting. Mathematics, 10(9), 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.\\n0-85131314499&doi=10.3390%2fmath10091599&partnerID=40&md5=3bad5037359e12fe5716c\\n0ec738de8df.\\n[56] K. Guo, Y. Hu, Y. Sun, S. Qian, J. Gao, and B. Yin: Hierarchical graph convolution network for\\ntraffic forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):151–159,\\nMay 2021, ISSN 2159-5399. http://dx.doi.org/10.1609/aaai.v35i1.16088.\\n[57] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan: Attention based spatial-temporal graph convo-\\nlutional networks for traffic flow forecasting . Proceedings of the AAAI Conference on Artificial\\nIntelligence, 33(01):922–929, July 2019, ISSN 2159-5399. http://dx.doi.org/10.1609/aaai.v33i01\\n.3301922.\\n[58] T. Guo, F. Hou, Y. Pang, X. Jia, Z. Wang, and R. Wang: Learning and integration of adaptive\\nhybrid graph structures for multivariate time series forecasting . Information Sciences, 648, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803579&doi=10.1016%2fj.ins.202\\n3.119560&partnerID=40&md5=dee1e3f0920bea4ba839ff1fc9fdd8d1.\\n[59] X. Guo, X. Kong, W. Xing, X. Wei, J. Zhang, and W. Lu: Adaptive graph generation based\\non generalized pagerank graph neural network for traffic flow forecasting . Applied Intelligence,\\n53(24):30971 – 30986, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851782014\\n09&doi=10.1007%2fs10489-023-05137-8&partnerID=40&md5=296d355220e0724e134517b26636\\n5088.\\n[60] J. Hamkins and M.K. Simon: Modulation classification, May 2006, ISBN 9780470087800. http:\\n//dx.doi.org/10.1002/9780470087800.ch9.\\n[61] X. Han, Y. Huang, Z. Pan, W. Li, Y. Hu, and G. Lin: Multi-task time series forecasting based\\non graph neural networks . Entropy, 25(8), 2023. https://www.scopus.com/inward/record.uri?e\\nid=2-s2.0-85168860312&doi=10.3390%2fe25081136&partnerID=40&md5=c7384a7e92bb7c4fe6\\n2bac7dd02f4692.\\n[62] D.M. Hawkins: Identification of Outliers . Springer Netherlands, 1980, ISBN 9789401539944.\\nhttp://dx.doi.org/10.1007/978-94-015-3994-4.\\n[63] H. He, F. Fu, and D. Luo: Multiplex parallel gat-alstm: A novel spatial-temporal learning model\\nfor multi-sites wind power collaborative forecasting . Frontiers in Energy Research, 10, 2022.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85137693094&doi=10.3389%2ffenrg.20\\n22.974682&partnerID=40&md5=a3e82a523729602ca7eace4defe611f2.\\n[64] Z. He, C. Zhao, and Y. Huang: Multivariate time series deep spatiotemporal forecasting with\\ngraph neural network. Applied Sciences (Switzerland), 12(11), 2022. https://www.scopus.com/i\\nnward/record.uri?eid=2-s2.0-85132031664&doi=10.3390%2fapp12115731&partnerID=40&md\\n5=ae0fa3f72321ddc86ad64eedc727eeb8.\\n[65] M. Hou, F. Xia, X. Chen, V. Saikrishna, and H. Chen: Adaptive spatio-temporal graph learning\\nfor bus station profiling . ACM Trans. Spatial Algorithms Syst., dec 2023, ISSN 2374-0353.\\nhttps://doi.org/10.1145/3636459, Just Accepted.\\n[66] Y.L. Hsu, Y.C. Tsai, and C.T. Li: Fingat: Financial graph attention networks for recommending\\ntop-k profitable stocks, 2021. https://arxiv.org/abs/2106.10159.\\n55'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 55}, page_content='[67] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec: Open graph\\nbenchmark: Datasets for machine learning on graphs . In H. Larochelle, M. Ranzato, R. Hadsell,\\nM. Balcan, and H. Lin (eds.): Advances in Neural Information Processing Systems , vol. 33, pp.\\n22118–22133. Curran Associates, Inc., 2020. https://proceedings.neurips.cc/paper files/paper\\n/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf.\\n[68] Y. Hu, X. Cheng, S. Wang, J. Chen, T. Zhao, and E. Dai: Times series forecasting for urban\\nbuilding energy consumption based on graph convolutional network . Applied Energy, 307, 2022.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85120306571&doi=10.1016%2fj.apene\\nrgy.2021.118231&partnerID=40&md5=4a08b73905bdc943102b8ee119cbc7e0.\\n[69] K. Huang, X. Li, F. Liu, X. Yang, and W. Yu: Ml-gat:a multilevel graph attention model for stock\\nprediction. IEEE Access, 10:86408–86422, 2022.\\n[70] R. Huang, L. Ma, J. He, and X. Chu: T-gan: A deep learning framework for prediction of temporal\\ncomplex networks with adaptive graph convolution and attention mechanism. Displays, 68:102023,\\n2021, ISSN 0141-9382. https://www.sciencedirect.com/science/article/pii/S0141938221000366.\\n[71] S. Huang, D. Wang, X. Wu, and A. Tang: Dsanet: Dual self-attention network for multivariate\\ntime series forecasting. In Proceedings of the 28th ACM International Conference on Information\\nand Knowledge Management , CIKM ’19, p. 2129–2132, New York, NY, USA, 2019. Association\\nfor Computing Machinery, ISBN 9781450369763. https://doi.org/10.1145/3357384.3358132.\\n[72] W.C. Huang, C.T. Chen, C. Lee, F.H. Kuo, and S.H. Huang: Attentive gated graph sequence\\nneural network-based time-series information fusion for financial trading . Information Fusion,\\n91:261 – 276, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140905775&doi\\n=10.1016%2fj.inffus.2022.10.006&partnerID=40&md5=348fca344088ff14086e56e5f445e688.\\n[73] H. Jiajun, Y. Chuanjin, L. Yongle, and X. Huoyue: Ultra-short term wind prediction with wavelet\\ntransform, deep belief network and ensemble learning . Energy Conversion and Management,\\n205:112418, 2020, ISSN 0196-8904. https://www.sciencedirect.com/science/article/pii/S01968\\n90419314256.\\n[74] R. Jiang, Z. Wang, J. Yong, P. Jeph, Q. Chen, Y. Kobayashi, X. Song, S. Fukushima, and T. Suzu-\\nmura: Spatio-temporal meta-graph learning for traffic forecasting . In Proceedings of the Thirty-\\nSeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative\\nApplications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in\\nArtificial Intelligence, AAAI’23/IAAI’23/EAAI’23. AAAI Press, 2023, ISBN 978-1-57735-880-0.\\nhttps://doi.org/10.1609/aaai.v37i7.25976.\\n[75] R. Jiang, D. Yin, Z. Wang, Y. Wang, J. Deng, H. Liu, Z. Cai, J. Deng, X. Song, and R. Shibasaki:\\nDl-traff: Survey and benchmark of deep learning models for urban traffic prediction . In Pro-\\nceedings of the 30th ACM International Conference on Information & Knowledge Management ,\\nCIKM ’21, p. 4515–4525, New York, NY, USA, 2021. Association for Computing Machinery,\\nISBN 9781450384469. https://doi.org/10.1145/3459637.3482000.\\n[76] W. Jiang and J. Luo: Graph neural network for traffic forecasting: A survey . Expert Systems\\nwith Applications, 207:117921, 2022, ISSN 0957-4174. https://www.sciencedirect.com/science/\\narticle/pii/S0957417422011654.\\n[77] W. Jiang, Y. Xiao, Y. Liu, Q. Liu, and Z. Li: Bi-grcn: A spatio-temporal traffic flow prediction\\nmodel based on graph neural network . Journal of Advanced Transportation, 2022, 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85124792614&doi=10.1155%2f2022%2f5221\\n362&partnerID=40&md5=8d4c93e3d09f3eb19d2d444575866223.\\n[78] J. Jiao, M. Zhao, J. Lin, and K. Liang: Hierarchical discriminating sparse coding for weak fault\\nfeature extraction of rolling bearings. Reliability Engineering and System Safety, 184, Feb. 2018.\\n[79] G. Jin, F. Li, J. Zhang, M. Wang, and J. Huang: Automated dilated spatio-temporal synchronous\\ngraph modeling for traffic prediction. IEEE Transactions on Intelligent Transportation Systems,\\n24(8):8820 – 8830, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136873211\\n56'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 56}, page_content='&doi=10.1109%2fTITS.2022.3195232&partnerID=40&md5=595891f180eaecd989f7c51387005d\\n26.\\n[80] G. Jin, Y. Liang, Y. Fang, Z. Shao, J. Huang, J. Zhang, and Y. Zheng: Spatio-temporal graph\\nneural networks for predictive learning in urban computing: A survey . IEEE Transactions on\\nKnowledge and Data Engineering, pp. 1–20, 2023.\\n[81] G. Jin, C. Liu, Z. Xi, H. Sha, Y. Liu, and J. Huang: Adaptive dual-view wavenet for urban\\nspatial–temporal event prediction. Information Sciences, 588:315 – 330, 2022. https://www.scop\\nus.com/inward/record.uri?eid=2-s2.0-85122370178&doi=10.1016%2fj.ins.2021.12.085&partner\\nID=40&md5=ad5d2f35eb2cadff0957e36e064577a0.\\n[82] M. Jin, Y. Zheng, Y.F. Li, S. Chen, B. Yang, and S. Pan: Multivariate time series forecasting with\\ndynamic graph neural odes. IEEE Transactions on Knowledge and Data Engineering, 35(9):9168\\n– 9180, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142836712&doi=10.11\\n09%2fTKDE.2022.3221989&partnerID=40&md5=3b24722f23228be12c2422c258c679d5.\\n[83] M. Jing, Y. Zhu, Y. Xu, H. Liu, T. Zang, C. Wang, and J. Yu:Learning shared representations for\\nrecommendation with dynamic heterogeneous graph convolutional networks. ACM Trans. Knowl.\\nDiscov. Data, 17(4), feb 2023, ISSN 1556-4681. https://doi.org/10.1145/3565575.\\n[84] W.O. Kermack and A.G. McKendrick: A contribution to the mathematical theory of epidemics .\\nProceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and\\nPhysical Character, 115(772):700–721, 1927, ISSN 2053-9150. http://dx.doi.org/10.1098/rspa.\\n1927.0118.\\n[85] A. Khaled, A.M.T. Elsir, and Y. Shen: Tfgan: Traffic forecasting using generative adversarial\\nnetwork with multi-graph convolutional network . Knowledge-Based Systems, 249, 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85133943024&doi=10.1016%2fj.knosys.2022\\n.108990&partnerID=40&md5=875da936b13a56a1c87fd51cef6638dd.\\n[86] D.Y. Kim, D.Y. Jin, and H.I. Suk: Spatiotemporal graph neural networks for predicting mid-to-\\nlong-term pm2.5 concentrations. Journal of Cleaner Production, 425, 2023. https://www.scop\\nus.com/inward/record.uri?eid=2-s2.0-85171550808&doi=10.1016%2fj.jclepro.2023.138880&par\\ntnerID=40&md5=445659b1403b119cd0a9f87fcf0bd8d7.\\n[87] J. Kim, T. Kim, J.G. Ryu, and J. Kim: Spatiotemporal graph neural network for multivariate\\nmulti-step ahead time-series forecasting of sea temperature. Engineering Applications of Artificial\\nIntelligence, 126, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166650882&d\\noi=10.1016%2fj.engappai.2023.106854&partnerID=40&md5=9b24721b81f02853bf13c19dfa20c\\n84f.\\n[88] J. Kim, H. Lee, S. Yu, U. Hwang, W. Jung, and K. Yoon: Hierarchical joint graph learning and\\nmultivariate time series forecasting . IEEE Access, 11:118386 – 118394, 2023. https://www.scop\\nus.com/inward/record.uri?eid=2-s2.0-85174798168&doi=10.1109%2fACCESS.2023.3325041&p\\nartnerID=40&md5=ceff0865660fb97951c9f1fbf5bb698c.\\n[89] R. Kim, C.H. So, M. Jeong, S. Lee, J. Kim, and J. Kang: Hats: A hierarchical graph attention\\nnetwork for stock movement prediction , 2019. https://arxiv.org/abs/1908.07999.\\n[90] T. Kipf, E. Fetaya, K.C. Wang, M. Welling, and R. Zemel: Neural relational inference for inter-\\nacting systems. In J. Dy and A. Krause (eds.): Proceedings of the 35th International Conference\\non Machine Learning , vol. 80 of Proceedings of Machine Learning Research , pp. 2688–2697.\\nPMLR, 10–15 Jul 2018. https://proceedings.mlr.press/v80/kipf18a.html.\\n[91] T.N. Kipf and M. Welling: Semi-supervised classification with graph convolutional networks, 2017.\\nhttps://arxiv.org/abs/1609.02907.\\n[92] N. Kitaev,  Lukasz Kaiser, and A. Levskaya: Reformer: The efficient transformer , 2020. https:\\n//arxiv.org/abs/2001.04451.\\n57'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 57}, page_content='[93] Z. Kong, X. Jin, Z. Xu, and B. Zhang: Spatio-temporal fusion attention: A novel approach for\\nremaining useful life prediction based on graph neural network . IEEE Transactions on Instru-\\nmentation and Measurement, 71, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.\\n0-85133611731&doi=10.1109%2fTIM.2022.3184352&partnerID=40&md5=b2e8f405827ac977d\\n34217de70f0863c.\\n[94] R. Kumar, J. Mendes Moreira, and J. Chandra: Dygcn-lstm: A dynamic gcn-lstm based encoder-\\ndecoder framework for multistep traffic prediction . Applied Intelligence, 53(21):25388 – 25411,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167340209&doi=10.1007%2fs\\n10489-023-04871-3&partnerID=40&md5=798e1790ca84b22703943c7134a720ac.\\n[95] M. Lablack and Y. Shen: Spatio-temporal graph mixformer for traffic forecasting. Expert Systems\\nwith Applications, 228, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8515910\\n1478&doi=10.1016%2fj.eswa.2023.120281&partnerID=40&md5=9c83ec232aa2eff7a61557294321\\n45dc.\\n[96] L. Lacasa, B. Luque, F. Ballesteros, J. Luque, and J.C. Nu˜ no: From time series to complex net-\\nworks: The visibility graph . Proceedings of the National Academy of Sciences, 105(13):4972–4975,\\nApr. 2008, ISSN 1091-6490. http://dx.doi.org/10.1073/pnas.0709247105.\\n[97] G. Lai, W.C. Chang, Y. Yang, and H. Liu: Modeling long- and short-term temporal patterns\\nwith deep neural networks . In The 41st International ACM SIGIR Conference on Research\\n& Development in Information Retrieval , SIGIR ’18, p. 95–104, New York, NY, USA, 2018.\\nAssociation for Computing Machinery, ISBN 9781450356572. https://doi.org/10.1145/3209978.\\n3210006.\\n[98] S. Lan, Y. Ma, W. Huang, W. Wang, H. Yang, and P. Li: DSTAGNN: Dynamic spatial-temporal\\naware graph neural network for traffic flow forecasting . In K. Chaudhuri, S. Jegelka, L. Song,\\nC. Szepesvari, G. Niu, and S. Sabato (eds.): Proceedings of the 39th International Conference\\non Machine Learning , vol. 162 of Proceedings of Machine Learning Research, pp. 11906–11917.\\nPMLR, 17–23 Jul 2022. https://proceedings.mlr.press/v162/lan22a.html.\\n[99] F. Lei, X. Luo, Z. Chen, and H. Zhou: Signal feature extract based on dual-channel wavelet\\nconvolutional network mixed with hypergraph convolutional network for fault diagnosis . IEEE\\nSensors Journal, 23(22):28378–28389, 2023.\\n[100] C. Li, L. Mo, and R. Yan: Fault diagnosis of rolling bearing based on whvg and gcn . IEEE\\nTransactions on Instrumentation and Measurement, PP:1–1, June 2021.\\n[101] D. Li, W. Zhao, J. Hu, S. Zhao, and S. Liu: A long-term water quality prediction model for\\nmarine ranch based on time-graph convolutional neural network . Ecological Indicators, 154,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169001765&doi=10.1016%2fj.e\\ncolind.2023.110782&partnerID=40&md5=f98027fd5241681bce48d53d2137b9db.\\n[102] F. Li, J. Feng, H. Yan, D. Jin, and Y. Li:Crowd flow prediction for irregular regions with semantic\\ngraph attention network . ACM Trans. Intell. Syst. Technol., 13(5), jun 2022, ISSN 2157-6904.\\nhttps://doi.org/10.1145/3501805.\\n[103] F. Li, J. Feng, H. Yan, G. Jin, F. Yang, F. Sun, D. Jin, and Y. Li: Dynamic graph convolutional\\nrecurrent network for traffic prediction: Benchmark and solution . ACM Trans. Knowl. Discov.\\nData, 17(1), feb 2023, ISSN 1556-4681. https://doi.org/10.1145/3532611.\\n[104] G. Li, C. Xiong, A. Thabet, and B. Ghanem: Deepergcn: All you need to train deeper gcns, 2020.\\nhttps://arxiv.org/abs/2006.07739.\\n[105] H. Li: Short-term wind power prediction via spatial temporal analysis and deep residual networks.\\nFrontiers in Energy Research, 10, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.\\n0-85130333390&doi=10.3389%2ffenrg.2022.920407&partnerID=40&md5=a4e2502a85c2d669b78\\nc54adec019d84.\\n58'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 58}, page_content='[106] H. Li, D. Jin, X. Li, H. Huang, J. Yun, and L. Huang: Multi-view spatial-temporal graph neural\\nnetwork for traffic prediction . Computer Journal, 66(10):2393 – 2408, 2023. https://www.scop\\nus.com/inward/record.uri?eid=2-s2.0-85150044802&doi=10.1093%2fcomjnl%2fbxac086&partne\\nrID=40&md5=63074182feabe4eb53bde812d1126bfe.\\n[107] H. Li, D. Jin, X. Li, J. Huang, X. Ma, J. Cui, D. Huang, S. Qiao, and J. Yoo: Dmgf-net: An\\nefficient dynamic multi-graph fusion network for traffic prediction . ACM Trans. Knowl. Discov.\\nData, 17(7), apr 2023, ISSN 1556-4681. https://doi.org/10.1145/3586164.\\n[108] H. Li, X. Li, L. Su, D. Jin, J. Huang, and D. Huang: Deep spatio-temporal adaptive 3d convolu-\\ntional neural networks for traffic flow prediction . ACM Trans. Intell. Syst. Technol., 13(2), jan\\n2022, ISSN 2157-6904. https://doi.org/10.1145/3510829.\\n[109] H. Li, X. Wang, Z. Yang, S. Ali, N. Tong, and S. Baseer: Correlation-based anomaly detection\\nmethod for multi-sensor system. Computational Intelligence and Neuroscience, 2022, 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85131710989&doi=10.1155%2f2022%2f4756\\n480&partnerID=40&md5=f45613d88b19e0c7b4dc1430d27da950.\\n[110] J. Li, J. Crooks, J. Murdock, P. de Souza, K. Hohsfield, B. Obermann, and T. Stockman: A\\nnested machine learning approach to short-term pm2.5 prediction in metropolitan areas using\\npm2.5 data from different sensor networks . Science of the Total Environment, 873, 2023. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85148685120&doi=10.1016%2fj.scitotenv.202\\n3.162336&partnerID=40&md5=e85075a817b43dd06dcc4bdce0c460a2.\\n[111] J. Li and X. Yao: Corporate investment prediction using a weighted temporal graph neural net-\\nwork. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12(6), 2022.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85133454187&doi=10.1002%2fwidm.14\\n72&partnerID=40&md5=22329c873f6e7eaed67d953223cb0def.\\n[112] K. Li, T. Zhang, W. Dong, and H. Ye: Abnormality detection of blast furnace ironmaking process\\nbased on an improved diffusion convolutional gated recurrent unit network . IEEE Transactions\\non Instrumentation and Measurement, 72:1–12, 2023.\\n[113] L. Li, N. Lv, and W. Li: Research on application of graph neural network in water quality\\nprediction. International Journal on Artificial Intelligence Tools, 31(1), 2022. https://www.scop\\nus.com/inward/record.uri?eid=2-s2.0-85125598654&doi=10.1142%2fS021821302250018X&par\\ntnerID=40&md5=c5b366dea706a8ec3fc2012ccc64cf87.\\n[114] M. Li and Z. Zhu: Spatial-temporal fusion graph neural networks for traffic flow forecasting .\\nProceedings of the AAAI Conference on Artificial Intelligence, 35(5):4189–4196, May 2021. https:\\n//ojs.aaai.org/index.php/AAAI/article/view/16542.\\n[115] T. Li, Z. Zhao, C. Sun, R. Yan, and X. Chen: Hierarchical attention graph convolutional network\\nto fuse multi-sensor signals for remaining useful life prediction . Reliability Engineering and\\nSystem Safety, 215:107878, Nov. 2021, ISSN 0951-8320. http://dx.doi.org/10.1016/j.ress.2021.1\\n07878.\\n[116] X. Li, Z. Wang, X. Chen, B. Guo, and Z. Yu: A hybrid continuous-time dynamic graph represen-\\ntation learning model by exploring both temporal and repetitive information . ACM Trans. Knowl.\\nDiscov. Data, 17(9), jun 2023, ISSN 1556-4681. https://doi.org/10.1145/3596447.\\n[117] Y. Li, K. Li, C. Chen, X. Zhou, Z. Zeng, and K. Li: Modeling temporal patterns with dilated\\nconvolutions for time-series forecasting . ACM Trans. Knowl. Discov. Data, 16(1), July 2021,\\nISSN 1556-4681. https://doi.org/10.1145/3453724.\\n[118] Y. Li, Y. Wang, and K. Ma: Integrating transformer and gcn for covid-19 forecasting. Sustainabil-\\nity (Switzerland), 14(16), 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8513768\\n0940&doi=10.3390%2fsu141610393&partnerID=40&md5=981a36ae933a31ffd23628ffd8bf8c59.\\n[119] Y. Li, R. Yu, C. Shahabi, and Y. Liu: Diffusion convolutional recurrent neural network: Data-\\ndriven traffic forecasting. arXiv: Learning, 2017. https://api.semanticscholar.org/CorpusID:\\n3508727.\\n59'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 59}, page_content='[120] Z. Li, J. Yu, G. Zhang, and L. Xu: Dynamic spatio-temporal graph network with adaptive prop-\\nagation mechanism for multivariate time series forecasting . Expert Systems with Applications,\\n216, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144433062&doi=10.1016\\n%2fj.eswa.2022.119374&partnerID=40&md5=d596c82c52cf7b8e22d44605df474fb8.\\n[121] Z.L. Li, G.W. Zhang, J. Yu, and L.Y. Xu: Dynamic graph structure learning for multivariate\\ntime series forecasting. Pattern Recognition, 138, 2023. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85149751108&doi=10.1016%2fj.patcog.2023.109423&partnerID=40&md5=9\\n52ffbe39750e9f0aacfcabfa5bf8e4f.\\n[122] Y. Liang, F. Ding, G. Huang, and Z. Zhao: Deep trip generation with graph neural networks for\\nbike sharing system expansion . Transportation Research Part C: Emerging Technologies, 154,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166619924&doi=10.1016%2fj.t\\nrc.2023.104241&partnerID=40&md5=46e66f959812ce430c802bced3ff3ec9.\\n[123] Y. Liang, S. Ke, J. Zhang, X. Yi, and Y. Zheng: Geoman: multi-level attention networks for\\ngeo-sensory time series prediction. In Proceedings of the 27th International Joint Conference on\\nArtificial Intelligence, IJCAI’18, p. 3428–3434. AAAI Press, 2018, ISBN 9780999241127.\\n[124] W. Liao, B. Zeng, J. Liu, P. Wei, and X. Cheng: Taxi demand forecasting based on the temporal\\nmultimodal information fusion graph neural network. Applied Intelligence, 52(10):12077 – 12090,\\n2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123916554&doi=10.1007%2fs\\n10489-021-03128-1&partnerID=40&md5=66741b796d2c7159d1366f22a0cc24ef.\\n[125] H. Lin, M. Yan, X. Ye, D. Fan, S. Pan, W. Chen, and Y. Xie: A comprehensive survey on\\ndistributed training of graph neural networks . Proceedings of the IEEE, 111(12):1572–1606,\\n2023.\\n[126] H. Lira, L. Mart´ ı, and N. Sanchez-Pi:A graph neural network with spatio-temporal attention for\\nmulti-sources time series data: An application to frost forecast . Sensors, 22(4), 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85124480934&doi=10.3390%2fs22041486&p\\nartnerID=40&md5=67a1c98bc66d6a85ea773efa9f86ee7f.\\n[127] J. Liu, G. Guo, and X. Jiang: Bayesian combination approach to traffic forecasting with graph\\nattention network and arima model . IEEE Access, 11:94732–94741, 2023.\\n[128] J. Liu, X. Wang, H. Lin, and F. Yu: Gsaa: A novel graph spatiotemporal attention algorithm\\nfor smart city traffic prediction . ACM Trans. Sen. Netw., nov 2023, ISSN 1550-4859. https:\\n//doi.org/10.1145/3631608, Just Accepted.\\n[129] M. Liu, A. Zeng, M. Chen, Z. Xu, Q. LAI, L. Ma, and Q. Xu: Scinet: Time series modeling\\nand forecasting with sample convolution and interaction. In S. Koyejo, S. Mohamed, A. Agarwal,\\nD. Belgrave, K. Cho, and A. Oh (eds.): Advances in Neural Information Processing Systems ,\\nvol. 35, pp. 5816–5828. Curran Associates, Inc., 2022. https://proceedings.neurips.cc/paper fil\\nes/paper/2022/file/266983d0949aed78a16fa4782237dea7-Paper-Conference.pdf.\\n[130] W. Liu and M.J. Pyrcz: Physics-informed graph neural network for spatial-temporal production\\nforecasting. Geoenergy Science and Engineering, 223, 2023. https://www.scopus.com/inward/re\\ncord.uri?eid=2-s2.0-85159768181&doi=10.1016%2fj.geoen.2023.211486&partnerID=40&md5=\\n6ca9e02af21ce637a72c91c3cb384d87.\\n[131] X. Liu and W. Li: Mgc-lstm: a deep learning model based on graph convolution of multiple\\ngraphs for pm2.5 prediction . International Journal of Environmental Science and Technology,\\n20(9):10297 – 10312, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851391979\\n16&doi=10.1007%2fs13762-022-04553-6&partnerID=40&md5=ede9d5fe2e88873842e36142d5df\\ne870.\\n[132] A. Longa, V. Lachi, G. Santin, M. Bianchini, B. Lepri, P. Lio, F. Scarselli, and A. Passerini:\\nGraph neural networks for temporal graphs: State of the art, open challenges, and opportunities ,\\n2023. https://arxiv.org/abs/2302.01018.\\n60'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 60}, page_content='[133] B. Lu, X. Gan, H. Jin, L. Fu, X. Wang, and H. Zhang: Make more connections: Urban traffic\\nflow forecasting with spatiotemporal adaptive gated graph convolution network. ACM Trans. Intell.\\nSyst. Technol., 13(2), jan 2022, ISSN 2157-6904. https://doi.org/10.1145/3488902.\\n[134] Z. Lu, W. Lv, Z. Xie, B. Du, G. Xiong, L. Sun, and H. Wang: Graph sequence neural network with\\nan attention mechanism for traffic speed prediction . ACM Transactions on Intelligent Systems\\nand Technology, 13(2), 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8512949\\n0445&doi=10.1145%2f3470889&partnerID=40&md5=b5718186ba7f6dd2ef89c24aaefa598c.\\n[135] H. Miao, Y. Zhang, Z. Ning, Z. Jiang, and L. Wang:Tdg4msf: A temporal decomposition enhanced\\ngraph neural network for multivariate time series forecasting . Applied Intelligence, 53(23):28254\\n– 28267, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172877266&doi=10.\\n1007%2fs10489-023-04987-6&partnerID=40&md5=16a1a0b034d9f2997c7126bf63e593c2.\\n[136] R. Mondal, D. Mukherjee, P.K. Singh, V. Bhateja, and R. Sarkar: A new framework for smart-\\nphone sensor-based human activity recognition using graph neural network. IEEE Sensors Journal,\\n21(10):11461–11468, 2021.\\n[137] C. Murphy, E. Laurence, and A. Allard: Deep learning of contagion dynamics on complex net-\\nworks. Nature Communications, 12(1), 2021. https://www.scopus.com/inward/record.uri?eid\\n=2-s2.0-85112016386&doi=10.1038%2fs41467-021-24732-2&partnerID=40&md5=35122d1d1e\\n1a230cde7e4eeb084ed2e0.\\n[138] C. Mylonas and E. Chatzi: Remaining useful life estimation for engineered systems operating\\nunder uncertainty with causal graphnets . Sensors, 21(19), 2021. https://www.scopus.com/inwar\\nd/record.uri?eid=2-s2.0-85115354292&doi=10.3390%2fs21196325&partnerID=40&md5=36bb\\n0e224a3382c64ffb79f3e3406f08.\\n[139] Q. Ni, Y. Wang, and J. Yuan: Adaptive scalable spatio-temporal graph convolutional network\\nfor pm2.5 prediction . Engineering Applications of Artificial Intelligence, 126, 2023. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85170435593&doi=10.1016%2fj.engappai.20\\n23.107080&partnerID=40&md5=0b1f47522e4e9e46ce3f7b05a8f66222.\\n[140] X. Nie, X. Zhou, Z. Li, L. Wang, X. Lin, and T. Tong: Logtrans: Providing efficient local-\\nglobal fusion with transformer and cnn parallel network for biomedical image segmentation . 2022\\nIEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data\\nScience & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor,\\nCloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys), pp. 769–776,\\n2022. https://api.semanticscholar.org/CorpusID:257809606.\\n[141] S. Ning, Y. Ren, and Y. Wu: Intelligent fault diagnosis of rolling bearings based on the visibility\\nalgorithm and graph neural networks . Journal of the Brazilian Society of Mechanical Sciences\\nand Engineering, 45(2), 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8514577\\n6581&doi=10.1007%2fs40430-022-03913-0&partnerID=40&md5=af94ddfb589153eb2c10d564cd\\n0cfcb0.\\n[142] L.C. Oliveira, J.T. Oliva, M.H.D. Ribeiro, M. Teixeira, and D. Casanova: Forecasting the covid-\\n19 space-time dynamics in brazil with convolutional graph neural networks and transport modals .\\nIEEE Access, 10:85064 – 85079, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.\\n0-85135735451&doi=10.1109%2fACCESS.2022.3195535&partnerID=40&md5=5c50b939a6898c\\n1bff1f8b10f4995172.\\n[143] B.N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio: N-beats: Neural basis expansion analysis\\nfor interpretable time series forecasting , 2020. https://arxiv.org/abs/1905.10437.\\n[144] X. Ou, Y. Chen, S. Zhou, and J. Shi: Online educational video engagement prediction based on\\ndynamic graph neural networks . International Journal of Web Information Systems, 19(5-6):190\\n– 207, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169846306&doi=10.11\\n08%2fIJWIS-05-2023-0083&partnerID=40&md5=15e322f1f49558c503a7caecaa56d1c1.\\n61'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 61}, page_content='[145] M.J. Page, J.E. McKenzie, P.M. Bossuyt, I. Boutron, T.C. Hoffmann, C.D. Mulrow, L. Shamseer,\\nJ.M. Tetzlaff, E.A. Akl, S.E. Brennan, R. Chou, J. Glanville, J.M. Grimshaw, A. Hr´ objartsson,\\nM.M. Lalu, T. Li, E.W. Loder, E. Mayo-Wilson, S. McDonald, L.A. McGuinness, L.A. Stewart,\\nJ. Thomas, A.C. Tricco, V.A. Welch, P. Whiting, and D. Moher: The PRISMA 2020 statement:\\nan updated guideline for reporting systematic reviews . Syst. Rev., 10(1):89, Mar. 2021.\\n[146] C. Pan, J. Zhu, Z. Kong, H. Shi, and W. Yang: Dc-stgcn: Dual-channel based graph convolutional\\nnetworks for network traffic forecasting . Electronics (Switzerland), 10(9), 2021. https://www.sc\\nopus.com/inward/record.uri?eid=2-s2.0-85104608714&doi=10.3390%2felectronics10091014&p\\nartnerID=40&md5=1454417922864f06be4f4c5825c0ea0b.\\n[147] J. Pan, Z. Li, S. Shi, L. Xu, J. Yu, and X. Wu: Adaptive graph neural network based south china\\nsea seawater temperature prediction and multivariate uncertainty correlation analysis. Stochastic\\nEnvironmental Research and Risk Assessment, 37(5):1877 – 1896, 2023. https://www.scopus.c\\nom/inward/record.uri?eid=2-s2.0-85144879589&doi=10.1007%2fs00477-022-02371-3&partner\\nID=40&md5=4ea37dcb258343afe06b749f6ee7f056.\\n[148] Z. Pan, Y. Liang, W. Wang, Y. Yu, Y. Zheng, and J. Zhang: Urban traffic prediction from spatio-\\ntemporal data using deep meta learning . In Proceedings of the 25th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining , KDD ’19, p. 1720–1730, New York, NY,\\nUSA, 2019. Association for Computing Machinery, ISBN 9781450362016. https://doi.org/10.1\\n145/3292500.3330884.\\n[149] B. Pang, W. Wei, X. Li, X. Feng, and C. Li: A representation-learning-based approach to predict\\nstock price trend via dynamic spatiotemporal feature embedding . Engineering Applications of\\nArtificial Intelligence, 126, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166\\n943902&doi=10.1016%2fj.engappai.2023.106849&partnerID=40&md5=98137deaa406487e8472f\\n3117d7bfd75.\\n[150] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-\\njani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala: Pytorch: An imperative style,\\nhigh-performance deep learning library. In Advances in Neural Information Processing Systems\\n32, pp. 8024–8035. Curran Associates, Inc., 2019. http://papers.neurips.cc/paper/9015-pytorch\\n-an-imperative-style-high-performance-deep-learning-library.pdf.\\n[151] Y. Pei, C.J. Huang, Y. Shen, and Y. Ma: An ensemble model with adaptive variational mode\\ndecomposition and multivariate temporal graph neural network for pm2.5 concentration forecast-\\ning. Sustainability (Switzerland), 14(20), 2022. https://www.scopus.com/inward/record.uri?eid\\n=2-s2.0-85140791792&doi=10.3390%2fsu142013191&partnerID=40&md5=a3f1334c578f08e00a\\n57aee71f9aac5e.\\n[152] X. Peng, Q. Li, L. Chen, X. Ning, H. Chu, and J. Liu: A structured graph neural network\\nfor improving the numerical weather prediction of rainfall . Journal of Geophysical Research:\\nAtmospheres, 128, Nov. 2023.\\n[153] M.A. Prado-Romero, B. Prenkaj, G. Stilo, and F. Giannotti: A survey on graph counterfactual\\nexplanations: Definitions, methods, evaluation, and research challenges . ACM Comput. Surv.,\\n56(7), apr 2024, ISSN 0360-0300. https://doi.org/10.1145/3618105.\\n[154] Y. Qi, Q. Li, H. Karimian, and D. Liu: A hybrid model for spatiotemporal forecasting of pm2.5\\nbased on graph convolutional neural network and long short-term memory . Science of The Total\\nEnvironment, 664:1–10, 2019, ISSN 0048-9697. https://www.sciencedirect.com/science/article/\\npii/S0048969719303821.\\n[155] C. Qin, A.K. Srivastava, A.Y. Saber, D. Matthews, and K. Davies:Geometric deep-learning-based\\nspatiotemporal forecasting for inverter-based solar power . IEEE Systems Journal, 17(3):3425–\\n3435, 2023.\\n62'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 62}, page_content='[156] Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G.W. Cottrell: A dual-stage attention-based\\nrecurrent neural network for time series prediction . IJCAI’17, p. 2627–2633. AAAI Press, 2017,\\nISBN 9780999241103.\\n[157] Y. Ren, Z. Li, L. Xu, and J. Yu: The data-based adaptive graph learning network for analysis\\nand prediction of offshore wind speed. Energy, 267, 2023. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85146061902&doi=10.1016%2fj.energy.2022.126590&partnerID=40&md5=b\\n3fe9bd30bab36c543b09b195a909ecb.\\n[158] S. Ruan, S. Han, C. Lu, and Q. Gu: Proactive control model for safety prediction in tailing dam\\nmanagement: Applying graph depth learning optimization . Process Safety and Environmental\\nProtection, 172:329 – 340, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148\\n375797&doi=10.1016%2fj.psep.2023.02.019&partnerID=40&md5=2020073e349291aee21a43f91\\n7acbbcd.\\n[159] R. Sawhney, S. Agarwal, A. Wadhwa, and R.R. Shah: Spatiotemporal hypergraph convolution\\nnetwork for stock movement forecasting. In 2020 IEEE International Conference on Data Mining\\n(ICDM), pp. 482–491, 2020.\\n[160] F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini: The graph neural network\\nmodel. IEEE Transactions on Neural Networks, 20(1):61–80, Jan. 2009, ISSN 1941-0093. http:\\n//dx.doi.org/10.1109/TNN.2008.2005605.\\n[161] C. Shang, J. Chen, and J. Bi: Discrete graph structure learning for forecasting multiple time\\nseries, 2021. https://arxiv.org/abs/2101.06861.\\n[162] Y. Shao, H. Li, X. Gu, H. Yin, Y. Li, X. Miao, W. Zhang, B. Cui, and L. Chen: Distributed\\ngraph neural network training: A survey . ACM Comput. Surv., 56(8), apr 2024, ISSN 0360-0300.\\nhttps://doi.org/10.1145/3648358.\\n[163] F. Shen, J. Wang, Z. Zhang, X. Wang, Y. Li, Z. Geng, B. Pan, Z. Lu, W. Zhao, and W. Zhu:\\nLong-term multivariate time series forecasting in data centers based on multi-factor separation\\nevolutionary spatial–temporal graph neural networks . Knowledge-Based Systems, 280:110997,\\n2023, ISSN 0950-7051. https://www.sciencedirect.com/science/article/pii/S0950705123007475.\\n[164] L. Sheng, L. Xu, J. Yu, and Z. Li: A graph multi-head self-attention neural network for the multi-\\npoint long-term prediction of sea surface temperature. Remote Sensing Letters, 14:786–796, July\\n2023.\\n[165] X. Shi, Z. Chen, H. Wang, D.Y. Yeung, W.k. Wong, and W.c. Woo: Convolutional lstm network:\\na machine learning approach for precipitation nowcasting. NIPS’15, p. 802–810, Cambridge, MA,\\nUSA, 2015. MIT Press.\\n[166] S.Y. Shih, F.K. Sun, and H.y. Lee: Temporal pattern attention for multivariate time se-\\nries forecasting. Machine Learning, 108(8–9):1421–1441, June 2019, ISSN 1573-0565. http:\\n//dx.doi.org/10.1007/s10994-019-05815-0.\\n[167] J. Simeunovi´ c, B. Schubnel, P.J. Alet, and R.E. Carrillo:Spatio-temporal graph neural networks\\nfor multi-site pv power forecasting . IEEE Transactions on Sustainable Energy, 13(2):1210–1220,\\n2022.\\n[168] J. Simeunovi´ c, B. Schubnel, P.J. Alet, R.E. Carrillo, and P. Frossard: Interpretable temporal-\\nspatial graph attention network for multi-site pv power forecasting . Applied Energy, 327, 2022.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85140322468&doi=10.1016%2fj.apene\\nrgy.2022.120127&partnerID=40&md5=aa7fdfb795cee1dccf2ce60089a6ad1a.\\n[169] C. Song, Y. Lin, S. Guo, and H. Wan: Spatial-temporal synchronous graph convolutional net-\\nworks: A new framework for spatial-temporal network data forecasting . Proceedings of the AAAI\\nConference on Artificial Intelligence, 34:914–921, Apr. 2020.\\n63'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 63}, page_content='[170] A. Sriramulu, N. Fourrier, and C. Bergmeir: Adaptive dependency learning graph neural networks.\\nInformation Sciences, 625:700 – 714, 2023. https://www.scopus.com/inward/record.uri?eid=2-s\\n2.0-85146595559&doi=10.1016%2fj.ins.2022.12.086&partnerID=40&md5=7dc4e3cb8c0e80c95e\\n1b64e1cebe3174.\\n[171] I.F. Su, Y.C. Chung, C. Lee, and P.M. Huang: Effective pm2.5 concentration forecasting based\\non multiple spatial–temporal gnn for areas without monitoring stations . Expert Systems with\\nApplications, 234, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172418127\\n&doi=10.1016%2fj.eswa.2023.121074&partnerID=40&md5=b2052aeba20547520da4343cdef9e53\\n5.\\n[172] J. Sun, J. Zhang, Q. Li, X. Yi, and Y. Zheng: Predicting citywide crowd flows in irregular\\nregions using multi-view graph convolutional networks . IEEE Transactions on Knowledge and\\nData Engineering, 34:2348–2359, 2019. https://api.semanticscholar.org/CorpusID:83458663.\\n[173] Y. Sun, X. Yao, X. Bi, X. Huang, X. Zhao, and B. Qiao: Time-series graph network for sea\\nsurface temperature prediction. Big Data Research, 25, 2021. https://www.scopus.com/inward/\\nrecord.uri?eid=2-s2.0-85108111192&doi=10.1016%2fj.bdr.2021.100237&partnerID=40&md5=\\nf5517ee8a8aeeab05c9c93a7c09a8065.\\n[174] Z. Sun, A. Harit, A.I. Cristea, J. Wang, and P. Lio: Money: Ensemble learning for stock price\\nmovement prediction via a convolutional network with adversarial hypergraph model . AI Open,\\n4:165 – 174, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174744697&doi=1\\n0.1016%2fj.aiopen.2023.10.002&partnerID=40&md5=39df342a801ddf4d2ff212b098780ccc.\\n[175] I. Sutskever, O. Vinyals, and Q.V. Le: Sequence to sequence learning with neural networks . In\\nProceedings of the 27th International Conference on Neural Information Processing Systems -\\nVolume 2, NIPS’14, p. 3104–3112, Cambridge, MA, USA, 2014. MIT Press.\\n[176] H. Tian, X. Zhang, X. Zheng, and D.D. Zeng: Learning dynamic dependencies with graph evolu-\\ntion recurrent unit for stock predictions. IEEE Transactions on Systems, Man, and Cybernetics:\\nSystems, 53(11):6705 – 6717, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851\\n64722955&doi=10.1109%2fTSMC.2023.3284840&partnerID=40&md5=44bf60e571f4029bcae879\\n9a54044eaa.\\n[177] J. Van Gompel, D. Spina, and C. Develder: Cost-effective fault diagnosis of nearby photovoltaic\\nsystems using graph neural networks . Energy, 266, 2023. https://www.scopus.com/inward/reco\\nrd.uri?eid=2-s2.0-85145678193&doi=10.1016%2fj.energy.2022.126444&partnerID=40&md5=6\\n1789eedf9afd2352aa6a69fa1102cbe.\\n[178] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, and I. Polo-\\nsukhin: Attention is all you need . In Proceedings of the 31st International Conference on Neural\\nInformation Processing Systems , NIPS’17, p. 6000–6010, Red Hook, NY, USA, 2017. Curran\\nAssociates Inc., ISBN 9781510860964.\\n[179] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio’, and Y. Bengio: Graph attention\\nnetworks. ArXiv, abs/1710.10903, 2017. https://api.semanticscholar.org/CorpusID:3292002.\\n[180] M.N. Vu and M.T. Thai: On the limit of explaining black-box temporal graph neural networks ,\\n2022. https://arxiv.org/abs/2212.00952.\\n[181] D. Wang, M. Lin, X. Zhang, Y. Huang, and Y. Zhu: Automatic modulation classification based\\non cnn-transformer graph neural network . Sensors, 23(16), 2023. https://www.scopus.com/inw\\nard/record.uri?eid=2-s2.0-85168726736&doi=10.3390%2fs23167281&partnerID=40&md5=2da8\\n3f36537b612739971912a18a3e94.\\n[182] H. Wang, Z. Zhang, X. Li, X. Deng, and W. Jiang:Comprehensive dynamic structure graph neural\\nnetwork for aero-engine remaining useful life prediction . IEEE Transactions on Instrumentation\\nand Measurement, 72, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851748033\\n68&doi=10.1109%2fTIM.2023.3322481&partnerID=40&md5=8b8174ae3bbaeff2410bf9bcc0687e\\nf9.\\n64'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 64}, page_content='[183] J. Wang, R. Gao, H. Zheng, H. Zhu, and C.J.R. Shi: Ssgcnet: A sparse spectra graph convolu-\\ntional network for epileptic eeg signal classification . IEEE Transactions on Neural Networks and\\nLearning Systems, pp. 1–15, 2023.\\n[184] J. Wang, Y. Zhang, Y. Hu, and B. Yin: Metro flow prediction with hierarchical hypergraph\\nattention networks. IEEE Transactions on Artificial Intelligence, p. 1–10, 2023. https://www.sc\\nopus.com/inward/record.uri?eid=2-s2.0-85179113347&doi=10.1109%2fTAI.2023.3337052&part\\nnerID=40&md5=de07f1f83b50334a08a1ed9015edc004.\\n[185] T. Wang and S.C. Chen: Adaptive joint spatio-temporal graph learning network for traffic data\\nforecasting. ACM Trans. Spatial Algorithms Syst., nov 2023, ISSN 2374-0353. https://doi.org/\\n10.1145/3634913, Just Accepted.\\n[186] T. Wang, J. Guo, Y. Shan, Y. Zhang, B. Peng, and Z. Wu: A knowledge graph–gcn–community\\ndetection integrated model for large-scale stock price prediction . Applied Soft Computing, 145,\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165231582&doi=10.1016%2fj.a\\nsoc.2023.110595&partnerID=40&md5=d650cd25939f6c273bac39b33b937c8d.\\n[187] T. Wang, Z. Li, X. Geng, B. Jin, and L. Xu: Time series prediction of sea surface temperature\\nbased on an adaptive graph learning neural model . Future Internet, 14(6), 2022. https://www.sc\\nopus.com/inward/record.uri?eid=2-s2.0-85131754722&doi=10.3390%2ffi14060171&partnerID=\\n40&md5=46e4a145ffbeaf77c7a04aefbd9e21d5.\\n[188] X. Wang, Y. Ma, Y. Wang, W. Jin, X. Wang, J. Tang, C. Jia, and J. Yu: Traffic flow predic-\\ntion via spatial temporal graph neural network . In Proceedings of The Web Conference 2020 ,\\nWWW ’20, p. 1082–1092, New York, NY, USA, 2020. Association for Computing Machinery,\\nISBN 9781450370233. https://doi.org/10.1145/3366423.3380186.\\n[189] X. Wang, Y. Wang, J. Peng, and Z. Zhang: Multivariate long sequence time-series forecasting\\nusing dynamic graph learning . Journal of Ambient Intelligence and Humanized Computing,\\n14(6):7679 – 7693, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149947973\\n&doi=10.1007%2fs12652-023-04579-9&partnerID=40&md5=a5008855cddc0f51d1b2682ed2ec41\\nf0.\\n[190] Y. Wang, Z. Duan, Y. Huang, H. Xu, J. Feng, and A. Ren: Mthetgnn: A heterogeneous graph em-\\nbedding framework for multivariate time series forecasting. Pattern Recognition Letters, 153:151\\n– 158, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122529802&doi=10.10\\n16%2fj.patrec.2021.12.008&partnerID=40&md5=bc4718383ac1de860c4f8b961f840f3c.\\n[191] Y. Wang, L. Rui, J. Ma, and Q. jin: A short-term residential load forecasting scheme based on\\nthe multiple correlation-temporal graph neural networks . Applied Soft Computing, 146, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85166632235&doi=10.1016%2fj.asoc.20\\n23.110629&partnerID=40&md5=d8381e1128741bef231d2502968769a7.\\n[192] Y. Wang, M. Wu, R. Jin, X. Li, L. Xie, and Z. Chen: Local-global correlation fusion-based graph\\nneural network for remaining useful life prediction . IEEE Transactions on Neural Networks and\\nLearning Systems, p. 1–14, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-851\\n78023045&doi=10.1109%2fTNNLS.2023.3330487&partnerID=40&md5=748cbeab0a7b0e9a123\\n5f8eba377496e.\\n[193] Y. Wang, H. Yin, T. Chen, C. Liu, B. Wang, T. Wo, and J. Xu: Passenger mobility prediction\\nvia representation learning for dynamic directed and weighted graphs . ACM Trans. Intell. Syst.\\nTechnol., 13(1), nov 2021, ISSN 2157-6904. https://doi.org/10.1145/3446344.\\n[194] Z. Wang, J. Hu, G. Min, Z. Zhao, Z. Chang, and Z. Wang: Spatial-temporal cellular traffic\\nprediction for 5g and beyond: A graph neural networks-based approach . IEEE Transactions on\\nIndustrial Informatics, 19(4):5722 – 5731, 2023. https://www.scopus.com/inward/record.uri?e\\nid=2-s2.0-85133780529&doi=10.1109%2fTII.2022.3182768&partnerID=40&md5=023f8dd0880\\nac99b14176e474a84e651.\\n65'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 65}, page_content='[195] Z. Wang, X. Liu, Y. Huang, P. Zhang, and Y. Fu:A multivariate time series graph neural network\\nfor district heat load forecasting . Energy, 278, 2023. https://www.scopus.com/inward/record.u\\nri?eid=2-s2.0-85160794010&doi=10.1016%2fj.energy.2023.127911&partnerID=40&md5=6e6da\\nf9126a0d450a91779e9c7dd2f8a.\\n[196] Y. Wei and D. Wu: Prediction of state of health and remaining useful life of lithium-ion battery\\nusing graph convolutional network with dual attention mechanisms . Reliability Engineering and\\nSystem Safety, 230, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141763841\\n&doi=10.1016%2fj.ress.2022.108947&partnerID=40&md5=3e921c5bf14ac76a0bb1fa77148a083\\n2.\\n[197] Z. Wen, Y. Li, H. Wang, and Y. Peng: Data-driven spatiotemporal modeling for structural dy-\\nnamics on irregular domains by stochastic dependency neural estimation . Computer Methods in\\nApplied Mechanics and Engineering, 404, 2023. https://www.scopus.com/inward/record.uri?e\\nid=2-s2.0-85144323571&doi=10.1016%2fj.cma.2022.115831&partnerID=40&md5=42798d4a57\\na7badef1e91cf83ea00686.\\n[198] H. Wu, J. Xu, J. Wang, and M. Long: Autoformer: Decomposition transformers with auto-\\ncorrelation for long-term series forecasting , 2022. https://arxiv.org/abs/2106.13008.\\n[199] M. Wu, M. Yan, W. Li, X. Ye, D. Fan, and Y. Xie: A comprehensive survey on gnn characteri-\\nzation, 2024. https://arxiv.org/abs/2408.01902.\\n[200] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. Yu: A comprehensive survey on graph neural\\nnetworks. IEEE Transactions on Neural Networks and Learning Systems, PP:1–21, Mar. 2020.\\n[201] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang: Connecting the dots: Multivariate\\ntime series forecasting with graph neural networks . In Proceedings of the 26th ACM SIGKDD\\nInternational Conference on Knowledge Discovery & Data Mining , KDD ’20, p. 753–763, New\\nYork, NY, USA, 2020. Association for Computing Machinery, ISBN 9781450379984. https:\\n//doi.org/10.1145/3394486.3403118.\\n[202] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang: Graph wavenet for deep spatial-temporal\\ngraph modeling. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial\\nIntelligence, IJCAI-19, pp. 1907–1913. International Joint Conferences on Artificial Intelligence\\nOrganization, July 2019. https://doi.org/10.24963/ijcai.2019/264.\\n[203] T. Xia, J. Lin, Y. Li, J. Feng, P. Hui, F. Sun, D. Guo, and D. Jin: 3dgcn: 3-dimensional\\ndynamic graph convolutional network for citywide crowd flow prediction . ACM Trans. Knowl.\\nDiscov. Data, 15(6), jun 2021, ISSN 1556-4681. https://doi.org/10.1145/3451394.\\n[204] J. Xie, J. Zhang, J. Yu, and L. Xu: An adaptive scale sea surface temperature predicting method\\nbased on deep learning with attention mechanism. IEEE Geoscience and Remote Sensing Letters,\\n17(5):740–744, 2020.\\n[205] Z. Xie, W. Lv, S. Huang, Z. Lu, B. Du, and R. Huang: Sequential graph neural network for urban\\nroad traffic speed prediction. IEEE Access, 8:63349–63358, 2020.\\n[206] L. Xu, X. Geng, X. He, J. Li, and J. Yu: Prediction in autism by deep learning short-time spon-\\ntaneous hemodynamic fluctuations . Frontiers in Neuroscience, 13, Nov. 2019, ISSN 1662-453X.\\nhttp://dx.doi.org/10.3389/fnins.2019.01120.\\n[207] Y. Xu, H. Ying, S. Qian, F. Zhuang, X. Zhang, D. Wang, J. Wu, and H. Xiong: Time-aware\\ncontext-gated graph attention network for clinical risk prediction . IEEE Transactions on Knowl-\\nedge and Data Engineering, p. 1–12, 2022, ISSN 2326-3865. http://dx.doi.org/10.1109/TKDE.\\n2022.3181780.\\n[208] Q. Xuan, J. Zhou, K. Qiu, Z. Chen, D. Xu, S. Zheng, and X. Yang: Avgnet: Adaptive visibility\\ngraph neural network and its application in modulation classification . IEEE Transactions on\\nNetwork Science and Engineering, 9(3):1516–1526, 2022.\\n66'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 66}, page_content='[209] H. Yang, W. Li, S. Hou, J. Guan, and S. Zhou: Higrn: A hierarchical graph recurrent network\\nfor global sea surface temperature prediction. ACM Trans. Intell. Syst. Technol., 14(4), jul 2023,\\nISSN 2157-6904. https://doi.org/10.1145/3597937.\\n[210] J. Yang, F. Xie, J. Yang, J. Shi, J. Zhao, and R. Zhang: Spatial-temporal correlated graph neural\\nnetworks based on neighborhood feature selection for traffic data prediction. Applied Intelligence,\\n53(4):4717 – 4732, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132117827\\n&doi=10.1007%2fs10489-022-03753-4&partnerID=40&md5=bb4aa31bb7abfe9d7a5657d14087\\n899b.\\n[211] S. Yang, Y. Zhang, and Z. Zhang:Runoff prediction based on dynamic spatiotemporal graph neural\\nnetwork. Water, 15(13), 2023, ISSN 2073-4441. https://www.mdpi.com/2073-4441/15/13/2463.\\n[212] X. Yang, Y. Zheng, Y. Zhang, D.S.H. Wong, and W. Yang: Bearing remaining useful life predic-\\ntion based on regression shapalet and graph neural network . IEEE Transactions on Instrumenta-\\ntion and Measurement, 71, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124\\n758480&doi=10.1109%2fTIM.2022.3151169&partnerID=40&md5=b18668dbd301b28fe64d3b4\\n47e86a287.\\n[213] Y. Yang, J. Dong, X. Sun, E. Lima, Q. Mu, and X. Wang: A cfcc-lstm model for sea surface\\ntemperature prediction. IEEE Geoscience and Remote Sensing Letters, 15(2):207–211, 2018.\\n[214] Z. Yang, Z. Zhihan, L. Haiying, Z. Weiyi, D. Qian, and T. Mingjie: Research on commodities\\nconstraint optimization based on graph neural network prediction. IEEE Access, 11:90131–90142,\\n2023.\\n[215] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec: GNNExplainer: generating explana-\\ntions for graph neural networks . Curran Associates Inc., Red Hook, NY, USA, 2019.\\n[216] B. Yu, H. Yin, and Z. Zhu: Spatio-temporal graph convolutional networks: A deep learning frame-\\nwork for traffic forecasting. In Proceedings of the Twenty-Seventh International Joint Conference\\non Artificial Intelligence, IJCAI-18, pp. 3634–3640. International Joint Conferences on Artificial\\nIntelligence Organization, July 2018. https://doi.org/10.24963/ijcai.2018/505.\\n[217] B. Yu, H. Yin, and Z. Zhu: St-unet: A spatio-temporal u-network for graph-structured time series\\nmodeling, 2021. https://arxiv.org/abs/1903.05631.\\n[218] L. Yu, M. Li, W. Jin, Y. Guo, Q. Wang, F. Yan, and P. Li: Step: A spatio-temporal fine-granular\\nuser traffic prediction system for cellular networks . IEEE Transactions on Mobile Computing,\\n20(12):3453 – 3466, 2021. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112368991\\n&doi=10.1109%2fTMC.2020.3001225&partnerID=40&md5=c709d018d04cea671a8416889aeb0\\n4db.\\n[219] M. Yu, Z. Zhang, X. Li, J. Yu, J. Gao, Z. Liu, B. You, X. Zheng, and R. Yu: Superposition\\ngraph neural network for offshore wind power prediction . Future Generation Computer Systems,\\n113:145 – 157, 2020. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087721191&doi\\n=10.1016%2fj.future.2020.06.024&partnerID=40&md5=423d4b6fc27457eb5dc06399b3e03900.\\n[220] S. Yu, F. Xia, S. Li, M. Hou, and Q.Z. Sheng: Spatio-temporal graph learning for epidemic\\nprediction. ACM Trans. Intell. Syst. Technol., 14(2), feb 2023, ISSN 2157-6904. https://doi.or\\ng/10.1145/3579815.\\n[221] Z. Yuan, X. Li, S. Liu, and Z. Ma: A recursive multi-head graph attention residual network for\\nhigh-speed train wheelset bearing fault diagnosis . Measurement Science and Technology, 34(6),\\n2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149833631&doi=10.1088%2f1\\n361-6501%2facb609&partnerID=40&md5=c64869e9105b0a425b18db2c2a9d7893.\\n[222] A. Zanfei, B.M. Brentan, A. Menapace, M. Righetti, and M. Herrera: Graph convolutional re-\\ncurrent neural networks for water demand forecasting . Water Resources Research, 58(7), 2022.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85134874398&doi=10.1029%2f2022W\\nR032299&partnerID=40&md5=f97e6f84ca176efd73575a0b897df1b7.\\n67'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 67}, page_content='[223] A. Zeghina, A. Leborgne, F. Le Ber, and A. Vacavant: Deep learning on spatiotemporal graphs:\\nA systematic review, methodological landscape, and research opportunities . Neurocomputing,\\n594:127861, 2024, ISSN 0925-2312. https://www.sciencedirect.com/science/article/pii/S09252\\n31224006325.\\n[224] K. Zeinalipour and M. Gori: Graph Neural Networks for Topological Feature Extraction in ECG\\nClassification, pp. 17–27. Springer Nature Singapore, Singapore, 2023, ISBN 978-981-99-3592-5.\\nhttps://doi.org/10.1007/978-981-99-3592-5 2.\\n[225] F. Zhan, X. Zhou, S. Li, D. Jia, and H. Song: Learning latent odes with graph rnn for multi-\\nchannel time series forecasting . IEEE Signal Processing Letters, 30:1432 – 1436, 2023. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85173313670&doi=10.1109%2fLSP.2023.3320\\n439&partnerID=40&md5=df7ee320417ca9e17c369f1b044f4de6.\\n[226] J. Zhang, Y. Cheng, and X. He: Fault diagnosis of energy networks based on improved spa-\\ntial–temporal graph neural network with massive missing data . IEEE Transactions on Automa-\\ntion Science and Engineering, p. 1–12, 2023. https://www.scopus.com/inward/record.uri?eid=2\\n-s2.0-85181574140&doi=10.1109%2fTASE.2023.3281394&partnerID=40&md5=246569091900\\n417dc74818e2fe35ee1c.\\n[227] J. Zhang, P. Zhou, Y. Zheng, and H. Wu: Predicting influenza with pandemic-awareness via\\ndynamic virtual graph significance networks . Computers in Biology and Medicine, 158, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85151297427&doi=10.1016%2fj.compb\\niomed.2023.106807&partnerID=40&md5=8665ba7b3020111e335effcc0d6705c1.\\n[228] P. Zhang: Zhang, g.p.: Time series forecasting using a hybrid arima and neural network model.\\nneurocomputing 50, 159-175. Neurocomputing, 50:159–175, Jan. 2003.\\n[229] Q. Zhang, J. Chen, G. Xiao, S. He, and K. Deng: Transformgraph: A novel short-term electricity\\nnet load forecasting model. Energy Reports, 9:2705 – 2717, 2023. https://www.scopus.com/inw\\nard/record.uri?eid=2-s2.0-85147090957&doi=10.1016%2fj.egyr.2023.01.050&partnerID=40&m\\nd5=e9cc36dd59c178150589dbf056814624.\\n[230] Q. Zhang, K. Yu, Z. Guo, S. Garg, J.J.P.C. Rodrigues, M.M. Hassan, and M. Guizani: Graph\\nneural network-driven traffic forecasting for the connected internet of vehicles. IEEE Transactions\\non Network Science and Engineering, 9(5):3015–3027, 2022.\\n[231] T. Zhang and G. Guo: Graph attention lstm: A spatiotemporal approach for traffic flow fore-\\ncasting. IEEE Intelligent Transportation Systems Magazine, 14(2):190 – 196, 2022. https:\\n//www.scopus.com/inward/record.uri?eid=2-s2.0-85127914420&doi=10.1109%2fMITS.2020.2\\n990165&partnerID=40&md5=6d07c1fa98271c0da4f2226456c3c16e.\\n[232] W. Zhang, Y. Yin, J. Tang, and B. Yi: A method for the spatiotemporal correlation prediction of\\nthe quality of multiple operational processes based on s-ggru . Advanced Engineering Informatics,\\n58, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174932585&doi=10.1016\\n%2fj.aei.2023.102219&partnerID=40&md5=7737358558c75a7aa894ea8f53ba02d1.\\n[233] X. Zhang, Z. Long, J. Peng, G. Wu, H. Hu, M. Lyu, G. Qin, and D. Song: Fault prediction for\\nelectromechanical equipment based on spatial-temporal graph information. IEEE Transactions on\\nIndustrial Informatics, 19(2):1413 – 1424, 2023. https://www.scopus.com/inward/record.uri?e\\nid=2-s2.0-85130824180&doi=10.1109%2fTII.2022.3176891&partnerID=40&md5=d1ccc936509\\n7c6b3b0fb2c415e064e08.\\n[234] X. Zhang, Y. Wang, L. Zhang, B. Jin, and H. Zhang: Exploring unsupervised multivariate time\\nseries representation learning for chronic disease diagnosis. International Journal of Data Science\\nand Analytics, 15(2):173 – 186, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8\\n5118890785&doi=10.1007%2fs41060-021-00290-0&partnerID=40&md5=6afc2285535ab47515ce\\n46642c9532b9.\\n[235] Y. Zhang, J. Liu, B. Guo, Z. Wang, Y. Liang, and Z. Yu: App popularity prediction by in-\\ncorporating time-varying hierarchical interactions . IEEE Transactions on Mobile Computing,\\n21(5):1566–1579, 2022.\\n68'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 68}, page_content='[236] Y. Zhang, L. Xu, and J. Yu: Significant wave height prediction based on dynamic graph neural\\nnetwork with fusion of ocean characteristics . Dynamics of Atmospheres and Oceans, 103, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85168558949&doi=10.1016%2fj.dynat\\nmoce.2023.101388&partnerID=40&md5=5677e219ed6c9680c8a4e1a544472953.\\n[237] Z. Zhang, Y. Han, B. Ma, M. Liu, and Z. Geng: Temporal chain network with intuitive attention\\nmechanism for long-term series forecasting . IEEE Transactions on Instrumentation and Mea-\\nsurement, 72, 2023. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174801615&doi\\n=10.1109%2fTIM.2023.3322508&partnerID=40&md5=76d1d7f28736a276573d2cb4960c5422.\\n[238] C. Zhao, X. Li, Z. Shao, H. Yang, and F. Wang: Multi-featured spatial-temporal and dynamic\\nmulti-graph convolutional network for metro passenger flow prediction . Connection Science,\\n34(1):1252 – 1272, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-8512885\\n3908&doi=10.1080%2f09540091.2022.2061915&partnerID=40&md5=217c9be7fc265fecbc20f25a\\n90f1d1ad.\\n[239] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng, and H. Li: T-gcn: A temporal\\ngraph convolutional network for traffic prediction . IEEE Transactions on Intelligent Transporta-\\ntion Systems, 21(9):3848–3858, 2020.\\n[240] T. Zhao, M. Yue, and J. Wang: Structure-informed graph learning of networked dependencies for\\nonline prediction of power system transient dynamics . IEEE Transactions on Power Systems,\\n37(6):4885 – 4895, 2022. https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125352651\\n&doi=10.1109%2fTPWRS.2022.3153328&partnerID=40&md5=503f1e159bed6ffbba9b7dce482b\\n535f.\\n[241] W. Zhao, S. Zhang, B. Wang, and B. Zhou: Spatio-temporal causal graph attention network for\\ntraffic flow prediction in intelligent transportation systems . PeerJ Computer Science, 9, 2023.\\nhttps://www.scopus.com/inward/record.uri?eid=2-s2.0-85168804113&doi=10.7717%2fPEERJ\\n-CS.1484&partnerID=40&md5=95b32f54a0071452c46023d8b0384c2c.\\n[242] C. Zheng, X. Fan, C. Wang, and J. Qi: Gman: A graph multi-attention network for traffic\\nprediction. Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):1234–1241,\\nApr. 2020. https://ojs.aaai.org/index.php/AAAI/article/view/5477.\\n[243] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang: Informer: Beyond efficient\\ntransformer for long sequence time-series forecasting . Proceedings of the AAAI Conference on\\nArtificial Intelligence, 35(12):11106–11115, May 2021, ISSN 2159-5399. http://dx.doi.org/10.16\\n09/aaai.v35i12.17325.\\n[244] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun: Graph neural\\nnetworks: A review of methods and applications . AI Open, 1:57–81, 2020, ISSN 2666-6510.\\nhttps://www.sciencedirect.com/science/article/pii/S2666651021000012.\\n[245] Q. Zhu, J. Chen, L. Zhu, X. Duan, and Y. Liu: Wind speed prediction with spatio–temporal\\ncorrelation: A deep learning approach . Energies, 11(4):705, Mar. 2018, ISSN 1996-1073. http:\\n//dx.doi.org/10.3390/en11040705.\\n[246] Q. Zhu, Q. Xiong, Z. Yang, and Y. Yu: A novel feature-fusion-based end-to-end approach for\\nremaining useful life prediction . J. Intell. Manuf., 34(8):3495–3505, sep 2022, ISSN 0956-5515.\\nhttps://doi.org/10.1007/s10845-022-02015-x.\\nA List of selected journal papers\\nTab. 35 lists all the journal papers included in this SLR, together with the year of publication, group\\nthey belong to, case study, and nature of the task (e.g., classification or forecasting).\\n69'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 69}, page_content='Table 35: List of selected journal papers, with year, group they belong to, case study, and nature of\\nthe task.\\nRef. Year Group Case study Task\\n[195] 2023 Energy Heat load Forecasting\\n[229] 2023 Energy Electricity net load Forecasting\\n[191] 2023 Energy Residential load Forecasting\\n[45] 2023 Energy Electricity load Forecasting\\n[34] 2023 Energy Photovoltaic power Forecasting\\n[63] 2022 Energy Wind power Forecasting\\n[167] 2022 Energy Photovoltaic power Forecasting\\n[168] 2022 Energy Photovoltaic power Forecasting\\n[240] 2022 Energy Power system transient dynamics Forecasting\\n[219] 2020 Energy Wind power Forecasting\\n[68] 2022 Energy Urban buildings energy consumption Forecasting\\n[155] 2023 Energy Solar power Forecasting\\n[105] 2022 Energy Wind power Forecasting\\n[22] 2023 Environment Air quality index Forecasting\\n[131] 2023 Environment PM 2.5 concentration Forecasting\\n[171] 2023 Environment PM 2.5 concentration Forecasting\\n[110] 2023 Environment PM 2.5 concentration Forecasting\\n[39] 2023 Environment Air quality index Forecasting\\n[139] 2023 Environment PM 2.5 concentration Forecasting\\n[86] 2023 Environment PM 2.5 concentration Forecasting\\n[151] 2022 Environment PM 2.5 concentration Forecasting\\n[209] 2023 Environment Sea temperature Forecasting\\n[7] 2023 Environment Wind speed Forecasting\\n[101] 2023 Environment Water quality Forecasting\\n[157] 2023 Environment Wind speed Forecasting\\n[236] 2023 Environment Significant wave height Forecasting\\n[87] 2023 Environment Sea temperature Forecasting\\n[8] 2023 Environment Wind speed Forecasting\\n[147] 2023 Environment Sea temperature Forecasting\\n[47] 2023 Environment Sea temperature Forecasting\\n[126] 2022 Environment Frost Classification/forecasting\\n[187] 2022 Environment Sea temperature Forecasting\\n[49] 2021 Environment Wind speed Forecasting\\n[173] 2021 Environment Sea temperature Forecasting\\n[113] 2022 Environment Water quality Forecasting\\n[6] 2023 Environment Groundwater level Forecasting\\n[46] 2023 Environment Wind speed Forecasting\\n[152] 2023 Environment Rainfall Forecasting\\n[40] 2023 Environment Wind speed Forecasting\\n[164] 2023 Environment Sea temperature Forecasting\\n[69] 2022 Finance Stock prediction Classification\\n[149] 2023 Finance Stock prediction Classification\\n[176] 2023 Finance Stock prediction Classification\\n[186] 2023 Finance Stock prediction Classification\\n70'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 70}, page_content='[111] 2022 Finance Investment prediction Forecasting\\n[174] 2023 Finance Stock prediction Classification\\n[28] 2022 Finance Financial prediction Classification\\n[72] 2023 Finance Trading Classification\\n[214] 2023 Finance Sale prediction Forecasting\\n[19] 2023 Generic Batch workloads Forecasting\\n[116] 2023 Generic Node classification for websites Classification\\n[61] 2023 Generic Clinical risk Classification\\n[32] 2023 Generic Sensor network Forecasting\\n[120] 2023 Generic Multivariate time series Forecasting\\n[48] 2023 Generic Multivariate time series Forecasting\\n[82] 2023 Generic Multivariate time series Forecasting\\n[237] 2023 Generic Long-term series forecasting Forecasting\\n[41] 2023 Generic Multivariate time series Forecasting\\n[121] 2023 Generic Multivariate time series Forecasting\\n[58] 2023 Generic Multivariate time series Forecasting\\n[21] 2023 Generic Multivariate time series Forecasting\\n[135] 2023 Generic Multivariate time series Forecasting\\n[189] 2023 Generic Multivariate long sequence time series Forecasting\\n[170] 2023 Generic Multivariate time series Forecasting\\n[26] 2022 Generic Multivariate time series Classification/forecasting\\n[88] 2023 Generic Multivariate time series Forecasting\\n[27] 2022 Generic Multivariate time series Forecasting\\n[190] 2022 Generic Multivariate time series Forecasting\\n[225] 2023 Generic Multi-channel time series Forecasting\\n[30] 2023 Generic Multivariate time series Forecasting\\n[64] 2022 Generic Multivariate time series Forecasting\\n[10] 2023 Generic Impact of graph construction Forecasting\\n[70] 2021 Generic Generic Classification/forecasting\\n[183] 2023 Health Epilepsy diagnosis Classification\\n[227] 2023 Health Epidemic prediction Forecasting\\n[220] 2023 Health Epidemic prediction Forecasting\\n[118] 2022 Health Epidemic prediction Forecasting\\n[137] 2021 Health Epidemic prediction Forecasting\\n[142] 2022 Health Epidemic prediction Forecasting\\n[234] 2023 Health Disease diagnosis Classification\\n[207] 2023 Health Clinical risk classification Classification\\n[133] 2022 Mobility Urban traffic Forecasting\\n[203] 2021 Mobility Crowd flow Forecasting\\n[185] 2023 Mobility Flight delays, urban traffic Forecasting\\n[102] 2022 Mobility Crowd flow Forecasting\\n[33] 2023 Mobility Urban traffic Forecasting\\n[108] 2022 Mobility Urban traffic Forecasting\\n[107] 2023 Mobility Urban traffic Forecasting\\n[103] 2023 Mobility Urban traffic Forecasting\\n[65] 2023 Mobility Bus station profiling Profiling\\n[193] 2021 Mobility Passenger demand Forecasting\\n[128] 2023 Mobility Urban traffic Forecasting\\n71'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 71}, page_content='[230] 2022 Mobility Internet of vehicles Forecasting\\n[205] 2020 Mobility Urban traffic Forecasting\\n[127] 2023 Mobility Urban traffic Forecasting\\n[184] 2023 Mobility Metro flow Forecasting\\n[79] 2023 Mobility Urban traffic Forecasting\\n[106] 2023 Mobility Urban traffic Forecasting\\n[95] 2023 Mobility Urban traffic Forecasting\\n[94] 2023 Mobility Urban traffic Forecasting\\n[122] 2023 Mobility Bike station demand Forecasting\\n[210] 2023 Mobility Urban traffic Forecasting\\n[59] 2023 Mobility Urban traffic Forecasting\\n[54] 2023 Mobility Urban traffic Forecasting\\n[124] 2022 Mobility Taxi demand Forecasting\\n[231] 2022 Mobility Urban traffic Forecasting\\n[238] 2022 Mobility Metro passengers Forecasting\\n[85] 2022 Mobility Urban traffic Forecasting\\n[146] 2021 Mobility Urban traffic Forecasting\\n[134] 2022 Mobility Urban traffic Forecasting\\n[241] 2023 Mobility Urban traffic Forecasting\\n[20] 2020 Mobility Urban traffic Forecasting\\n[2] 2021 Mobility Urban traffic Forecasting\\n[13] 2022 Mobility Urban traffic Forecasting\\n[14] 2022 Mobility Flight delay Forecasting\\n[55] 2022 Mobility Urban traffic Forecasting\\n[36] 2023 Mobility Urban traffic Forecasting\\n[15] 2022 Mobility Urban traffic Forecasting\\n[38] 2022 Mobility Bicycle demand Forecasting\\n[77] 2022 Mobility Urban traffic Forecasting\\n[181] 2023 Other Modulation classification Classification\\n[208] 2022 Other Modulation classification Classification\\n[37] 2023 Other Encrypted traffic classification Classification\\n[81] 2022 Other Spatial temporal events Forecasting\\n[232] 2023 Other Quality indicators Forecasting\\n[194] 2023 Other Cellular traffic Forecasting\\n[11] 2023 Other Seismic data Forecasting\\n[197] 2023 Other Structural dynamics on irregular domains Forecasting\\n[158] 2023 Other Tailing dam monitoring Forecasting\\n[144] 2023 Other Video engagement Forecasting\\n[130] 2023 Other Subsurface production Forecasting\\n[17] 2023 Other Cellular traffic Forecasting\\n[163] 2023 Other Data centers maintenance Forecasting\\n[211] 2023 Other Hydraulic runoff Forecasting\\n[235] 2022 Other App popularity Forecasting\\n[218] 2021 Other Cellular traffic Forecasting\\n[222] 2022 Other Water demand Forecasting\\n[83] 2023 Other User preference Edge prediction\\n[136] 2021 Other Human activity recognition Classification\\n[3] 2023 Other Modulation classification Classification\\n72'),\n",
       " Document(metadata={'source': 'pdfs\\\\2410.22377v1.pdf', 'page': 72}, page_content='[109] 2022 Predictive monitoring Anomaly detection Classification\\n[112] 2023 Predictive monitoring Anomaly detection Forecasting\\n[177] 2023 Predictive monitoring Fault diagnosis Classification\\n[221] 2023 Predictive monitoring Fault diagnosis Classification\\n[100] 2021 Predictive monitoring Fault diagnosis Classification\\n[226] 2023 Predictive monitoring Fault diagnosis Classification\\n[141] 2023 Predictive monitoring Fault diagnosis Classification\\n[233] 2023 Predictive monitoring Fault prediction Forecasting\\n[192] 2023 Predictive monitoring Remaining useful life Forecasting\\n[25] 2023 Predictive monitoring Remaining useful life Forecasting\\n[182] 2023 Predictive monitoring Remaining useful life Forecasting\\n[138] 2021 Predictive monitoring Remaining useful life Forecasting\\n[212] 2022 Predictive monitoring Remaining useful life Forecasting\\n[93] 2022 Predictive monitoring Remaining useful life Forecasting\\n[196] 2023 Predictive monitoring Remaining useful life Forecasting\\n[99] 2023 Predictive monitoring Fault diagnosis Classification\\n73')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader=PyPDFDirectoryLoader(\"pdfs\")\n",
    "\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings(\n",
    "  base_url=\"http://127.0.0.1:1234/v1\",\n",
    "  openai_api_key=\"lm-studio\",\n",
    "  model=\"text-embedding-bge-large-zh-v1.5\",\n",
    "  check_embedding_ctx_length=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x198d06d3850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoreDB=FAISS.from_documents(docs,embeddings)\n",
    "vectorstoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstoreDB.as_retriever(\n",
    "     search_type=\"similarity_score_threshold\",\n",
    "     search_kwargs={\"score_threshold\": 0.3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template=\"\"\"\n",
    "只根据以下文档回答问题：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间序列预测是一种用于预测未来数据的方法，它基于历史数据来预测未来的值。"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel\n",
    "\n",
    "chain=(\n",
    "    {\"context\": retriver, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in chain.stream(\"时间序列预测是什么\"):\n",
    "    print(chunk,end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
